{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from model import *\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "train_path = Path.cwd() / 'toy training data' / 'preprocessed'\r\n",
    "# Define dataset\r\n",
    "train_data = Dataset(train_path)\r\n",
    "train_loader = torch.utils.data.DataLoader(\r\n",
    "    train_data,\r\n",
    "    batch_size=1,\r\n",
    "    shuffle=False,\r\n",
    "    num_workers=0    \r\n",
    ")\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "# Training hyperparameters\r\n",
    "num_epochs = 10\r\n",
    "learning_rate = 1e-4\r\n",
    "batch_size = 1\r\n",
    "\r\n",
    "# Model hyperparameters\r\n",
    "# src_vocab_size = 0 # There is no src vocab since the src is spectrogram frames\r\n",
    "trg_vocab_size = 0 # <output length>\r\n",
    "embedding_size = 512\r\n",
    "num_heads = 8\r\n",
    "num_encoder_layers = 3\r\n",
    "num_decoder_layers = 3\r\n",
    "dropout = 0.1\r\n",
    "max_len = 400\r\n",
    "forward_expansion = 2048\r\n",
    "src_pad_idx = 0 # ???\r\n",
    "\r\n",
    "# Tensorboard for nice plots\r\n",
    "writer = SummaryWriter('runs/loss_plot')\r\n",
    "step = 0\r\n",
    "\r\n",
    "# Define model\r\n",
    "model = Transformer(\r\n",
    "    embedding_size,\r\n",
    "    trg_vocab_size,\r\n",
    "    src_pad_idx,\r\n",
    "    num_heads,\r\n",
    "    num_encoder_layers,\r\n",
    "    num_decoder_layers,\r\n",
    "    forward_expansion,\r\n",
    "    dropout,\r\n",
    "    max_len,\r\n",
    "    device,\r\n",
    ").to(device)\r\n",
    "\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "pad_idx = 0 # TODO \r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    \r\n",
    "    model.train()\r\n",
    "    for batch_idx, batch in enumerate(train_loader):\r\n",
    "        spec, notes = batch.to(device)\r\n",
    "\r\n",
    "        # forward prop\r\n",
    "        output = model(spec, notes[:-1])\r\n",
    "\r\n",
    "        output = output.reshape(-1, output.shape[2])\r\n",
    "        notes = notes[1:].reshape(-1)\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        loss = criterion(output, notes)\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\r\n",
    "\r\n",
    "        optimizer.step()\r\n",
    "        writer.add_scalar(\"Training Loss\", loss, global_step=step)\r\n",
    "        step += 1\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}