{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ewais\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Batch 0\n",
      "Training Loss: 6.309049129486084\n",
      "\n",
      "Epoch 1, Batch 25\n",
      "Training Loss: 4.7210588455200195\n",
      "\n",
      "Epoch 1, Batch 50\n",
      "Training Loss: 4.010907173156738\n",
      "\n",
      "Epoch 1, Batch 75\n",
      "Training Loss: 3.8372321128845215\n",
      "\n",
      "Epoch 1, Batch 100\n",
      "Training Loss: 3.91545033454895\n",
      "\n",
      "Epoch 1, Batch 125\n",
      "Training Loss: 3.8134384155273438\n",
      "\n",
      "Epoch 1, Batch 150\n",
      "Training Loss: 3.718294382095337\n",
      "\n",
      "Epoch 1, Batch 175\n",
      "Training Loss: 3.78564453125\n",
      "\n",
      "Epoch 1, Batch 200\n",
      "Training Loss: 3.572188138961792\n",
      "\n",
      "Epoch 1, Batch 225\n",
      "Training Loss: 3.5495753288269043\n",
      "\n",
      "Epoch 1, Batch 250\n",
      "Training Loss: 3.4048354625701904\n",
      "\n",
      "Epoch 1, Batch 275\n",
      "Training Loss: 3.539491891860962\n",
      "\n",
      "Epoch 1, Batch 300\n",
      "Training Loss: 3.363840103149414\n",
      "\n",
      "Epoch 1, Batch 325\n",
      "Training Loss: 3.253021001815796\n",
      "\n",
      "Epoch 1, Batch 350\n",
      "Training Loss: 3.4574532508850098\n",
      "\n",
      "Epoch 1, Batch 375\n",
      "Training Loss: 3.4201650619506836\n",
      "\n",
      "Epoch 1, Batch 400\n",
      "Training Loss: 3.3002572059631348\n",
      "\n",
      "Epoch 1, Batch 425\n",
      "Training Loss: 3.300520896911621\n",
      "\n",
      "Epoch 1, Batch 450\n",
      "Training Loss: 3.115417242050171\n",
      "\n",
      "Epoch 1, Batch 475\n",
      "Training Loss: 3.2824087142944336\n",
      "\n",
      "Epoch 1, Batch 500\n",
      "Training Loss: 3.322338104248047\n",
      "\n",
      "Epoch 1, Batch 525\n",
      "Training Loss: 3.113872766494751\n",
      "\n",
      "Epoch 1, Batch 550\n",
      "Training Loss: 3.2032880783081055\n",
      "\n",
      "Epoch 1, Batch 575\n",
      "Training Loss: 3.0130927562713623\n",
      "\n",
      "Epoch 1, Batch 600\n",
      "Training Loss: 3.208174228668213\n",
      "\n",
      "Epoch 1, Batch 625\n",
      "Training Loss: 2.990154981613159\n",
      "\n",
      "Epoch 1, Batch 650\n",
      "Training Loss: 3.024061679840088\n",
      "\n",
      "Epoch 1, Batch 675\n",
      "Training Loss: 3.109605550765991\n",
      "\n",
      "Epoch 1, Batch 700\n",
      "Training Loss: 2.8206801414489746\n",
      "\n",
      "Epoch 1, Batch 725\n",
      "Training Loss: 3.012641668319702\n",
      "\n",
      "Epoch 1, Batch 750\n",
      "Training Loss: 3.02666974067688\n",
      "\n",
      "Epoch 1, Batch 775\n",
      "Training Loss: 2.8613548278808594\n",
      "\n",
      "Epoch 1, Batch 800\n",
      "Training Loss: 2.9692881107330322\n",
      "\n",
      "Epoch 1, Batch 825\n",
      "Training Loss: 3.019993543624878\n",
      "\n",
      "Epoch 1, Batch 850\n",
      "Training Loss: 2.9739866256713867\n",
      "\n",
      "Epoch 1, Batch 875\n",
      "Training Loss: 2.941688060760498\n",
      "\n",
      "Epoch 1, Batch 900\n",
      "Training Loss: 2.9728612899780273\n",
      "\n",
      "Epoch 1, Batch 925\n",
      "Training Loss: 2.820955991744995\n",
      "\n",
      "Epoch 1, Batch 950\n",
      "Training Loss: 2.7257611751556396\n",
      "\n",
      "Epoch 1, Batch 975\n",
      "Training Loss: 2.7422943115234375\n",
      "\n",
      "Epoch 1, Batch 1000\n",
      "Training Loss: 2.8309521675109863\n",
      "\n",
      "Epoch 1, Batch 1025\n",
      "Training Loss: 2.94232177734375\n",
      "\n",
      "Epoch 1, Batch 1050\n",
      "Training Loss: 2.999932050704956\n",
      "\n",
      "Epoch 1, Batch 1075\n",
      "Training Loss: 2.8641178607940674\n",
      "\n",
      "Epoch 1, Batch 1100\n",
      "Training Loss: 2.643807888031006\n",
      "\n",
      "Epoch 1, Batch 1125\n",
      "Training Loss: 2.6727805137634277\n",
      "\n",
      "Epoch 1, Batch 1150\n",
      "Training Loss: 2.8133633136749268\n",
      "\n",
      "Epoch 1, Batch 1175\n",
      "Training Loss: 2.639679193496704\n",
      "\n",
      "Epoch 1, Batch 1200\n",
      "Training Loss: 2.740689992904663\n",
      "\n",
      "Epoch 1, Batch 1225\n",
      "Training Loss: 2.938904285430908\n",
      "\n",
      "Epoch 1, Batch 1250\n",
      "Training Loss: 2.7918362617492676\n",
      "\n",
      "Epoch 1, Batch 1275\n",
      "Training Loss: 2.5598907470703125\n",
      "\n",
      "Epoch 1, Batch 1300\n",
      "Training Loss: 2.6909186840057373\n",
      "\n",
      "Epoch 1, Batch 1325\n",
      "Training Loss: 2.481070041656494\n",
      "\n",
      "Epoch 1, Batch 1350\n",
      "Training Loss: 2.730639696121216\n",
      "\n",
      "Epoch 1, Batch 1375\n",
      "Training Loss: 2.6333370208740234\n",
      "\n",
      "Epoch 1, Batch 1400\n",
      "Training Loss: 2.5459842681884766\n",
      "\n",
      "Epoch 1, Batch 1425\n",
      "Training Loss: 2.6407012939453125\n",
      "\n",
      "Epoch 1, Batch 1450\n",
      "Training Loss: 2.7081916332244873\n",
      "\n",
      "Epoch 1, Batch 1475\n",
      "Training Loss: 2.524953603744507\n",
      "\n",
      "Epoch 1, Batch 1500\n",
      "Training Loss: 2.623943567276001\n",
      "\n",
      "Epoch 1, Batch 1525\n",
      "Training Loss: 2.821627140045166\n",
      "\n",
      "Epoch 1, Batch 1550\n",
      "Training Loss: 2.5568459033966064\n",
      "\n",
      "Epoch 1, Batch 1575\n",
      "Training Loss: 2.39267897605896\n",
      "\n",
      "Epoch 1, Batch 1600\n",
      "Training Loss: 2.645721435546875\n",
      "\n",
      "Epoch 1, Batch 1625\n",
      "Training Loss: 2.5236175060272217\n",
      "\n",
      "Epoch 1, Batch 1650\n",
      "Training Loss: 2.452530860900879\n",
      "\n",
      "Epoch 1, Batch 1675\n",
      "Training Loss: 2.3566229343414307\n",
      "\n",
      "Epoch 1, Batch 1700\n",
      "Training Loss: 2.772838830947876\n",
      "\n",
      "Epoch 1, Batch 1725\n",
      "Training Loss: 2.74955415725708\n",
      "\n",
      "Epoch 1, Batch 1750\n",
      "Training Loss: 2.521498203277588\n",
      "\n",
      "Epoch 1, Batch 1775\n",
      "Training Loss: 2.303720235824585\n",
      "\n",
      "Epoch 1, Batch 1800\n",
      "Training Loss: 2.681248188018799\n",
      "\n",
      "Epoch 1, Batch 1825\n",
      "Training Loss: 2.79206919670105\n",
      "\n",
      "Epoch 1, Batch 1850\n",
      "Training Loss: 2.446398973464966\n",
      "\n",
      "Epoch 1, Batch 1875\n",
      "Training Loss: 2.4360642433166504\n",
      "\n",
      "Epoch 1, Batch 1900\n",
      "Training Loss: 2.4624154567718506\n",
      "\n",
      "Epoch 1, Batch 1925\n",
      "Training Loss: 2.3161816596984863\n",
      "\n",
      "Epoch 1, Batch 1950\n",
      "Training Loss: 2.4120516777038574\n",
      "\n",
      "Epoch 1, Batch 1975\n",
      "Training Loss: 2.6138949394226074\n",
      "\n",
      "Epoch 1, Batch 2000\n",
      "Training Loss: 2.3597240447998047\n",
      "\n",
      "Epoch 1, Batch 2025\n",
      "Training Loss: 2.365070343017578\n",
      "\n",
      "Epoch 1, Batch 2050\n",
      "Training Loss: 2.3195314407348633\n",
      "\n",
      "Epoch 1, Batch 2075\n",
      "Training Loss: 2.4554555416107178\n",
      "\n",
      "Epoch 1, Batch 2100\n",
      "Training Loss: 2.5312929153442383\n",
      "\n",
      "Epoch 1, Batch 2125\n",
      "Training Loss: 2.295083522796631\n",
      "\n",
      "Epoch 1, Batch 2150\n",
      "Training Loss: 2.3527143001556396\n",
      "\n",
      "Epoch 1, Batch 2175\n",
      "Training Loss: 2.5503647327423096\n",
      "\n",
      "Epoch 1, Batch 2200\n",
      "Training Loss: 2.3502016067504883\n",
      "\n",
      "Epoch 1, Batch 2225\n",
      "Training Loss: 2.3332326412200928\n",
      "\n",
      "Epoch 1, Batch 2250\n",
      "Training Loss: 2.349743366241455\n",
      "\n",
      "Epoch 1, Batch 2275\n",
      "Training Loss: 2.1178839206695557\n",
      "\n",
      "Epoch 1, Batch 2300\n",
      "Training Loss: 2.343663215637207\n",
      "\n",
      "Epoch 1, Batch 2325\n",
      "Training Loss: 2.1948959827423096\n",
      "\n",
      "Epoch 1, Batch 2350\n",
      "Training Loss: 2.5284745693206787\n",
      "\n",
      "Epoch 1, Batch 2375\n",
      "Training Loss: 2.2588298320770264\n",
      "\n",
      "Epoch 1, Batch 2400\n",
      "Training Loss: 2.6536128520965576\n",
      "\n",
      "Epoch 1, Batch 2425\n",
      "Training Loss: 2.337247133255005\n",
      "\n",
      "Epoch 1, Batch 2450\n",
      "Training Loss: 2.188066005706787\n",
      "\n",
      "Epoch 1, Batch 2475\n",
      "Training Loss: 2.4074594974517822\n",
      "\n",
      "Epoch 1, Batch 2500\n",
      "Training Loss: 2.3486616611480713\n",
      "\n",
      "Epoch 1, Batch 2525\n",
      "Training Loss: 2.475679397583008\n",
      "\n",
      "Epoch 1, Batch 2550\n",
      "Training Loss: 2.4922025203704834\n",
      "\n",
      "Epoch 1, Batch 2575\n",
      "Training Loss: 2.345289945602417\n",
      "\n",
      "Epoch 1, Batch 2600\n",
      "Training Loss: 2.2218594551086426\n",
      "\n",
      "Epoch 1, Batch 2625\n",
      "Training Loss: 2.2529683113098145\n",
      "\n",
      "Epoch 1, Batch 2650\n",
      "Training Loss: 2.3283135890960693\n",
      "\n",
      "Epoch 1, Batch 2675\n",
      "Training Loss: 2.261368751525879\n",
      "\n",
      "Epoch 1, Batch 2700\n",
      "Training Loss: 2.5772838592529297\n",
      "\n",
      "Epoch 1, Batch 2725\n",
      "Training Loss: 2.268200635910034\n",
      "\n",
      "Epoch 1, Batch 2750\n",
      "Training Loss: 2.7502927780151367\n",
      "\n",
      "Epoch 1, Batch 2775\n",
      "Training Loss: 2.4369239807128906\n",
      "\n",
      "Epoch 1, Batch 2800\n",
      "Training Loss: 2.317413091659546\n",
      "\n",
      "Epoch 1, Batch 2825\n",
      "Training Loss: 2.1200225353240967\n",
      "\n",
      "Epoch 1, Batch 2850\n",
      "Training Loss: 2.4403610229492188\n",
      "\n",
      "Epoch 1, Batch 2875\n",
      "Training Loss: 2.2479705810546875\n",
      "\n",
      "Epoch 1, Batch 2900\n",
      "Training Loss: 2.1423604488372803\n",
      "\n",
      "Epoch 1, Batch 2925\n",
      "Training Loss: 2.339144229888916\n",
      "\n",
      "Epoch 1, Batch 2950\n",
      "Training Loss: 2.1118671894073486\n",
      "\n",
      "Epoch 1/30\n",
      "Training Loss: 2.356316566467285\n",
      "model saved\n",
      "\n",
      "Epoch 2, Batch 0\n",
      "Training Loss: 2.4547932147979736\n",
      "\n",
      "Epoch 2, Batch 25\n",
      "Training Loss: 2.3219540119171143\n",
      "\n",
      "Epoch 2, Batch 50\n",
      "Training Loss: 2.190977096557617\n",
      "\n",
      "Epoch 2, Batch 75\n",
      "Training Loss: 2.3588874340057373\n",
      "\n",
      "Epoch 2, Batch 100\n",
      "Training Loss: 2.2534167766571045\n",
      "\n",
      "Epoch 2, Batch 125\n",
      "Training Loss: 2.697949171066284\n",
      "\n",
      "Epoch 2, Batch 150\n",
      "Training Loss: 2.061980962753296\n",
      "\n",
      "Epoch 2, Batch 175\n",
      "Training Loss: 2.237974166870117\n",
      "\n",
      "Epoch 2, Batch 200\n",
      "Training Loss: 2.311896562576294\n",
      "\n",
      "Epoch 2, Batch 225\n",
      "Training Loss: 2.19535756111145\n",
      "\n",
      "Epoch 2, Batch 250\n",
      "Training Loss: 2.2843594551086426\n",
      "\n",
      "Epoch 2, Batch 275\n",
      "Training Loss: 2.067034959793091\n",
      "\n",
      "Epoch 2, Batch 300\n",
      "Training Loss: 2.2303903102874756\n",
      "\n",
      "Epoch 2, Batch 325\n",
      "Training Loss: 2.316387891769409\n",
      "\n",
      "Epoch 2, Batch 350\n",
      "Training Loss: 2.343472719192505\n",
      "\n",
      "Epoch 2, Batch 375\n",
      "Training Loss: 2.2326669692993164\n",
      "\n",
      "Epoch 2, Batch 400\n",
      "Training Loss: 2.2490811347961426\n",
      "\n",
      "Epoch 2, Batch 425\n",
      "Training Loss: 2.6289477348327637\n",
      "\n",
      "Epoch 2, Batch 450\n",
      "Training Loss: 2.4278860092163086\n",
      "\n",
      "Epoch 2, Batch 475\n",
      "Training Loss: 2.288269519805908\n",
      "\n",
      "Epoch 2, Batch 500\n",
      "Training Loss: 2.1519293785095215\n",
      "\n",
      "Epoch 2, Batch 525\n",
      "Training Loss: 2.130030870437622\n",
      "\n",
      "Epoch 2, Batch 550\n",
      "Training Loss: 2.5121798515319824\n",
      "\n",
      "Epoch 2, Batch 575\n",
      "Training Loss: 2.3806557655334473\n",
      "\n",
      "Epoch 2, Batch 600\n",
      "Training Loss: 2.485342025756836\n",
      "\n",
      "Epoch 2, Batch 625\n",
      "Training Loss: 2.2317569255828857\n",
      "\n",
      "Epoch 2, Batch 650\n",
      "Training Loss: 2.3500208854675293\n",
      "\n",
      "Epoch 2, Batch 675\n",
      "Training Loss: 2.473695993423462\n",
      "\n",
      "Epoch 2, Batch 700\n",
      "Training Loss: 2.1700778007507324\n",
      "\n",
      "Epoch 2, Batch 725\n",
      "Training Loss: 2.4085187911987305\n",
      "\n",
      "Epoch 2, Batch 750\n",
      "Training Loss: 2.229980945587158\n",
      "\n",
      "Epoch 2, Batch 775\n",
      "Training Loss: 2.028106927871704\n",
      "\n",
      "Epoch 2, Batch 800\n",
      "Training Loss: 2.41396164894104\n",
      "\n",
      "Epoch 2, Batch 825\n",
      "Training Loss: 2.2271182537078857\n",
      "\n",
      "Epoch 2, Batch 850\n",
      "Training Loss: 2.301074504852295\n",
      "\n",
      "Epoch 2, Batch 875\n",
      "Training Loss: 2.12842059135437\n",
      "\n",
      "Epoch 2, Batch 900\n",
      "Training Loss: 2.421743869781494\n",
      "\n",
      "Epoch 2, Batch 925\n",
      "Training Loss: 1.9344463348388672\n",
      "\n",
      "Epoch 2, Batch 950\n",
      "Training Loss: 2.335061550140381\n",
      "\n",
      "Epoch 2, Batch 975\n",
      "Training Loss: 2.4829204082489014\n",
      "\n",
      "Epoch 2, Batch 1000\n",
      "Training Loss: 2.280179738998413\n",
      "\n",
      "Epoch 2, Batch 1025\n",
      "Training Loss: 2.4977221488952637\n",
      "\n",
      "Epoch 2, Batch 1050\n",
      "Training Loss: 2.320664882659912\n",
      "\n",
      "Epoch 2, Batch 1075\n",
      "Training Loss: 2.4090187549591064\n",
      "\n",
      "Epoch 2, Batch 1100\n",
      "Training Loss: 2.070045232772827\n",
      "\n",
      "Epoch 2, Batch 1125\n",
      "Training Loss: 2.4321234226226807\n",
      "\n",
      "Epoch 2, Batch 1150\n",
      "Training Loss: 2.029184103012085\n",
      "\n",
      "Epoch 2, Batch 1175\n",
      "Training Loss: 2.6622962951660156\n",
      "\n",
      "Epoch 2, Batch 1200\n",
      "Training Loss: 2.2293667793273926\n",
      "\n",
      "Epoch 2, Batch 1225\n",
      "Training Loss: 2.1773853302001953\n",
      "\n",
      "Epoch 2, Batch 1250\n",
      "Training Loss: 1.8593778610229492\n",
      "\n",
      "Epoch 2, Batch 1275\n",
      "Training Loss: 2.472010374069214\n",
      "\n",
      "Epoch 2, Batch 1300\n",
      "Training Loss: 2.4382007122039795\n",
      "\n",
      "Epoch 2, Batch 1325\n",
      "Training Loss: 2.4485437870025635\n",
      "\n",
      "Epoch 2, Batch 1350\n",
      "Training Loss: 2.4254071712493896\n",
      "\n",
      "Epoch 2, Batch 1375\n",
      "Training Loss: 2.3362207412719727\n",
      "\n",
      "Epoch 2, Batch 1400\n",
      "Training Loss: 2.339521646499634\n",
      "\n",
      "Epoch 2, Batch 1425\n",
      "Training Loss: 2.303478717803955\n",
      "\n",
      "Epoch 2, Batch 1450\n",
      "Training Loss: 2.5992934703826904\n",
      "\n",
      "Epoch 2, Batch 1475\n",
      "Training Loss: 2.186424493789673\n",
      "\n",
      "Epoch 2, Batch 1500\n",
      "Training Loss: 2.588426113128662\n",
      "\n",
      "Epoch 2, Batch 1525\n",
      "Training Loss: 2.0092368125915527\n",
      "\n",
      "Epoch 2, Batch 1550\n",
      "Training Loss: 2.1552889347076416\n",
      "\n",
      "Epoch 2, Batch 1575\n",
      "Training Loss: 1.9783285856246948\n",
      "\n",
      "Epoch 2, Batch 1600\n",
      "Training Loss: 2.4773800373077393\n",
      "\n",
      "Epoch 2, Batch 1625\n",
      "Training Loss: 2.293544054031372\n",
      "\n",
      "Epoch 2, Batch 1650\n",
      "Training Loss: 2.217999219894409\n",
      "\n",
      "Epoch 2, Batch 1675\n",
      "Training Loss: 2.101267099380493\n",
      "\n",
      "Epoch 2, Batch 1700\n",
      "Training Loss: 2.6963274478912354\n",
      "\n",
      "Epoch 2, Batch 1725\n",
      "Training Loss: 2.1476330757141113\n",
      "\n",
      "Epoch 2, Batch 1750\n",
      "Training Loss: 2.0476396083831787\n",
      "\n",
      "Epoch 2, Batch 1775\n",
      "Training Loss: 1.8756606578826904\n",
      "\n",
      "Epoch 2, Batch 1800\n",
      "Training Loss: 2.342576503753662\n",
      "\n",
      "Epoch 2, Batch 1825\n",
      "Training Loss: 2.2263998985290527\n",
      "\n",
      "Epoch 2, Batch 1850\n",
      "Training Loss: 2.3837499618530273\n",
      "\n",
      "Epoch 2, Batch 1875\n",
      "Training Loss: 2.163823366165161\n",
      "\n",
      "Epoch 2, Batch 1900\n",
      "Training Loss: 2.382168769836426\n",
      "\n",
      "Epoch 2, Batch 1925\n",
      "Training Loss: 2.6033928394317627\n",
      "\n",
      "Epoch 2, Batch 1950\n",
      "Training Loss: 2.280625820159912\n",
      "\n",
      "Epoch 2, Batch 1975\n",
      "Training Loss: 1.9459072351455688\n",
      "\n",
      "Epoch 2, Batch 2000\n",
      "Training Loss: 2.0278165340423584\n",
      "\n",
      "Epoch 2, Batch 2025\n",
      "Training Loss: 2.1827824115753174\n",
      "\n",
      "Epoch 2, Batch 2050\n",
      "Training Loss: 2.1233530044555664\n",
      "\n",
      "Epoch 2, Batch 2075\n",
      "Training Loss: 2.3513102531433105\n",
      "\n",
      "Epoch 2, Batch 2100\n",
      "Training Loss: 2.043351888656616\n",
      "\n",
      "Epoch 2, Batch 2125\n",
      "Training Loss: 2.2281463146209717\n",
      "\n",
      "Epoch 2, Batch 2150\n",
      "Training Loss: 2.033769369125366\n",
      "\n",
      "Epoch 2, Batch 2175\n",
      "Training Loss: 2.439744710922241\n",
      "\n",
      "Epoch 2, Batch 2200\n",
      "Training Loss: 2.4155757427215576\n",
      "\n",
      "Epoch 2, Batch 2225\n",
      "Training Loss: 2.2758171558380127\n",
      "\n",
      "Epoch 2, Batch 2250\n",
      "Training Loss: 2.270228385925293\n",
      "\n",
      "Epoch 2, Batch 2275\n",
      "Training Loss: 2.2842049598693848\n",
      "\n",
      "Epoch 2, Batch 2300\n",
      "Training Loss: 2.382704257965088\n",
      "\n",
      "Epoch 2, Batch 2325\n",
      "Training Loss: 2.297477960586548\n",
      "\n",
      "Epoch 2, Batch 2350\n",
      "Training Loss: 2.2415666580200195\n",
      "\n",
      "Epoch 2, Batch 2375\n",
      "Training Loss: 2.18119740486145\n",
      "\n",
      "Epoch 2, Batch 2400\n",
      "Training Loss: 2.3223586082458496\n",
      "\n",
      "Epoch 2, Batch 2425\n",
      "Training Loss: 1.9709802865982056\n",
      "\n",
      "Epoch 2, Batch 2450\n",
      "Training Loss: 2.185014486312866\n",
      "\n",
      "Epoch 2, Batch 2475\n",
      "Training Loss: 2.138061761856079\n",
      "\n",
      "Epoch 2, Batch 2500\n",
      "Training Loss: 2.1581177711486816\n",
      "\n",
      "Epoch 2, Batch 2525\n",
      "Training Loss: 1.9835798740386963\n",
      "\n",
      "Epoch 2, Batch 2550\n",
      "Training Loss: 2.0475659370422363\n",
      "\n",
      "Epoch 2, Batch 2575\n",
      "Training Loss: 2.2597177028656006\n",
      "\n",
      "Epoch 2, Batch 2600\n",
      "Training Loss: 2.16416335105896\n",
      "\n",
      "Epoch 2, Batch 2625\n",
      "Training Loss: 2.3549318313598633\n",
      "\n",
      "Epoch 2, Batch 2650\n",
      "Training Loss: 2.2010107040405273\n",
      "\n",
      "Epoch 2, Batch 2675\n",
      "Training Loss: 1.9733961820602417\n",
      "\n",
      "Epoch 2, Batch 2700\n",
      "Training Loss: 2.423128843307495\n",
      "\n",
      "Epoch 2, Batch 2725\n",
      "Training Loss: 2.1042439937591553\n",
      "\n",
      "Epoch 2, Batch 2750\n",
      "Training Loss: 2.245960235595703\n",
      "\n",
      "Epoch 2, Batch 2775\n",
      "Training Loss: 1.979668140411377\n",
      "\n",
      "Epoch 2, Batch 2800\n",
      "Training Loss: 2.089003324508667\n",
      "\n",
      "Epoch 2, Batch 2825\n",
      "Training Loss: 2.175475597381592\n",
      "\n",
      "Epoch 2, Batch 2850\n",
      "Training Loss: 2.173246383666992\n",
      "\n",
      "Epoch 2, Batch 2875\n",
      "Training Loss: 2.240222215652466\n",
      "\n",
      "Epoch 2, Batch 2900\n",
      "Training Loss: 2.43904972076416\n",
      "\n",
      "Epoch 2, Batch 2925\n",
      "Training Loss: 2.3529176712036133\n",
      "\n",
      "Epoch 2, Batch 2950\n",
      "Training Loss: 2.2643566131591797\n",
      "\n",
      "Epoch 2/30\n",
      "Training Loss: 2.1814804077148438\n",
      "model saved\n",
      "\n",
      "Epoch 3, Batch 0\n",
      "Training Loss: 2.071192979812622\n",
      "\n",
      "Epoch 3, Batch 25\n",
      "Training Loss: 2.104966640472412\n",
      "\n",
      "Epoch 3, Batch 50\n",
      "Training Loss: 1.9228122234344482\n",
      "\n",
      "Epoch 3, Batch 75\n",
      "Training Loss: 2.1982789039611816\n",
      "\n",
      "Epoch 3, Batch 100\n",
      "Training Loss: 2.3375797271728516\n",
      "\n",
      "Epoch 3, Batch 125\n",
      "Training Loss: 1.8854104280471802\n",
      "\n",
      "Epoch 3, Batch 150\n",
      "Training Loss: 2.1222639083862305\n",
      "\n",
      "Epoch 3, Batch 175\n",
      "Training Loss: 1.988585352897644\n",
      "\n",
      "Epoch 3, Batch 200\n",
      "Training Loss: 2.1082992553710938\n",
      "\n",
      "Epoch 3, Batch 225\n",
      "Training Loss: 2.291879177093506\n",
      "\n",
      "Epoch 3, Batch 250\n",
      "Training Loss: 2.455702781677246\n",
      "\n",
      "Epoch 3, Batch 275\n",
      "Training Loss: 1.8365622758865356\n",
      "\n",
      "Epoch 3, Batch 300\n",
      "Training Loss: 2.166592597961426\n",
      "\n",
      "Epoch 3, Batch 325\n",
      "Training Loss: 2.013007402420044\n",
      "\n",
      "Epoch 3, Batch 350\n",
      "Training Loss: 2.266535758972168\n",
      "\n",
      "Epoch 3, Batch 375\n",
      "Training Loss: 2.1572742462158203\n",
      "\n",
      "Epoch 3, Batch 400\n",
      "Training Loss: 1.9657728672027588\n",
      "\n",
      "Epoch 3, Batch 425\n",
      "Training Loss: 2.1218173503875732\n",
      "\n",
      "Epoch 3, Batch 450\n",
      "Training Loss: 2.1371681690216064\n",
      "\n",
      "Epoch 3, Batch 475\n",
      "Training Loss: 1.7023875713348389\n",
      "\n",
      "Epoch 3, Batch 500\n",
      "Training Loss: 2.321892499923706\n",
      "\n",
      "Epoch 3, Batch 525\n",
      "Training Loss: 2.165492057800293\n",
      "\n",
      "Epoch 3, Batch 550\n",
      "Training Loss: 1.982131838798523\n",
      "\n",
      "Epoch 3, Batch 575\n",
      "Training Loss: 2.156153917312622\n",
      "\n",
      "Epoch 3, Batch 600\n",
      "Training Loss: 2.3790156841278076\n",
      "\n",
      "Epoch 3, Batch 625\n",
      "Training Loss: 2.1940221786499023\n",
      "\n",
      "Epoch 3, Batch 650\n",
      "Training Loss: 2.2182278633117676\n",
      "\n",
      "Epoch 3, Batch 675\n",
      "Training Loss: 2.2319436073303223\n",
      "\n",
      "Epoch 3, Batch 700\n",
      "Training Loss: 2.171018362045288\n",
      "\n",
      "Epoch 3, Batch 725\n",
      "Training Loss: 2.450584650039673\n",
      "\n",
      "Epoch 3, Batch 750\n",
      "Training Loss: 2.296245574951172\n",
      "\n",
      "Epoch 3, Batch 775\n",
      "Training Loss: 1.9577714204788208\n",
      "\n",
      "Epoch 3, Batch 800\n",
      "Training Loss: 2.2620787620544434\n",
      "\n",
      "Epoch 3, Batch 825\n",
      "Training Loss: 2.049262285232544\n",
      "\n",
      "Epoch 3, Batch 850\n",
      "Training Loss: 2.1631617546081543\n",
      "\n",
      "Epoch 3, Batch 875\n",
      "Training Loss: 2.2035555839538574\n",
      "\n",
      "Epoch 3, Batch 900\n",
      "Training Loss: 2.076620578765869\n",
      "\n",
      "Epoch 3, Batch 925\n",
      "Training Loss: 1.7679334878921509\n",
      "\n",
      "Epoch 3, Batch 950\n",
      "Training Loss: 2.099849224090576\n",
      "\n",
      "Epoch 3, Batch 975\n",
      "Training Loss: 1.887427568435669\n",
      "\n",
      "Epoch 3, Batch 1000\n",
      "Training Loss: 2.5605506896972656\n",
      "\n",
      "Epoch 3, Batch 1025\n",
      "Training Loss: 2.5491445064544678\n",
      "\n",
      "Epoch 3, Batch 1050\n",
      "Training Loss: 2.076481580734253\n",
      "\n",
      "Epoch 3, Batch 1075\n",
      "Training Loss: 2.215259313583374\n",
      "\n",
      "Epoch 3, Batch 1100\n",
      "Training Loss: 2.196232557296753\n",
      "\n",
      "Epoch 3, Batch 1125\n",
      "Training Loss: 2.1462886333465576\n",
      "\n",
      "Epoch 3, Batch 1150\n",
      "Training Loss: 2.25168514251709\n",
      "\n",
      "Epoch 3, Batch 1175\n",
      "Training Loss: 2.154477834701538\n",
      "\n",
      "Epoch 3, Batch 1200\n",
      "Training Loss: 2.1288106441497803\n",
      "\n",
      "Epoch 3, Batch 1225\n",
      "Training Loss: 2.218757390975952\n",
      "\n",
      "Epoch 3, Batch 1250\n",
      "Training Loss: 2.014320135116577\n",
      "\n",
      "Epoch 3, Batch 1275\n",
      "Training Loss: 1.7991001605987549\n",
      "\n",
      "Epoch 3, Batch 1300\n",
      "Training Loss: 2.164759635925293\n",
      "\n",
      "Epoch 3, Batch 1325\n",
      "Training Loss: 1.9668769836425781\n",
      "\n",
      "Epoch 3, Batch 1350\n",
      "Training Loss: 2.1898136138916016\n",
      "\n",
      "Epoch 3, Batch 1375\n",
      "Training Loss: 2.0407793521881104\n",
      "\n",
      "Epoch 3, Batch 1400\n",
      "Training Loss: 2.22066068649292\n",
      "\n",
      "Epoch 3, Batch 1425\n",
      "Training Loss: 1.9262065887451172\n",
      "\n",
      "Epoch 3, Batch 1450\n",
      "Training Loss: 1.9570392370224\n",
      "\n",
      "Epoch 3, Batch 1475\n",
      "Training Loss: 2.199965238571167\n",
      "\n",
      "Epoch 3, Batch 1500\n",
      "Training Loss: 2.463449716567993\n",
      "\n",
      "Epoch 3, Batch 1525\n",
      "Training Loss: 2.034015417098999\n",
      "\n",
      "Epoch 3, Batch 1550\n",
      "Training Loss: 1.853760838508606\n",
      "\n",
      "Epoch 3, Batch 1575\n",
      "Training Loss: 2.1156861782073975\n",
      "\n",
      "Epoch 3, Batch 1600\n",
      "Training Loss: 2.1830153465270996\n",
      "\n",
      "Epoch 3, Batch 1625\n",
      "Training Loss: 2.1702451705932617\n",
      "\n",
      "Epoch 3, Batch 1650\n",
      "Training Loss: 2.129426956176758\n",
      "\n",
      "Epoch 3, Batch 1675\n",
      "Training Loss: 2.2165684700012207\n",
      "\n",
      "Epoch 3, Batch 1700\n",
      "Training Loss: 1.980785608291626\n",
      "\n",
      "Epoch 3, Batch 1725\n",
      "Training Loss: 1.9890352487564087\n",
      "\n",
      "Epoch 3, Batch 1750\n",
      "Training Loss: 2.167332887649536\n",
      "\n",
      "Epoch 3, Batch 1775\n",
      "Training Loss: 2.0594162940979004\n",
      "\n",
      "Epoch 3, Batch 1800\n",
      "Training Loss: 2.39335298538208\n",
      "\n",
      "Epoch 3, Batch 1825\n",
      "Training Loss: 2.120897054672241\n",
      "\n",
      "Epoch 3, Batch 1850\n",
      "Training Loss: 1.8831629753112793\n",
      "\n",
      "Epoch 3, Batch 1875\n",
      "Training Loss: 2.5280797481536865\n",
      "\n",
      "Epoch 3, Batch 1900\n",
      "Training Loss: 2.2140939235687256\n",
      "\n",
      "Epoch 3, Batch 1925\n",
      "Training Loss: 2.4285523891448975\n",
      "\n",
      "Epoch 3, Batch 1950\n",
      "Training Loss: 2.13258695602417\n",
      "\n",
      "Epoch 3, Batch 1975\n",
      "Training Loss: 2.1829042434692383\n",
      "\n",
      "Epoch 3, Batch 2000\n",
      "Training Loss: 1.89044189453125\n",
      "\n",
      "Epoch 3, Batch 2025\n",
      "Training Loss: 2.1146557331085205\n",
      "\n",
      "Epoch 3, Batch 2050\n",
      "Training Loss: 2.139368772506714\n",
      "\n",
      "Epoch 3, Batch 2075\n",
      "Training Loss: 1.9234838485717773\n",
      "\n",
      "Epoch 3, Batch 2100\n",
      "Training Loss: 1.684208869934082\n",
      "\n",
      "Epoch 3, Batch 2125\n",
      "Training Loss: 2.6499826908111572\n",
      "\n",
      "Epoch 3, Batch 2150\n",
      "Training Loss: 1.760430097579956\n",
      "\n",
      "Epoch 3, Batch 2175\n",
      "Training Loss: 2.1009228229522705\n",
      "\n",
      "Epoch 3, Batch 2200\n",
      "Training Loss: 2.133850336074829\n",
      "\n",
      "Epoch 3, Batch 2225\n",
      "Training Loss: 1.90382719039917\n",
      "\n",
      "Epoch 3, Batch 2250\n",
      "Training Loss: 2.094951629638672\n",
      "\n",
      "Epoch 3, Batch 2275\n",
      "Training Loss: 2.043034791946411\n",
      "\n",
      "Epoch 3, Batch 2300\n",
      "Training Loss: 2.334449052810669\n",
      "\n",
      "Epoch 3, Batch 2325\n",
      "Training Loss: 1.9657567739486694\n",
      "\n",
      "Epoch 3, Batch 2350\n",
      "Training Loss: 2.2299644947052\n",
      "\n",
      "Epoch 3, Batch 2375\n",
      "Training Loss: 2.028773307800293\n",
      "\n",
      "Epoch 3, Batch 2400\n",
      "Training Loss: 1.8541491031646729\n",
      "\n",
      "Epoch 3, Batch 2425\n",
      "Training Loss: 2.0085091590881348\n",
      "\n",
      "Epoch 3, Batch 2450\n",
      "Training Loss: 1.96448814868927\n",
      "\n",
      "Epoch 3, Batch 2475\n",
      "Training Loss: 2.1382486820220947\n",
      "\n",
      "Epoch 3, Batch 2500\n",
      "Training Loss: 2.4228644371032715\n",
      "\n",
      "Epoch 3, Batch 2525\n",
      "Training Loss: 1.920822024345398\n",
      "\n",
      "Epoch 3, Batch 2550\n",
      "Training Loss: 1.980672836303711\n",
      "\n",
      "Epoch 3, Batch 2575\n",
      "Training Loss: 2.024348258972168\n",
      "\n",
      "Epoch 3, Batch 2600\n",
      "Training Loss: 2.168776273727417\n",
      "\n",
      "Epoch 3, Batch 2625\n",
      "Training Loss: 2.094367265701294\n",
      "\n",
      "Epoch 3, Batch 2650\n",
      "Training Loss: 2.4127588272094727\n",
      "\n",
      "Epoch 3, Batch 2675\n",
      "Training Loss: 2.18735671043396\n",
      "\n",
      "Epoch 3, Batch 2700\n",
      "Training Loss: 2.224947452545166\n",
      "\n",
      "Epoch 3, Batch 2725\n",
      "Training Loss: 2.080923318862915\n",
      "\n",
      "Epoch 3, Batch 2750\n",
      "Training Loss: 1.9929038286209106\n",
      "\n",
      "Epoch 3, Batch 2775\n",
      "Training Loss: 1.9996908903121948\n",
      "\n",
      "Epoch 3, Batch 2800\n",
      "Training Loss: 2.0766732692718506\n",
      "\n",
      "Epoch 3, Batch 2825\n",
      "Training Loss: 1.9027444124221802\n",
      "\n",
      "Epoch 3, Batch 2850\n",
      "Training Loss: 2.5341808795928955\n",
      "\n",
      "Epoch 3, Batch 2875\n",
      "Training Loss: 2.1792142391204834\n",
      "\n",
      "Epoch 3, Batch 2900\n",
      "Training Loss: 1.8184016942977905\n",
      "\n",
      "Epoch 3, Batch 2925\n",
      "Training Loss: 2.3470797538757324\n",
      "\n",
      "Epoch 3, Batch 2950\n",
      "Training Loss: 2.584839105606079\n",
      "\n",
      "Epoch 3/30\n",
      "Training Loss: 2.0850119590759277\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "params = {\n",
    "    'batch_size' : 16,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 4,\n",
    "    'drop_last' : True\n",
    "}\n",
    "\n",
    "# NOTE: max_trg_len <= max_src_len otherwise side asset error is triggered\n",
    "max_trg_len = 500 # length of all target note sequences, holds 99 notes max\n",
    "max_src_len = 500\n",
    "pad_idx = 434\n",
    "\n",
    "# Define data loaders\n",
    "train_path = Path(r'X:\\Training Data\\Model 1 Training\\train')\n",
    "train_data = LazierDataset(train_path, max_src_len, max_trg_len, pad_idx)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **params)\n",
    "val_path = Path(r'X:\\Training Data\\Model 1 Training\\val')\n",
    "val_data = LazierDataset(val_path, max_src_len, max_trg_len, pad_idx)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **params)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "# Model hyperparameters\n",
    "trg_vocab_size = 435  # <output length>\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dropout = 0.1\n",
    "max_len = max_src_len\n",
    "forward_expansion = 2048\n",
    "\n",
    "# Tensorboard for nice plots\n",
    "writer = SummaryWriter('runs/model5')\n",
    "step = 0  # how many times the model has gone through some input\n",
    "\n",
    "# Define model\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    trg_vocab_size,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), 'model.pt')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() # Multi-class loss, when you have a many class prediction problem\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=434)\n",
    "\n",
    "num_epochs = 30\n",
    "model.train() # Put model in training mode, so that it knows it's parameters should be updated\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Batches come through as a tuple defined in the return statement __getitem__ in the Dataset\n",
    "        spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # forward prop\n",
    "        output = model(spec, notes[..., :-1]) # Don't pass the last element into the decoder, want it to be predicted\n",
    "        # print('output shape : {}'.format(output.shape))\n",
    "        # output = output.reshape(-1, output.shape[2]) # Reshape the output for use by criterion\n",
    "        notes = notes[..., 1:] # .reshape(-1)           # Same for the notes\n",
    "        # print('notes shape 2 {}'.format(notes.shape))\n",
    "        optimizer.zero_grad()                        # Zero out the gradient so it doesn't accumulate\n",
    "\n",
    "        loss = criterion(output.permute(0,2,1), notes)     # Calculate loss, this is output vs ground truth\n",
    "        loss.backward()                     # Compute loss for every node in the computation graph\n",
    "\n",
    "        # This line to avoid the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()    # Update model parameters\n",
    "        writer.add_scalar(\"Training Loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "        if batch_idx%25 == 0:\n",
    "            print('\\nEpoch {}, Batch {}'.format(epoch+1,batch_idx))\n",
    "            print('Training Loss: {}'.format(loss.item()))\n",
    "        \n",
    "        # if batch_idx%100 == 0:\n",
    "            # print('Ground Truth (sample) : {}'.format(notes[0]))\n",
    "            # print('Canidate (sample)     : {}'.format(torch.argmax(output[0], dim=1)))\n",
    "        \n",
    "\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Training Loss: {loss.item()}')\n",
    "    torch.save(model.state_dict(), 'model5.pt')\n",
    "    print('model saved')\n",
    "    # # Evaluate on validation set\n",
    "    # model.eval()\n",
    "    # for batch_idx, batch in enumerate(val_loader):\n",
    "        # spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # # forward prop\n",
    "        # output = model(spec, notes[..., :-1]) # Don't pass the last element into the decoder, want it to be predicted\n",
    "\n",
    "        # # output = output.reshape(-1, output.shape[2]) # Reshape the output for use by criterion\n",
    "        # notes = notes[..., 1:] # .reshape(-1)           # Same for the notes\n",
    "        \n",
    "        # loss = criterion(output.permute(0,2,1), notes)     # Calculate loss, this is output vs ground truth\n",
    "\n",
    "        # writer.add_scalar(\"Validation Loss\", loss, global_step=step)\n",
    "        # step += 1\n",
    "    # print('Validation Loss: {}'.format(loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ewais\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4, Batch 0\n",
      "Training Loss: 1.7855188846588135\n",
      "\n",
      "Epoch 4, Batch 25\n",
      "Training Loss: 2.198181629180908\n",
      "\n",
      "Epoch 4, Batch 50\n",
      "Training Loss: 1.9903161525726318\n",
      "\n",
      "Epoch 4, Batch 75\n",
      "Training Loss: 2.1484198570251465\n",
      "\n",
      "Epoch 4, Batch 100\n",
      "Training Loss: 2.0231077671051025\n",
      "\n",
      "Epoch 4, Batch 125\n",
      "Training Loss: 1.8063194751739502\n",
      "\n",
      "Epoch 4, Batch 150\n",
      "Training Loss: 2.307825803756714\n",
      "\n",
      "Epoch 4, Batch 175\n",
      "Training Loss: 2.0214645862579346\n",
      "\n",
      "Epoch 4, Batch 200\n",
      "Training Loss: 2.2710390090942383\n",
      "\n",
      "Epoch 4, Batch 225\n",
      "Training Loss: 1.6788023710250854\n",
      "\n",
      "Epoch 4, Batch 250\n",
      "Training Loss: 2.0528149604797363\n",
      "\n",
      "Epoch 4, Batch 275\n",
      "Training Loss: 1.8540153503417969\n",
      "\n",
      "Epoch 4, Batch 300\n",
      "Training Loss: 2.053788900375366\n",
      "\n",
      "Epoch 4, Batch 325\n",
      "Training Loss: 2.0915684700012207\n",
      "\n",
      "Epoch 4, Batch 350\n",
      "Training Loss: 2.138932943344116\n",
      "\n",
      "Epoch 4, Batch 375\n",
      "Training Loss: 2.070681571960449\n",
      "\n",
      "Epoch 4, Batch 400\n",
      "Training Loss: 1.9695371389389038\n",
      "\n",
      "Epoch 4, Batch 425\n",
      "Training Loss: 1.7261240482330322\n",
      "\n",
      "Epoch 4, Batch 450\n",
      "Training Loss: 1.8403360843658447\n",
      "\n",
      "Epoch 4, Batch 475\n",
      "Training Loss: 1.9111241102218628\n",
      "\n",
      "Epoch 4, Batch 500\n",
      "Training Loss: 2.0932624340057373\n",
      "\n",
      "Epoch 4, Batch 525\n",
      "Training Loss: 1.9972056150436401\n",
      "\n",
      "Epoch 4, Batch 550\n",
      "Training Loss: 2.0964789390563965\n",
      "\n",
      "Epoch 4, Batch 575\n",
      "Training Loss: 1.9701625108718872\n",
      "\n",
      "Epoch 4, Batch 600\n",
      "Training Loss: 1.7834159135818481\n",
      "\n",
      "Epoch 4, Batch 625\n",
      "Training Loss: 2.0033066272735596\n",
      "\n",
      "Epoch 4, Batch 650\n",
      "Training Loss: 2.1789066791534424\n",
      "\n",
      "Epoch 4, Batch 675\n",
      "Training Loss: 1.786718487739563\n",
      "\n",
      "Epoch 4, Batch 700\n",
      "Training Loss: 1.7477025985717773\n",
      "\n",
      "Epoch 4, Batch 725\n",
      "Training Loss: 1.7565653324127197\n",
      "\n",
      "Epoch 4, Batch 750\n",
      "Training Loss: 2.040038824081421\n",
      "\n",
      "Epoch 4, Batch 775\n",
      "Training Loss: 1.8453209400177002\n",
      "\n",
      "Epoch 4, Batch 800\n",
      "Training Loss: 2.183387517929077\n",
      "\n",
      "Epoch 4, Batch 825\n",
      "Training Loss: 2.1749942302703857\n",
      "\n",
      "Epoch 4, Batch 850\n",
      "Training Loss: 2.2798759937286377\n",
      "\n",
      "Epoch 4, Batch 875\n",
      "Training Loss: 2.224914789199829\n",
      "\n",
      "Epoch 4, Batch 900\n",
      "Training Loss: 1.888099193572998\n",
      "\n",
      "Epoch 4, Batch 925\n",
      "Training Loss: 2.2249343395233154\n",
      "\n",
      "Epoch 4, Batch 950\n",
      "Training Loss: 2.1115615367889404\n",
      "\n",
      "Epoch 4, Batch 975\n",
      "Training Loss: 1.9252793788909912\n",
      "\n",
      "Epoch 4, Batch 1000\n",
      "Training Loss: 2.2781434059143066\n",
      "\n",
      "Epoch 4, Batch 1025\n",
      "Training Loss: 1.9577155113220215\n",
      "\n",
      "Epoch 4, Batch 1050\n",
      "Training Loss: 2.4087564945220947\n",
      "\n",
      "Epoch 4, Batch 1075\n",
      "Training Loss: 2.076439380645752\n",
      "\n",
      "Epoch 4, Batch 1100\n",
      "Training Loss: 2.253716468811035\n",
      "\n",
      "Epoch 4, Batch 1125\n",
      "Training Loss: 2.1072354316711426\n",
      "\n",
      "Epoch 4, Batch 1150\n",
      "Training Loss: 2.1408603191375732\n",
      "\n",
      "Epoch 4, Batch 1175\n",
      "Training Loss: 2.527634620666504\n",
      "\n",
      "Epoch 4, Batch 1200\n",
      "Training Loss: 2.0188777446746826\n",
      "\n",
      "Epoch 4, Batch 1225\n",
      "Training Loss: 1.9099291563034058\n",
      "\n",
      "Epoch 4, Batch 1250\n",
      "Training Loss: 2.061990261077881\n",
      "\n",
      "Epoch 4, Batch 1275\n",
      "Training Loss: 2.053569793701172\n",
      "\n",
      "Epoch 4, Batch 1300\n",
      "Training Loss: 2.0850377082824707\n",
      "\n",
      "Epoch 4, Batch 1325\n",
      "Training Loss: 2.246938467025757\n",
      "\n",
      "Epoch 4, Batch 1350\n",
      "Training Loss: 2.2162342071533203\n",
      "\n",
      "Epoch 4, Batch 1375\n",
      "Training Loss: 2.0474460124969482\n",
      "\n",
      "Epoch 4, Batch 1400\n",
      "Training Loss: 1.9069223403930664\n",
      "\n",
      "Epoch 4, Batch 1425\n",
      "Training Loss: 2.344491958618164\n",
      "\n",
      "Epoch 4, Batch 1450\n",
      "Training Loss: 1.793484091758728\n",
      "\n",
      "Epoch 4, Batch 1475\n",
      "Training Loss: 1.9966371059417725\n",
      "\n",
      "Epoch 4, Batch 1500\n",
      "Training Loss: 2.013684034347534\n",
      "\n",
      "Epoch 4, Batch 1525\n",
      "Training Loss: 1.7115228176116943\n",
      "\n",
      "Epoch 4, Batch 1550\n",
      "Training Loss: 1.9390239715576172\n",
      "\n",
      "Epoch 4, Batch 1575\n",
      "Training Loss: 2.3704187870025635\n",
      "\n",
      "Epoch 4, Batch 1600\n",
      "Training Loss: 2.383164405822754\n",
      "\n",
      "Epoch 4, Batch 1625\n",
      "Training Loss: 2.093620777130127\n",
      "\n",
      "Epoch 4, Batch 1650\n",
      "Training Loss: 2.034153938293457\n",
      "\n",
      "Epoch 4, Batch 1675\n",
      "Training Loss: 1.990240216255188\n",
      "\n",
      "Epoch 4, Batch 1700\n",
      "Training Loss: 1.8634034395217896\n",
      "\n",
      "Epoch 4, Batch 1725\n",
      "Training Loss: 2.022493600845337\n",
      "\n",
      "Epoch 4, Batch 1750\n",
      "Training Loss: 2.249570846557617\n",
      "\n",
      "Epoch 4, Batch 1775\n",
      "Training Loss: 2.090684413909912\n",
      "\n",
      "Epoch 4, Batch 1800\n",
      "Training Loss: 1.9869595766067505\n",
      "\n",
      "Epoch 4, Batch 1825\n",
      "Training Loss: 1.9959064722061157\n",
      "\n",
      "Epoch 4, Batch 1850\n",
      "Training Loss: 1.997053861618042\n",
      "\n",
      "Epoch 4, Batch 1875\n",
      "Training Loss: 1.9207597970962524\n",
      "\n",
      "Epoch 4, Batch 1900\n",
      "Training Loss: 1.8726301193237305\n",
      "\n",
      "Epoch 4, Batch 1925\n",
      "Training Loss: 2.308164596557617\n",
      "\n",
      "Epoch 4, Batch 1950\n",
      "Training Loss: 2.0126688480377197\n",
      "\n",
      "Epoch 4, Batch 1975\n",
      "Training Loss: 1.858721375465393\n",
      "\n",
      "Epoch 4, Batch 2000\n",
      "Training Loss: 2.356492042541504\n",
      "\n",
      "Epoch 4, Batch 2025\n",
      "Training Loss: 1.7869290113449097\n",
      "\n",
      "Epoch 4, Batch 2050\n",
      "Training Loss: 2.0838475227355957\n",
      "\n",
      "Epoch 4, Batch 2075\n",
      "Training Loss: 1.9014370441436768\n",
      "\n",
      "Epoch 4, Batch 2100\n",
      "Training Loss: 2.411397933959961\n",
      "\n",
      "Epoch 4, Batch 2125\n",
      "Training Loss: 2.0311856269836426\n",
      "\n",
      "Epoch 4, Batch 2150\n",
      "Training Loss: 1.9435160160064697\n",
      "\n",
      "Epoch 4, Batch 2175\n",
      "Training Loss: 2.069838523864746\n",
      "\n",
      "Epoch 4, Batch 2200\n",
      "Training Loss: 2.3852365016937256\n",
      "\n",
      "Epoch 4, Batch 2225\n",
      "Training Loss: 2.138568639755249\n",
      "\n",
      "Epoch 4, Batch 2250\n",
      "Training Loss: 2.057311773300171\n",
      "\n",
      "Epoch 4, Batch 2275\n",
      "Training Loss: 2.0372445583343506\n",
      "\n",
      "Epoch 4, Batch 2300\n",
      "Training Loss: 1.9491428136825562\n",
      "\n",
      "Epoch 4, Batch 2325\n",
      "Training Loss: 2.0773823261260986\n",
      "\n",
      "Epoch 4, Batch 2350\n",
      "Training Loss: 2.287806272506714\n",
      "\n",
      "Epoch 4, Batch 2375\n",
      "Training Loss: 1.941455364227295\n",
      "\n",
      "Epoch 4, Batch 2400\n",
      "Training Loss: 1.8552175760269165\n",
      "\n",
      "Epoch 4, Batch 2425\n",
      "Training Loss: 2.0782387256622314\n",
      "\n",
      "Epoch 4, Batch 2450\n",
      "Training Loss: 1.8302584886550903\n",
      "\n",
      "Epoch 4, Batch 2475\n",
      "Training Loss: 2.1615920066833496\n",
      "\n",
      "Epoch 4, Batch 2500\n",
      "Training Loss: 2.3432137966156006\n",
      "\n",
      "Epoch 4, Batch 2525\n",
      "Training Loss: 2.118860960006714\n",
      "\n",
      "Epoch 4, Batch 2550\n",
      "Training Loss: 2.228733777999878\n",
      "\n",
      "Epoch 4, Batch 2575\n",
      "Training Loss: 1.988527536392212\n",
      "\n",
      "Epoch 4, Batch 2600\n",
      "Training Loss: 2.128713607788086\n",
      "\n",
      "Epoch 4, Batch 2625\n",
      "Training Loss: 1.9024029970169067\n",
      "\n",
      "Epoch 4, Batch 2650\n",
      "Training Loss: 1.9258400201797485\n",
      "\n",
      "Epoch 4, Batch 2675\n",
      "Training Loss: 1.7449601888656616\n",
      "\n",
      "Epoch 4, Batch 2700\n",
      "Training Loss: 1.9084477424621582\n",
      "\n",
      "Epoch 4, Batch 2725\n",
      "Training Loss: 2.561906337738037\n",
      "\n",
      "Epoch 4, Batch 2750\n",
      "Training Loss: 1.8072677850723267\n",
      "\n",
      "Epoch 4, Batch 2775\n",
      "Training Loss: 2.090911626815796\n",
      "\n",
      "Epoch 4, Batch 2800\n",
      "Training Loss: 2.2126572132110596\n",
      "\n",
      "Epoch 4, Batch 2825\n",
      "Training Loss: 2.051544189453125\n",
      "\n",
      "Epoch 4, Batch 2850\n",
      "Training Loss: 1.8540940284729004\n",
      "\n",
      "Epoch 4, Batch 2875\n",
      "Training Loss: 2.143902063369751\n",
      "\n",
      "Epoch 4, Batch 2900\n",
      "Training Loss: 2.124748945236206\n",
      "\n",
      "Epoch 4, Batch 2925\n",
      "Training Loss: 1.9578616619110107\n",
      "\n",
      "Epoch 4, Batch 2950\n",
      "Training Loss: 2.2216804027557373\n",
      "\n",
      "Epoch 4/20\n",
      "Training Loss: 2.0293116569519043\n",
      "\n",
      "Epoch 5, Batch 0\n",
      "Training Loss: 1.6925619840621948\n",
      "\n",
      "Epoch 5, Batch 25\n",
      "Training Loss: 2.12007212638855\n",
      "\n",
      "Epoch 5, Batch 50\n",
      "Training Loss: 2.0656256675720215\n",
      "\n",
      "Epoch 5, Batch 75\n",
      "Training Loss: 2.256788730621338\n",
      "\n",
      "Epoch 5, Batch 100\n",
      "Training Loss: 1.9647419452667236\n",
      "\n",
      "Epoch 5, Batch 125\n",
      "Training Loss: 2.1631124019622803\n",
      "\n",
      "Epoch 5, Batch 150\n",
      "Training Loss: 2.041837692260742\n",
      "\n",
      "Epoch 5, Batch 175\n",
      "Training Loss: 1.8269271850585938\n",
      "\n",
      "Epoch 5, Batch 200\n",
      "Training Loss: 1.6906853914260864\n",
      "\n",
      "Epoch 5, Batch 225\n",
      "Training Loss: 1.6689740419387817\n",
      "\n",
      "Epoch 5, Batch 250\n",
      "Training Loss: 2.1730358600616455\n",
      "\n",
      "Epoch 5, Batch 275\n",
      "Training Loss: 2.152374505996704\n",
      "\n",
      "Epoch 5, Batch 300\n",
      "Training Loss: 2.1294403076171875\n",
      "\n",
      "Epoch 5, Batch 325\n",
      "Training Loss: 1.7366766929626465\n",
      "\n",
      "Epoch 5, Batch 350\n",
      "Training Loss: 1.7196040153503418\n",
      "\n",
      "Epoch 5, Batch 375\n",
      "Training Loss: 1.6684765815734863\n",
      "\n",
      "Epoch 5, Batch 400\n",
      "Training Loss: 1.770278811454773\n",
      "\n",
      "Epoch 5, Batch 425\n",
      "Training Loss: 2.2482264041900635\n",
      "\n",
      "Epoch 5, Batch 450\n",
      "Training Loss: 1.8781253099441528\n",
      "\n",
      "Epoch 5, Batch 475\n",
      "Training Loss: 1.843990683555603\n",
      "\n",
      "Epoch 5, Batch 500\n",
      "Training Loss: 2.084439516067505\n",
      "\n",
      "Epoch 5, Batch 525\n",
      "Training Loss: 1.8597331047058105\n",
      "\n",
      "Epoch 5, Batch 550\n",
      "Training Loss: 2.2122466564178467\n",
      "\n",
      "Epoch 5, Batch 575\n",
      "Training Loss: 2.458451509475708\n",
      "\n",
      "Epoch 5, Batch 600\n",
      "Training Loss: 2.291350841522217\n",
      "\n",
      "Epoch 5, Batch 625\n",
      "Training Loss: 1.935714840888977\n",
      "\n",
      "Epoch 5, Batch 650\n",
      "Training Loss: 1.449135661125183\n",
      "\n",
      "Epoch 5, Batch 675\n",
      "Training Loss: 1.9670941829681396\n",
      "\n",
      "Epoch 5, Batch 700\n",
      "Training Loss: 2.1695845127105713\n",
      "\n",
      "Epoch 5, Batch 725\n",
      "Training Loss: 1.8297322988510132\n",
      "\n",
      "Epoch 5, Batch 750\n",
      "Training Loss: 1.9632978439331055\n",
      "\n",
      "Epoch 5, Batch 775\n",
      "Training Loss: 2.1552345752716064\n",
      "\n",
      "Epoch 5, Batch 800\n",
      "Training Loss: 1.6958601474761963\n",
      "\n",
      "Epoch 5, Batch 825\n",
      "Training Loss: 1.9912139177322388\n",
      "\n",
      "Epoch 5, Batch 850\n",
      "Training Loss: 1.8179831504821777\n",
      "\n",
      "Epoch 5, Batch 875\n",
      "Training Loss: 2.255272626876831\n",
      "\n",
      "Epoch 5, Batch 900\n",
      "Training Loss: 2.1618406772613525\n",
      "\n",
      "Epoch 5, Batch 925\n",
      "Training Loss: 2.1443779468536377\n",
      "\n",
      "Epoch 5, Batch 950\n",
      "Training Loss: 2.0615134239196777\n",
      "\n",
      "Epoch 5, Batch 975\n",
      "Training Loss: 2.0264394283294678\n",
      "\n",
      "Epoch 5, Batch 1000\n",
      "Training Loss: 1.8505477905273438\n",
      "\n",
      "Epoch 5, Batch 1025\n",
      "Training Loss: 2.1762359142303467\n",
      "\n",
      "Epoch 5, Batch 1050\n",
      "Training Loss: 2.109943151473999\n",
      "\n",
      "Epoch 5, Batch 1075\n",
      "Training Loss: 1.9863133430480957\n",
      "\n",
      "Epoch 5, Batch 1100\n",
      "Training Loss: 1.9384112358093262\n",
      "\n",
      "Epoch 5, Batch 1125\n",
      "Training Loss: 1.9398506879806519\n",
      "\n",
      "Epoch 5, Batch 1150\n",
      "Training Loss: 1.874329924583435\n",
      "\n",
      "Epoch 5, Batch 1175\n",
      "Training Loss: 2.1086549758911133\n",
      "\n",
      "Epoch 5, Batch 1200\n",
      "Training Loss: 1.78846275806427\n",
      "\n",
      "Epoch 5, Batch 1225\n",
      "Training Loss: 2.1815884113311768\n",
      "\n",
      "Epoch 5, Batch 1250\n",
      "Training Loss: 1.9110729694366455\n",
      "\n",
      "Epoch 5, Batch 1275\n",
      "Training Loss: 2.119344711303711\n",
      "\n",
      "Epoch 5, Batch 1300\n",
      "Training Loss: 1.9785691499710083\n",
      "\n",
      "Epoch 5, Batch 1325\n",
      "Training Loss: 1.9860539436340332\n",
      "\n",
      "Epoch 5, Batch 1350\n",
      "Training Loss: 1.853335976600647\n",
      "\n",
      "Epoch 5, Batch 1375\n",
      "Training Loss: 1.9640991687774658\n",
      "\n",
      "Epoch 5, Batch 1400\n",
      "Training Loss: 2.034425973892212\n",
      "\n",
      "Epoch 5, Batch 1425\n",
      "Training Loss: 1.7789510488510132\n",
      "\n",
      "Epoch 5, Batch 1450\n",
      "Training Loss: 1.959223985671997\n",
      "\n",
      "Epoch 5, Batch 1475\n",
      "Training Loss: 2.0434277057647705\n",
      "\n",
      "Epoch 5, Batch 1500\n",
      "Training Loss: 2.1289093494415283\n",
      "\n",
      "Epoch 5, Batch 1525\n",
      "Training Loss: 1.4844902753829956\n",
      "\n",
      "Epoch 5, Batch 1550\n",
      "Training Loss: 1.9080268144607544\n",
      "\n",
      "Epoch 5, Batch 1575\n",
      "Training Loss: 2.163010597229004\n",
      "\n",
      "Epoch 5, Batch 1600\n",
      "Training Loss: 2.0561816692352295\n",
      "\n",
      "Epoch 5, Batch 1625\n",
      "Training Loss: 2.070265054702759\n",
      "\n",
      "Epoch 5, Batch 1650\n",
      "Training Loss: 2.029956340789795\n",
      "\n",
      "Epoch 5, Batch 1675\n",
      "Training Loss: 2.1215243339538574\n",
      "\n",
      "Epoch 5, Batch 1700\n",
      "Training Loss: 2.0225062370300293\n",
      "\n",
      "Epoch 5, Batch 1725\n",
      "Training Loss: 2.1377408504486084\n",
      "\n",
      "Epoch 5, Batch 1750\n",
      "Training Loss: 1.7015466690063477\n",
      "\n",
      "Epoch 5, Batch 1775\n",
      "Training Loss: 2.083982467651367\n",
      "\n",
      "Epoch 5, Batch 1800\n",
      "Training Loss: 1.7453234195709229\n",
      "\n",
      "Epoch 5, Batch 1825\n",
      "Training Loss: 1.800302267074585\n",
      "\n",
      "Epoch 5, Batch 1850\n",
      "Training Loss: 2.1343884468078613\n",
      "\n",
      "Epoch 5, Batch 1875\n",
      "Training Loss: 1.867539644241333\n",
      "\n",
      "Epoch 5, Batch 1900\n",
      "Training Loss: 2.156477451324463\n",
      "\n",
      "Epoch 5, Batch 1925\n",
      "Training Loss: 1.9504550695419312\n",
      "\n",
      "Epoch 5, Batch 1950\n",
      "Training Loss: 1.7744039297103882\n",
      "\n",
      "Epoch 5, Batch 1975\n",
      "Training Loss: 1.9832968711853027\n",
      "\n",
      "Epoch 5, Batch 2000\n",
      "Training Loss: 2.045900583267212\n",
      "\n",
      "Epoch 5, Batch 2025\n",
      "Training Loss: 1.8051494359970093\n",
      "\n",
      "Epoch 5, Batch 2050\n",
      "Training Loss: 1.972378134727478\n",
      "\n",
      "Epoch 5, Batch 2075\n",
      "Training Loss: 1.6588407754898071\n",
      "\n",
      "Epoch 5, Batch 2100\n",
      "Training Loss: 1.6276944875717163\n",
      "\n",
      "Epoch 5, Batch 2125\n",
      "Training Loss: 2.020907163619995\n",
      "\n",
      "Epoch 5, Batch 2150\n",
      "Training Loss: 1.8659368753433228\n",
      "\n",
      "Epoch 5, Batch 2175\n",
      "Training Loss: 1.7451943159103394\n",
      "\n",
      "Epoch 5, Batch 2200\n",
      "Training Loss: 1.9278039932250977\n",
      "\n",
      "Epoch 5, Batch 2225\n",
      "Training Loss: 1.8167736530303955\n",
      "\n",
      "Epoch 5, Batch 2250\n",
      "Training Loss: 2.021760940551758\n",
      "\n",
      "Epoch 5, Batch 2275\n",
      "Training Loss: 1.965341329574585\n",
      "\n",
      "Epoch 5, Batch 2300\n",
      "Training Loss: 1.8989819288253784\n",
      "\n",
      "Epoch 5, Batch 2325\n",
      "Training Loss: 1.970058560371399\n",
      "\n",
      "Epoch 5, Batch 2350\n",
      "Training Loss: 1.9233628511428833\n",
      "\n",
      "Epoch 5, Batch 2375\n",
      "Training Loss: 1.8260079622268677\n",
      "\n",
      "Epoch 5, Batch 2400\n",
      "Training Loss: 1.9332947731018066\n",
      "\n",
      "Epoch 5, Batch 2425\n",
      "Training Loss: 1.710349678993225\n",
      "\n",
      "Epoch 5, Batch 2450\n",
      "Training Loss: 1.7864359617233276\n",
      "\n",
      "Epoch 5, Batch 2475\n",
      "Training Loss: 2.2752623558044434\n",
      "\n",
      "Epoch 5, Batch 2500\n",
      "Training Loss: 2.0194060802459717\n",
      "\n",
      "Epoch 5, Batch 2525\n",
      "Training Loss: 2.009314775466919\n",
      "\n",
      "Epoch 5, Batch 2550\n",
      "Training Loss: 2.0198376178741455\n",
      "\n",
      "Epoch 5, Batch 2575\n",
      "Training Loss: 1.9358474016189575\n",
      "\n",
      "Epoch 5, Batch 2600\n",
      "Training Loss: 1.995426058769226\n",
      "\n",
      "Epoch 5, Batch 2625\n",
      "Training Loss: 1.9211125373840332\n",
      "\n",
      "Epoch 5, Batch 2650\n",
      "Training Loss: 1.952587366104126\n",
      "\n",
      "Epoch 5, Batch 2675\n",
      "Training Loss: 1.7237054109573364\n",
      "\n",
      "Epoch 5, Batch 2700\n",
      "Training Loss: 1.6031218767166138\n",
      "\n",
      "Epoch 5, Batch 2725\n",
      "Training Loss: 1.6121331453323364\n",
      "\n",
      "Epoch 5, Batch 2750\n",
      "Training Loss: 1.7860653400421143\n",
      "\n",
      "Epoch 5, Batch 2775\n",
      "Training Loss: 1.9078584909439087\n",
      "\n",
      "Epoch 5, Batch 2800\n",
      "Training Loss: 1.9885530471801758\n",
      "\n",
      "Epoch 5, Batch 2825\n",
      "Training Loss: 2.146451950073242\n",
      "\n",
      "Epoch 5, Batch 2850\n",
      "Training Loss: 1.8346202373504639\n",
      "\n",
      "Epoch 5, Batch 2875\n",
      "Training Loss: 2.1323094367980957\n",
      "\n",
      "Epoch 5, Batch 2900\n",
      "Training Loss: 2.1717753410339355\n",
      "\n",
      "Epoch 5, Batch 2925\n",
      "Training Loss: 1.7452517747879028\n",
      "\n",
      "Epoch 5, Batch 2950\n",
      "Training Loss: 1.7307294607162476\n",
      "\n",
      "Epoch 5/20\n",
      "Training Loss: 1.8238919973373413\n",
      "\n",
      "Epoch 6, Batch 0\n",
      "Training Loss: 1.8304738998413086\n",
      "\n",
      "Epoch 6, Batch 25\n",
      "Training Loss: 1.5202375650405884\n",
      "\n",
      "Epoch 6, Batch 50\n",
      "Training Loss: 1.8838118314743042\n",
      "\n",
      "Epoch 6, Batch 75\n",
      "Training Loss: 1.9637260437011719\n",
      "\n",
      "Epoch 6, Batch 100\n",
      "Training Loss: 1.982468843460083\n",
      "\n",
      "Epoch 6, Batch 125\n",
      "Training Loss: 1.8915002346038818\n",
      "\n",
      "Epoch 6, Batch 150\n",
      "Training Loss: 1.9089027643203735\n",
      "\n",
      "Epoch 6, Batch 175\n",
      "Training Loss: 1.7072826623916626\n",
      "\n",
      "Epoch 6, Batch 200\n",
      "Training Loss: 1.6565312147140503\n",
      "\n",
      "Epoch 6, Batch 225\n",
      "Training Loss: 1.8503642082214355\n",
      "\n",
      "Epoch 6, Batch 250\n",
      "Training Loss: 1.962056040763855\n",
      "\n",
      "Epoch 6, Batch 275\n",
      "Training Loss: 1.3850971460342407\n",
      "\n",
      "Epoch 6, Batch 300\n",
      "Training Loss: 1.9503010511398315\n",
      "\n",
      "Epoch 6, Batch 325\n",
      "Training Loss: 1.6337584257125854\n",
      "\n",
      "Epoch 6, Batch 350\n",
      "Training Loss: 1.9331611394882202\n",
      "\n",
      "Epoch 6, Batch 375\n",
      "Training Loss: 1.896422028541565\n",
      "\n",
      "Epoch 6, Batch 400\n",
      "Training Loss: 1.9537190198898315\n",
      "\n",
      "Epoch 6, Batch 425\n",
      "Training Loss: 1.463106393814087\n",
      "\n",
      "Epoch 6, Batch 450\n",
      "Training Loss: 1.9176197052001953\n",
      "\n",
      "Epoch 6, Batch 475\n",
      "Training Loss: 2.1631972789764404\n",
      "\n",
      "Epoch 6, Batch 500\n",
      "Training Loss: 1.9977494478225708\n",
      "\n",
      "Epoch 6, Batch 525\n",
      "Training Loss: 1.7981009483337402\n",
      "\n",
      "Epoch 6, Batch 550\n",
      "Training Loss: 1.9718754291534424\n",
      "\n",
      "Epoch 6, Batch 575\n",
      "Training Loss: 2.01162052154541\n",
      "\n",
      "Epoch 6, Batch 600\n",
      "Training Loss: 1.9675613641738892\n",
      "\n",
      "Epoch 6, Batch 625\n",
      "Training Loss: 2.0296356678009033\n",
      "\n",
      "Epoch 6, Batch 650\n",
      "Training Loss: 1.9149158000946045\n",
      "\n",
      "Epoch 6, Batch 675\n",
      "Training Loss: 2.0652263164520264\n",
      "\n",
      "Epoch 6, Batch 700\n",
      "Training Loss: 1.9075130224227905\n",
      "\n",
      "Epoch 6, Batch 725\n",
      "Training Loss: 2.0754778385162354\n",
      "\n",
      "Epoch 6, Batch 750\n",
      "Training Loss: 2.015286684036255\n",
      "\n",
      "Epoch 6, Batch 775\n",
      "Training Loss: 2.147042751312256\n",
      "\n",
      "Epoch 6, Batch 800\n",
      "Training Loss: 1.788917899131775\n",
      "\n",
      "Epoch 6, Batch 825\n",
      "Training Loss: 1.647099256515503\n",
      "\n",
      "Epoch 6, Batch 850\n",
      "Training Loss: 1.8605636358261108\n",
      "\n",
      "Epoch 6, Batch 875\n",
      "Training Loss: 1.9535257816314697\n",
      "\n",
      "Epoch 6, Batch 900\n",
      "Training Loss: 1.5790430307388306\n",
      "\n",
      "Epoch 6, Batch 925\n",
      "Training Loss: 2.0136430263519287\n",
      "\n",
      "Epoch 6, Batch 950\n",
      "Training Loss: 1.6282693147659302\n",
      "\n",
      "Epoch 6, Batch 975\n",
      "Training Loss: 1.8413680791854858\n",
      "\n",
      "Epoch 6, Batch 1000\n",
      "Training Loss: 1.7392138242721558\n",
      "\n",
      "Epoch 6, Batch 1025\n",
      "Training Loss: 1.7587871551513672\n",
      "\n",
      "Epoch 6, Batch 1050\n",
      "Training Loss: 1.945330023765564\n",
      "\n",
      "Epoch 6, Batch 1075\n",
      "Training Loss: 1.9389066696166992\n",
      "\n",
      "Epoch 6, Batch 1100\n",
      "Training Loss: 1.6592731475830078\n",
      "\n",
      "Epoch 6, Batch 1125\n",
      "Training Loss: 1.689371943473816\n",
      "\n",
      "Epoch 6, Batch 1150\n",
      "Training Loss: 1.6080338954925537\n",
      "\n",
      "Epoch 6, Batch 1175\n",
      "Training Loss: 1.8056914806365967\n",
      "\n",
      "Epoch 6, Batch 1200\n",
      "Training Loss: 2.129401922225952\n",
      "\n",
      "Epoch 6, Batch 1225\n",
      "Training Loss: 2.1828479766845703\n",
      "\n",
      "Epoch 6, Batch 1250\n",
      "Training Loss: 1.4236117601394653\n",
      "\n",
      "Epoch 6, Batch 1275\n",
      "Training Loss: 1.6990418434143066\n",
      "\n",
      "Epoch 6, Batch 1300\n",
      "Training Loss: 2.084531307220459\n",
      "\n",
      "Epoch 6, Batch 1325\n",
      "Training Loss: 1.540272831916809\n",
      "\n",
      "Epoch 6, Batch 1350\n",
      "Training Loss: 1.9425655603408813\n",
      "\n",
      "Epoch 6, Batch 1375\n",
      "Training Loss: 1.8843052387237549\n",
      "\n",
      "Epoch 6, Batch 1400\n",
      "Training Loss: 2.141495943069458\n",
      "\n",
      "Epoch 6, Batch 1425\n",
      "Training Loss: 2.0778744220733643\n",
      "\n",
      "Epoch 6, Batch 1450\n",
      "Training Loss: 2.1391475200653076\n",
      "\n",
      "Epoch 6, Batch 1475\n",
      "Training Loss: 1.7846046686172485\n",
      "\n",
      "Epoch 6, Batch 1500\n",
      "Training Loss: 1.8519777059555054\n",
      "\n",
      "Epoch 6, Batch 1525\n",
      "Training Loss: 1.7609189748764038\n",
      "\n",
      "Epoch 6, Batch 1550\n",
      "Training Loss: 1.9289129972457886\n",
      "\n",
      "Epoch 6, Batch 1575\n",
      "Training Loss: 1.671654462814331\n",
      "\n",
      "Epoch 6, Batch 1600\n",
      "Training Loss: 2.199101686477661\n",
      "\n",
      "Epoch 6, Batch 1625\n",
      "Training Loss: 1.7489204406738281\n",
      "\n",
      "Epoch 6, Batch 1650\n",
      "Training Loss: 1.9102318286895752\n",
      "\n",
      "Epoch 6, Batch 1675\n",
      "Training Loss: 1.8396682739257812\n",
      "\n",
      "Epoch 6, Batch 1700\n",
      "Training Loss: 2.127073287963867\n",
      "\n",
      "Epoch 6, Batch 1725\n",
      "Training Loss: 2.0563576221466064\n",
      "\n",
      "Epoch 6, Batch 1750\n",
      "Training Loss: 2.0745015144348145\n",
      "\n",
      "Epoch 6, Batch 1775\n",
      "Training Loss: 2.065622091293335\n",
      "\n",
      "Epoch 6, Batch 1800\n",
      "Training Loss: 1.957817792892456\n",
      "\n",
      "Epoch 6, Batch 1825\n",
      "Training Loss: 2.182448148727417\n",
      "\n",
      "Epoch 6, Batch 1850\n",
      "Training Loss: 2.1980395317077637\n",
      "\n",
      "Epoch 6, Batch 1875\n",
      "Training Loss: 2.3186168670654297\n",
      "\n",
      "Epoch 6, Batch 1900\n",
      "Training Loss: 1.7780420780181885\n",
      "\n",
      "Epoch 6, Batch 1925\n",
      "Training Loss: 2.1566872596740723\n",
      "\n",
      "Epoch 6, Batch 1950\n",
      "Training Loss: 1.7317779064178467\n",
      "\n",
      "Epoch 6, Batch 1975\n",
      "Training Loss: 1.889204502105713\n",
      "\n",
      "Epoch 6, Batch 2000\n",
      "Training Loss: 1.7213600873947144\n",
      "\n",
      "Epoch 6, Batch 2025\n",
      "Training Loss: 1.9841718673706055\n",
      "\n",
      "Epoch 6, Batch 2050\n",
      "Training Loss: 2.2526769638061523\n",
      "\n",
      "Epoch 6, Batch 2075\n",
      "Training Loss: 2.313197612762451\n",
      "\n",
      "Epoch 6, Batch 2100\n",
      "Training Loss: 1.7595607042312622\n",
      "\n",
      "Epoch 6, Batch 2125\n",
      "Training Loss: 1.7367514371871948\n",
      "\n",
      "Epoch 6, Batch 2150\n",
      "Training Loss: 1.9097570180892944\n",
      "\n",
      "Epoch 6, Batch 2175\n",
      "Training Loss: 2.023850440979004\n",
      "\n",
      "Epoch 6, Batch 2200\n",
      "Training Loss: 2.021535873413086\n",
      "\n",
      "Epoch 6, Batch 2225\n",
      "Training Loss: 1.687274694442749\n",
      "\n",
      "Epoch 6, Batch 2250\n",
      "Training Loss: 1.594443917274475\n",
      "\n",
      "Epoch 6, Batch 2275\n",
      "Training Loss: 1.7237954139709473\n",
      "\n",
      "Epoch 6, Batch 2300\n",
      "Training Loss: 1.8221735954284668\n",
      "\n",
      "Epoch 6, Batch 2325\n",
      "Training Loss: 2.0934720039367676\n",
      "\n",
      "Epoch 6, Batch 2350\n",
      "Training Loss: 2.0407745838165283\n",
      "\n",
      "Epoch 6, Batch 2375\n",
      "Training Loss: 2.034334421157837\n",
      "\n",
      "Epoch 6, Batch 2400\n",
      "Training Loss: 1.6258689165115356\n",
      "\n",
      "Epoch 6, Batch 2425\n",
      "Training Loss: 1.8180755376815796\n",
      "\n",
      "Epoch 6, Batch 2450\n",
      "Training Loss: 1.9152382612228394\n",
      "\n",
      "Epoch 6, Batch 2475\n",
      "Training Loss: 1.7166281938552856\n",
      "\n",
      "Epoch 6, Batch 2500\n",
      "Training Loss: 1.892409324645996\n",
      "\n",
      "Epoch 6, Batch 2525\n",
      "Training Loss: 1.7456895112991333\n",
      "\n",
      "Epoch 6, Batch 2550\n",
      "Training Loss: 1.6059218645095825\n",
      "\n",
      "Epoch 6, Batch 2575\n",
      "Training Loss: 2.019388198852539\n",
      "\n",
      "Epoch 6, Batch 2600\n",
      "Training Loss: 1.7146979570388794\n",
      "\n",
      "Epoch 6, Batch 2625\n",
      "Training Loss: 1.6219302415847778\n",
      "\n",
      "Epoch 6, Batch 2650\n",
      "Training Loss: 1.627161979675293\n",
      "\n",
      "Epoch 6, Batch 2675\n",
      "Training Loss: 2.1482858657836914\n",
      "\n",
      "Epoch 6, Batch 2700\n",
      "Training Loss: 2.1285338401794434\n",
      "\n",
      "Epoch 6, Batch 2725\n",
      "Training Loss: 1.8448151350021362\n",
      "\n",
      "Epoch 6, Batch 2750\n",
      "Training Loss: 1.9566982984542847\n",
      "\n",
      "Epoch 6, Batch 2775\n",
      "Training Loss: 2.0577197074890137\n",
      "\n",
      "Epoch 6, Batch 2800\n",
      "Training Loss: 2.185121536254883\n",
      "\n",
      "Epoch 6, Batch 2825\n",
      "Training Loss: 1.987576961517334\n",
      "\n",
      "Epoch 6, Batch 2850\n",
      "Training Loss: 2.0562546253204346\n",
      "\n",
      "Epoch 6, Batch 2875\n",
      "Training Loss: 1.8689130544662476\n",
      "\n",
      "Epoch 6, Batch 2900\n",
      "Training Loss: 1.6113373041152954\n",
      "\n",
      "Epoch 6, Batch 2925\n",
      "Training Loss: 1.7900832891464233\n",
      "\n",
      "Epoch 6, Batch 2950\n",
      "Training Loss: 1.7384490966796875\n",
      "\n",
      "Epoch 6/20\n",
      "Training Loss: 1.707499384880066\n",
      "\n",
      "Epoch 7, Batch 0\n",
      "Training Loss: 1.881822943687439\n",
      "\n",
      "Epoch 7, Batch 25\n",
      "Training Loss: 1.7115083932876587\n",
      "\n",
      "Epoch 7, Batch 50\n",
      "Training Loss: 2.387986660003662\n",
      "\n",
      "Epoch 7, Batch 75\n",
      "Training Loss: 1.6874430179595947\n",
      "\n",
      "Epoch 7, Batch 100\n",
      "Training Loss: 2.063800096511841\n",
      "\n",
      "Epoch 7, Batch 125\n",
      "Training Loss: 1.7801849842071533\n",
      "\n",
      "Epoch 7, Batch 150\n",
      "Training Loss: 1.5368728637695312\n",
      "\n",
      "Epoch 7, Batch 175\n",
      "Training Loss: 1.7930092811584473\n",
      "\n",
      "Epoch 7, Batch 200\n",
      "Training Loss: 2.005645751953125\n",
      "\n",
      "Epoch 7, Batch 225\n",
      "Training Loss: 1.8147269487380981\n",
      "\n",
      "Epoch 7, Batch 250\n",
      "Training Loss: 1.8525556325912476\n",
      "\n",
      "Epoch 7, Batch 275\n",
      "Training Loss: 1.6845115423202515\n",
      "\n",
      "Epoch 7, Batch 300\n",
      "Training Loss: 1.837332844734192\n",
      "\n",
      "Epoch 7, Batch 325\n",
      "Training Loss: 1.9671835899353027\n",
      "\n",
      "Epoch 7, Batch 350\n",
      "Training Loss: 1.8170044422149658\n",
      "\n",
      "Epoch 7, Batch 375\n",
      "Training Loss: 2.026642084121704\n",
      "\n",
      "Epoch 7, Batch 400\n",
      "Training Loss: 1.7474020719528198\n",
      "\n",
      "Epoch 7, Batch 425\n",
      "Training Loss: 1.6704550981521606\n",
      "\n",
      "Epoch 7, Batch 450\n",
      "Training Loss: 2.4439711570739746\n",
      "\n",
      "Epoch 7, Batch 475\n",
      "Training Loss: 1.5442333221435547\n",
      "\n",
      "Epoch 7, Batch 500\n",
      "Training Loss: 1.9087220430374146\n",
      "\n",
      "Epoch 7, Batch 525\n",
      "Training Loss: 1.9061689376831055\n",
      "\n",
      "Epoch 7, Batch 550\n",
      "Training Loss: 2.242849111557007\n",
      "\n",
      "Epoch 7, Batch 575\n",
      "Training Loss: 1.9652475118637085\n",
      "\n",
      "Epoch 7, Batch 600\n",
      "Training Loss: 1.863913893699646\n",
      "\n",
      "Epoch 7, Batch 625\n",
      "Training Loss: 1.888028860092163\n",
      "\n",
      "Epoch 7, Batch 650\n",
      "Training Loss: 1.5239378213882446\n",
      "\n",
      "Epoch 7, Batch 675\n",
      "Training Loss: 1.8385423421859741\n",
      "\n",
      "Epoch 7, Batch 700\n",
      "Training Loss: 1.7573885917663574\n",
      "\n",
      "Epoch 7, Batch 725\n",
      "Training Loss: 1.5982921123504639\n",
      "\n",
      "Epoch 7, Batch 750\n",
      "Training Loss: 2.1708295345306396\n",
      "\n",
      "Epoch 7, Batch 775\n",
      "Training Loss: 1.7131129503250122\n",
      "\n",
      "Epoch 7, Batch 800\n",
      "Training Loss: 2.1250691413879395\n",
      "\n",
      "Epoch 7, Batch 825\n",
      "Training Loss: 1.9223872423171997\n",
      "\n",
      "Epoch 7, Batch 850\n",
      "Training Loss: 1.9335697889328003\n",
      "\n",
      "Epoch 7, Batch 875\n",
      "Training Loss: 1.5730655193328857\n",
      "\n",
      "Epoch 7, Batch 900\n",
      "Training Loss: 2.072204113006592\n",
      "\n",
      "Epoch 7, Batch 925\n",
      "Training Loss: 1.8184601068496704\n",
      "\n",
      "Epoch 7, Batch 950\n",
      "Training Loss: 1.5834966897964478\n",
      "\n",
      "Epoch 7, Batch 975\n",
      "Training Loss: 2.056785821914673\n",
      "\n",
      "Epoch 7, Batch 1000\n",
      "Training Loss: 1.6225205659866333\n",
      "\n",
      "Epoch 7, Batch 1025\n",
      "Training Loss: 2.0235512256622314\n",
      "\n",
      "Epoch 7, Batch 1050\n",
      "Training Loss: 1.7843376398086548\n",
      "\n",
      "Epoch 7, Batch 1075\n",
      "Training Loss: 1.8271273374557495\n",
      "\n",
      "Epoch 7, Batch 1100\n",
      "Training Loss: 2.031585454940796\n",
      "\n",
      "Epoch 7, Batch 1125\n",
      "Training Loss: 1.7961480617523193\n",
      "\n",
      "Epoch 7, Batch 1150\n",
      "Training Loss: 1.821600079536438\n",
      "\n",
      "Epoch 7, Batch 1175\n",
      "Training Loss: 2.061627149581909\n",
      "\n",
      "Epoch 7, Batch 1200\n",
      "Training Loss: 1.992276906967163\n",
      "\n",
      "Epoch 7, Batch 1225\n",
      "Training Loss: 2.2248361110687256\n",
      "\n",
      "Epoch 7, Batch 1250\n",
      "Training Loss: 1.9913437366485596\n",
      "\n",
      "Epoch 7, Batch 1275\n",
      "Training Loss: 1.9555882215499878\n",
      "\n",
      "Epoch 7, Batch 1300\n",
      "Training Loss: 1.8707129955291748\n",
      "\n",
      "Epoch 7, Batch 1325\n",
      "Training Loss: 1.3660799264907837\n",
      "\n",
      "Epoch 7, Batch 1350\n",
      "Training Loss: 1.8077621459960938\n",
      "\n",
      "Epoch 7, Batch 1375\n",
      "Training Loss: 1.653334617614746\n",
      "\n",
      "Epoch 7, Batch 1400\n",
      "Training Loss: 2.1539206504821777\n",
      "\n",
      "Epoch 7, Batch 1425\n",
      "Training Loss: 1.937986969947815\n",
      "\n",
      "Epoch 7, Batch 1450\n",
      "Training Loss: 1.7554582357406616\n",
      "\n",
      "Epoch 7, Batch 1475\n",
      "Training Loss: 1.8467944860458374\n",
      "\n",
      "Epoch 7, Batch 1500\n",
      "Training Loss: 1.7765928506851196\n",
      "\n",
      "Epoch 7, Batch 1525\n",
      "Training Loss: 1.6377085447311401\n",
      "\n",
      "Epoch 7, Batch 1550\n",
      "Training Loss: 2.1739583015441895\n",
      "\n",
      "Epoch 7, Batch 1575\n",
      "Training Loss: 1.6332898139953613\n",
      "\n",
      "Epoch 7, Batch 1600\n",
      "Training Loss: 1.6677912473678589\n",
      "\n",
      "Epoch 7, Batch 1625\n",
      "Training Loss: 1.9568957090377808\n",
      "\n",
      "Epoch 7, Batch 1650\n",
      "Training Loss: 1.9780352115631104\n",
      "\n",
      "Epoch 7, Batch 1675\n",
      "Training Loss: 2.0420639514923096\n",
      "\n",
      "Epoch 7, Batch 1700\n",
      "Training Loss: 1.820120096206665\n",
      "\n",
      "Epoch 7, Batch 1725\n",
      "Training Loss: 1.87648344039917\n",
      "\n",
      "Epoch 7, Batch 1750\n",
      "Training Loss: 1.6736444234848022\n",
      "\n",
      "Epoch 7, Batch 1775\n",
      "Training Loss: 1.7452735900878906\n",
      "\n",
      "Epoch 7, Batch 1800\n",
      "Training Loss: 2.2316503524780273\n",
      "\n",
      "Epoch 7, Batch 1825\n",
      "Training Loss: 1.655104398727417\n",
      "\n",
      "Epoch 7, Batch 1850\n",
      "Training Loss: 1.8759430646896362\n",
      "\n",
      "Epoch 7, Batch 1875\n",
      "Training Loss: 1.7243367433547974\n",
      "\n",
      "Epoch 7, Batch 1900\n",
      "Training Loss: 1.697445034980774\n",
      "\n",
      "Epoch 7, Batch 1925\n",
      "Training Loss: 1.7896180152893066\n",
      "\n",
      "Epoch 7, Batch 1950\n",
      "Training Loss: 2.1471824645996094\n",
      "\n",
      "Epoch 7, Batch 1975\n",
      "Training Loss: 2.0186445713043213\n",
      "\n",
      "Epoch 7, Batch 2000\n",
      "Training Loss: 2.13771390914917\n",
      "\n",
      "Epoch 7, Batch 2025\n",
      "Training Loss: 1.9481534957885742\n",
      "\n",
      "Epoch 7, Batch 2050\n",
      "Training Loss: 2.005316972732544\n",
      "\n",
      "Epoch 7, Batch 2075\n",
      "Training Loss: 1.925832986831665\n",
      "\n",
      "Epoch 7, Batch 2100\n",
      "Training Loss: 1.6873184442520142\n",
      "\n",
      "Epoch 7, Batch 2125\n",
      "Training Loss: 2.2551956176757812\n",
      "\n",
      "Epoch 7, Batch 2150\n",
      "Training Loss: 1.7346023321151733\n",
      "\n",
      "Epoch 7, Batch 2175\n",
      "Training Loss: 1.9262381792068481\n",
      "\n",
      "Epoch 7, Batch 2200\n",
      "Training Loss: 1.7718249559402466\n",
      "\n",
      "Epoch 7, Batch 2225\n",
      "Training Loss: 1.6347955465316772\n",
      "\n",
      "Epoch 7, Batch 2250\n",
      "Training Loss: 1.641849160194397\n",
      "\n",
      "Epoch 7, Batch 2275\n",
      "Training Loss: 1.7140668630599976\n",
      "\n",
      "Epoch 7, Batch 2300\n",
      "Training Loss: 1.9816279411315918\n",
      "\n",
      "Epoch 7, Batch 2325\n",
      "Training Loss: 1.8141223192214966\n",
      "\n",
      "Epoch 7, Batch 2350\n",
      "Training Loss: 2.1043875217437744\n",
      "\n",
      "Epoch 7, Batch 2375\n",
      "Training Loss: 1.4676138162612915\n",
      "\n",
      "Epoch 7, Batch 2400\n",
      "Training Loss: 1.6305819749832153\n",
      "\n",
      "Epoch 7, Batch 2425\n",
      "Training Loss: 1.5640008449554443\n",
      "\n",
      "Epoch 7, Batch 2450\n",
      "Training Loss: 1.969828724861145\n",
      "\n",
      "Epoch 7, Batch 2475\n",
      "Training Loss: 1.7202333211898804\n",
      "\n",
      "Epoch 7, Batch 2500\n",
      "Training Loss: 1.7683368921279907\n",
      "\n",
      "Epoch 7, Batch 2525\n",
      "Training Loss: 1.5706661939620972\n",
      "\n",
      "Epoch 7, Batch 2550\n",
      "Training Loss: 1.8560292720794678\n",
      "\n",
      "Epoch 7, Batch 2575\n",
      "Training Loss: 1.91293203830719\n",
      "\n",
      "Epoch 7, Batch 2600\n",
      "Training Loss: 1.5903533697128296\n",
      "\n",
      "Epoch 7, Batch 2625\n",
      "Training Loss: 1.6506245136260986\n",
      "\n",
      "Epoch 7, Batch 2650\n",
      "Training Loss: 1.8009488582611084\n",
      "\n",
      "Epoch 7, Batch 2675\n",
      "Training Loss: 2.103262424468994\n",
      "\n",
      "Epoch 7, Batch 2700\n",
      "Training Loss: 1.811462163925171\n",
      "\n",
      "Epoch 7, Batch 2725\n",
      "Training Loss: 1.2689859867095947\n",
      "\n",
      "Epoch 7, Batch 2750\n",
      "Training Loss: 2.029123306274414\n",
      "\n",
      "Epoch 7, Batch 2775\n",
      "Training Loss: 1.883776307106018\n",
      "\n",
      "Epoch 7, Batch 2800\n",
      "Training Loss: 1.5847429037094116\n",
      "\n",
      "Epoch 7, Batch 2825\n",
      "Training Loss: 1.9342453479766846\n",
      "\n",
      "Epoch 7, Batch 2850\n",
      "Training Loss: 2.4553539752960205\n",
      "\n",
      "Epoch 7, Batch 2875\n",
      "Training Loss: 1.9391396045684814\n",
      "\n",
      "Epoch 7, Batch 2900\n",
      "Training Loss: 1.9513987302780151\n",
      "\n",
      "Epoch 7, Batch 2925\n",
      "Training Loss: 1.9435582160949707\n",
      "\n",
      "Epoch 7, Batch 2950\n",
      "Training Loss: 2.178426742553711\n",
      "\n",
      "Epoch 7/20\n",
      "Training Loss: 1.861856460571289\n",
      "\n",
      "Epoch 8, Batch 0\n",
      "Training Loss: 1.6270451545715332\n",
      "\n",
      "Epoch 8, Batch 25\n",
      "Training Loss: 1.967011570930481\n",
      "\n",
      "Epoch 8, Batch 50\n",
      "Training Loss: 1.9851571321487427\n",
      "\n",
      "Epoch 8, Batch 75\n",
      "Training Loss: 2.001605987548828\n",
      "\n",
      "Epoch 8, Batch 100\n",
      "Training Loss: 2.036670446395874\n",
      "\n",
      "Epoch 8, Batch 125\n",
      "Training Loss: 1.8893952369689941\n",
      "\n",
      "Epoch 8, Batch 150\n",
      "Training Loss: 1.6041934490203857\n",
      "\n",
      "Epoch 8, Batch 175\n",
      "Training Loss: 1.9927704334259033\n",
      "\n",
      "Epoch 8, Batch 200\n",
      "Training Loss: 1.664971113204956\n",
      "\n",
      "Epoch 8, Batch 225\n",
      "Training Loss: 1.6816062927246094\n",
      "\n",
      "Epoch 8, Batch 250\n",
      "Training Loss: 1.743408441543579\n",
      "\n",
      "Epoch 8, Batch 275\n",
      "Training Loss: 1.8207582235336304\n",
      "\n",
      "Epoch 8, Batch 300\n",
      "Training Loss: 1.6745455265045166\n",
      "\n",
      "Epoch 8, Batch 325\n",
      "Training Loss: 1.7442479133605957\n",
      "\n",
      "Epoch 8, Batch 350\n",
      "Training Loss: 1.69389009475708\n",
      "\n",
      "Epoch 8, Batch 375\n",
      "Training Loss: 1.905287742614746\n",
      "\n",
      "Epoch 8, Batch 400\n",
      "Training Loss: 1.7626450061798096\n",
      "\n",
      "Epoch 8, Batch 425\n",
      "Training Loss: 1.6959530115127563\n",
      "\n",
      "Epoch 8, Batch 450\n",
      "Training Loss: 1.5560097694396973\n",
      "\n",
      "Epoch 8, Batch 475\n",
      "Training Loss: 1.9449785947799683\n",
      "\n",
      "Epoch 8, Batch 500\n",
      "Training Loss: 1.4413987398147583\n",
      "\n",
      "Epoch 8, Batch 525\n",
      "Training Loss: 1.6728813648223877\n",
      "\n",
      "Epoch 8, Batch 550\n",
      "Training Loss: 2.0616538524627686\n",
      "\n",
      "Epoch 8, Batch 575\n",
      "Training Loss: 1.8644198179244995\n",
      "\n",
      "Epoch 8, Batch 600\n",
      "Training Loss: 1.7915085554122925\n",
      "\n",
      "Epoch 8, Batch 625\n",
      "Training Loss: 1.886483073234558\n",
      "\n",
      "Epoch 8, Batch 650\n",
      "Training Loss: 1.621909260749817\n",
      "\n",
      "Epoch 8, Batch 675\n",
      "Training Loss: 1.834950566291809\n",
      "\n",
      "Epoch 8, Batch 700\n",
      "Training Loss: 1.8375540971755981\n",
      "\n",
      "Epoch 8, Batch 725\n",
      "Training Loss: 1.8961575031280518\n",
      "\n",
      "Epoch 8, Batch 750\n",
      "Training Loss: 1.5608601570129395\n",
      "\n",
      "Epoch 8, Batch 775\n",
      "Training Loss: 1.760475754737854\n",
      "\n",
      "Epoch 8, Batch 800\n",
      "Training Loss: 1.9171257019042969\n",
      "\n",
      "Epoch 8, Batch 825\n",
      "Training Loss: 1.6706942319869995\n",
      "\n",
      "Epoch 8, Batch 850\n",
      "Training Loss: 1.6680948734283447\n",
      "\n",
      "Epoch 8, Batch 875\n",
      "Training Loss: 1.777866005897522\n",
      "\n",
      "Epoch 8, Batch 900\n",
      "Training Loss: 1.9772796630859375\n",
      "\n",
      "Epoch 8, Batch 925\n",
      "Training Loss: 1.788881540298462\n",
      "\n",
      "Epoch 8, Batch 950\n",
      "Training Loss: 2.050933837890625\n",
      "\n",
      "Epoch 8, Batch 975\n",
      "Training Loss: 1.6608192920684814\n",
      "\n",
      "Epoch 8, Batch 1000\n",
      "Training Loss: 1.5717926025390625\n",
      "\n",
      "Epoch 8, Batch 1025\n",
      "Training Loss: 1.8889275789260864\n",
      "\n",
      "Epoch 8, Batch 1050\n",
      "Training Loss: 2.4157001972198486\n",
      "\n",
      "Epoch 8, Batch 1075\n",
      "Training Loss: 1.7806594371795654\n",
      "\n",
      "Epoch 8, Batch 1100\n",
      "Training Loss: 1.9610157012939453\n",
      "\n",
      "Epoch 8, Batch 1125\n",
      "Training Loss: 1.731458306312561\n",
      "\n",
      "Epoch 8, Batch 1150\n",
      "Training Loss: 1.8003383874893188\n",
      "\n",
      "Epoch 8, Batch 1175\n",
      "Training Loss: 1.6680097579956055\n",
      "\n",
      "Epoch 8, Batch 1200\n",
      "Training Loss: 1.5960400104522705\n",
      "\n",
      "Epoch 8, Batch 1225\n",
      "Training Loss: 1.3891626596450806\n",
      "\n",
      "Epoch 8, Batch 1250\n",
      "Training Loss: 1.542506217956543\n",
      "\n",
      "Epoch 8, Batch 1275\n",
      "Training Loss: 1.7386223077774048\n",
      "\n",
      "Epoch 8, Batch 1300\n",
      "Training Loss: 1.6229333877563477\n",
      "\n",
      "Epoch 8, Batch 1325\n",
      "Training Loss: 1.5169399976730347\n",
      "\n",
      "Epoch 8, Batch 1350\n",
      "Training Loss: 1.7972952127456665\n",
      "\n",
      "Epoch 8, Batch 1375\n",
      "Training Loss: 1.7187749147415161\n",
      "\n",
      "Epoch 8, Batch 1400\n",
      "Training Loss: 1.7188869714736938\n",
      "\n",
      "Epoch 8, Batch 1425\n",
      "Training Loss: 1.7795826196670532\n",
      "\n",
      "Epoch 8, Batch 1450\n",
      "Training Loss: 1.8767518997192383\n",
      "\n",
      "Epoch 8, Batch 1475\n",
      "Training Loss: 1.7554477453231812\n",
      "\n",
      "Epoch 8, Batch 1500\n",
      "Training Loss: 2.173632860183716\n",
      "\n",
      "Epoch 8, Batch 1525\n",
      "Training Loss: 1.8239651918411255\n",
      "\n",
      "Epoch 8, Batch 1550\n",
      "Training Loss: 1.8512802124023438\n",
      "\n",
      "Epoch 8, Batch 1575\n",
      "Training Loss: 1.7681851387023926\n",
      "\n",
      "Epoch 8, Batch 1600\n",
      "Training Loss: 1.78658127784729\n",
      "\n",
      "Epoch 8, Batch 1625\n",
      "Training Loss: 1.9711227416992188\n",
      "\n",
      "Epoch 8, Batch 1650\n",
      "Training Loss: 1.7733335494995117\n",
      "\n",
      "Epoch 8, Batch 1675\n",
      "Training Loss: 1.9770087003707886\n",
      "\n",
      "Epoch 8, Batch 1700\n",
      "Training Loss: 1.635451078414917\n",
      "\n",
      "Epoch 8, Batch 1725\n",
      "Training Loss: 2.10095477104187\n",
      "\n",
      "Epoch 8, Batch 1750\n",
      "Training Loss: 1.8757719993591309\n",
      "\n",
      "Epoch 8, Batch 1775\n",
      "Training Loss: 1.5657730102539062\n",
      "\n",
      "Epoch 8, Batch 1800\n",
      "Training Loss: 1.8845463991165161\n",
      "\n",
      "Epoch 8, Batch 1825\n",
      "Training Loss: 1.7698901891708374\n",
      "\n",
      "Epoch 8, Batch 1850\n",
      "Training Loss: 1.8806875944137573\n",
      "\n",
      "Epoch 8, Batch 1875\n",
      "Training Loss: 1.8112545013427734\n",
      "\n",
      "Epoch 8, Batch 1900\n",
      "Training Loss: 1.706372857093811\n",
      "\n",
      "Epoch 8, Batch 1925\n",
      "Training Loss: 1.8472771644592285\n",
      "\n",
      "Epoch 8, Batch 1950\n",
      "Training Loss: 1.871414065361023\n",
      "\n",
      "Epoch 8, Batch 1975\n",
      "Training Loss: 1.7958654165267944\n",
      "\n",
      "Epoch 8, Batch 2000\n",
      "Training Loss: 1.4721112251281738\n",
      "\n",
      "Epoch 8, Batch 2025\n",
      "Training Loss: 1.6319620609283447\n",
      "\n",
      "Epoch 8, Batch 2050\n",
      "Training Loss: 1.9757494926452637\n",
      "\n",
      "Epoch 8, Batch 2075\n",
      "Training Loss: 1.7873486280441284\n",
      "\n",
      "Epoch 8, Batch 2100\n",
      "Training Loss: 1.9588299989700317\n",
      "\n",
      "Epoch 8, Batch 2125\n",
      "Training Loss: 1.9072989225387573\n",
      "\n",
      "Epoch 8, Batch 2150\n",
      "Training Loss: 1.786745548248291\n",
      "\n",
      "Epoch 8, Batch 2175\n",
      "Training Loss: 1.543466329574585\n",
      "\n",
      "Epoch 8, Batch 2200\n",
      "Training Loss: 2.069286346435547\n",
      "\n",
      "Epoch 8, Batch 2225\n",
      "Training Loss: 1.6891099214553833\n",
      "\n",
      "Epoch 8, Batch 2250\n",
      "Training Loss: 1.5365620851516724\n",
      "\n",
      "Epoch 8, Batch 2275\n",
      "Training Loss: 1.6445508003234863\n",
      "\n",
      "Epoch 8, Batch 2300\n",
      "Training Loss: 1.9234275817871094\n",
      "\n",
      "Epoch 8, Batch 2325\n",
      "Training Loss: 1.833840012550354\n",
      "\n",
      "Epoch 8, Batch 2350\n",
      "Training Loss: 1.8325389623641968\n",
      "\n",
      "Epoch 8, Batch 2375\n",
      "Training Loss: 1.5954880714416504\n",
      "\n",
      "Epoch 8, Batch 2400\n",
      "Training Loss: 1.4478563070297241\n",
      "\n",
      "Epoch 8, Batch 2425\n",
      "Training Loss: 1.41826593875885\n",
      "\n",
      "Epoch 8, Batch 2450\n",
      "Training Loss: 1.6388981342315674\n",
      "\n",
      "Epoch 8, Batch 2475\n",
      "Training Loss: 1.9221025705337524\n",
      "\n",
      "Epoch 8, Batch 2500\n",
      "Training Loss: 1.5159556865692139\n",
      "\n",
      "Epoch 8, Batch 2525\n",
      "Training Loss: 1.8858081102371216\n",
      "\n",
      "Epoch 8, Batch 2550\n",
      "Training Loss: 1.9478163719177246\n",
      "\n",
      "Epoch 8, Batch 2575\n",
      "Training Loss: 1.853539228439331\n",
      "\n",
      "Epoch 8, Batch 2600\n",
      "Training Loss: 1.6528033018112183\n",
      "\n",
      "Epoch 8, Batch 2625\n",
      "Training Loss: 1.7752326726913452\n",
      "\n",
      "Epoch 8, Batch 2650\n",
      "Training Loss: 1.445086121559143\n",
      "\n",
      "Epoch 8, Batch 2675\n",
      "Training Loss: 1.5831066370010376\n",
      "\n",
      "Epoch 8, Batch 2700\n",
      "Training Loss: 1.8619781732559204\n",
      "\n",
      "Epoch 8, Batch 2725\n",
      "Training Loss: 2.2920522689819336\n",
      "\n",
      "Epoch 8, Batch 2750\n",
      "Training Loss: 2.2262890338897705\n",
      "\n",
      "Epoch 8, Batch 2775\n",
      "Training Loss: 1.8123539686203003\n",
      "\n",
      "Epoch 8, Batch 2800\n",
      "Training Loss: 1.947095513343811\n",
      "\n",
      "Epoch 8, Batch 2825\n",
      "Training Loss: 1.5639081001281738\n",
      "\n",
      "Epoch 8, Batch 2850\n",
      "Training Loss: 1.8990806341171265\n",
      "\n",
      "Epoch 8, Batch 2875\n",
      "Training Loss: 1.8018453121185303\n",
      "\n",
      "Epoch 8, Batch 2900\n",
      "Training Loss: 1.785353660583496\n",
      "\n",
      "Epoch 8, Batch 2925\n",
      "Training Loss: 1.7958723306655884\n",
      "\n",
      "Epoch 8, Batch 2950\n",
      "Training Loss: 1.9273039102554321\n",
      "\n",
      "Epoch 8/20\n",
      "Training Loss: 1.419638752937317\n",
      "\n",
      "Epoch 9, Batch 0\n",
      "Training Loss: 1.4758332967758179\n",
      "\n",
      "Epoch 9, Batch 25\n",
      "Training Loss: 2.043358564376831\n",
      "\n",
      "Epoch 9, Batch 50\n",
      "Training Loss: 2.0783188343048096\n",
      "\n",
      "Epoch 9, Batch 75\n",
      "Training Loss: 2.028122663497925\n",
      "\n",
      "Epoch 9, Batch 100\n",
      "Training Loss: 1.6235804557800293\n",
      "\n",
      "Epoch 9, Batch 125\n",
      "Training Loss: 1.6460022926330566\n",
      "\n",
      "Epoch 9, Batch 150\n",
      "Training Loss: 1.766021490097046\n",
      "\n",
      "Epoch 9, Batch 175\n",
      "Training Loss: 1.57607102394104\n",
      "\n",
      "Epoch 9, Batch 200\n",
      "Training Loss: 1.9674078226089478\n",
      "\n",
      "Epoch 9, Batch 225\n",
      "Training Loss: 1.6148765087127686\n",
      "\n",
      "Epoch 9, Batch 250\n",
      "Training Loss: 1.3082581758499146\n",
      "\n",
      "Epoch 9, Batch 275\n",
      "Training Loss: 1.856553316116333\n",
      "\n",
      "Epoch 9, Batch 300\n",
      "Training Loss: 2.143188238143921\n",
      "\n",
      "Epoch 9, Batch 325\n",
      "Training Loss: 1.6775901317596436\n",
      "\n",
      "Epoch 9, Batch 350\n",
      "Training Loss: 1.5116257667541504\n",
      "\n",
      "Epoch 9, Batch 375\n",
      "Training Loss: 1.7358548641204834\n",
      "\n",
      "Epoch 9, Batch 400\n",
      "Training Loss: 1.7003633975982666\n",
      "\n",
      "Epoch 9, Batch 425\n",
      "Training Loss: 2.0518622398376465\n",
      "\n",
      "Epoch 9, Batch 450\n",
      "Training Loss: 1.7340244054794312\n",
      "\n",
      "Epoch 9, Batch 475\n",
      "Training Loss: 1.3152883052825928\n",
      "\n",
      "Epoch 9, Batch 500\n",
      "Training Loss: 1.884035587310791\n",
      "\n",
      "Epoch 9, Batch 525\n",
      "Training Loss: 1.963797688484192\n",
      "\n",
      "Epoch 9, Batch 550\n",
      "Training Loss: 1.8226840496063232\n",
      "\n",
      "Epoch 9, Batch 575\n",
      "Training Loss: 1.7997286319732666\n",
      "\n",
      "Epoch 9, Batch 600\n",
      "Training Loss: 1.6145553588867188\n",
      "\n",
      "Epoch 9, Batch 625\n",
      "Training Loss: 1.8573570251464844\n",
      "\n",
      "Epoch 9, Batch 650\n",
      "Training Loss: 1.7477015256881714\n",
      "\n",
      "Epoch 9, Batch 675\n",
      "Training Loss: 1.754088282585144\n",
      "\n",
      "Epoch 9, Batch 700\n",
      "Training Loss: 1.6943162679672241\n",
      "\n",
      "Epoch 9, Batch 725\n",
      "Training Loss: 1.6241858005523682\n",
      "\n",
      "Epoch 9, Batch 750\n",
      "Training Loss: 1.9714363813400269\n",
      "\n",
      "Epoch 9, Batch 775\n",
      "Training Loss: 1.9386991262435913\n",
      "\n",
      "Epoch 9, Batch 800\n",
      "Training Loss: 1.7602453231811523\n",
      "\n",
      "Epoch 9, Batch 825\n",
      "Training Loss: 1.735655426979065\n",
      "\n",
      "Epoch 9, Batch 850\n",
      "Training Loss: 1.6513632535934448\n",
      "\n",
      "Epoch 9, Batch 875\n",
      "Training Loss: 2.1444733142852783\n",
      "\n",
      "Epoch 9, Batch 900\n",
      "Training Loss: 1.7298849821090698\n",
      "\n",
      "Epoch 9, Batch 925\n",
      "Training Loss: 1.5077143907546997\n",
      "\n",
      "Epoch 9, Batch 950\n",
      "Training Loss: 1.7171928882598877\n",
      "\n",
      "Epoch 9, Batch 975\n",
      "Training Loss: 1.8257675170898438\n",
      "\n",
      "Epoch 9, Batch 1000\n",
      "Training Loss: 1.845416784286499\n",
      "\n",
      "Epoch 9, Batch 1025\n",
      "Training Loss: 1.6645739078521729\n",
      "\n",
      "Epoch 9, Batch 1050\n",
      "Training Loss: 1.8259761333465576\n",
      "\n",
      "Epoch 9, Batch 1075\n",
      "Training Loss: 1.4212228059768677\n",
      "\n",
      "Epoch 9, Batch 1100\n",
      "Training Loss: 1.4108694791793823\n",
      "\n",
      "Epoch 9, Batch 1125\n",
      "Training Loss: 2.1484591960906982\n",
      "\n",
      "Epoch 9, Batch 1150\n",
      "Training Loss: 1.8287767171859741\n",
      "\n",
      "Epoch 9, Batch 1175\n",
      "Training Loss: 1.609762191772461\n",
      "\n",
      "Epoch 9, Batch 1200\n",
      "Training Loss: 2.1579630374908447\n",
      "\n",
      "Epoch 9, Batch 1225\n",
      "Training Loss: 1.6942028999328613\n",
      "\n",
      "Epoch 9, Batch 1250\n",
      "Training Loss: 1.92474365234375\n",
      "\n",
      "Epoch 9, Batch 1275\n",
      "Training Loss: 1.684085488319397\n",
      "\n",
      "Epoch 9, Batch 1300\n",
      "Training Loss: 1.6436514854431152\n",
      "\n",
      "Epoch 9, Batch 1325\n",
      "Training Loss: 1.7410569190979004\n",
      "\n",
      "Epoch 9, Batch 1350\n",
      "Training Loss: 1.7157459259033203\n",
      "\n",
      "Epoch 9, Batch 1375\n",
      "Training Loss: 1.6697665452957153\n",
      "\n",
      "Epoch 9, Batch 1400\n",
      "Training Loss: 2.038890838623047\n",
      "\n",
      "Epoch 9, Batch 1425\n",
      "Training Loss: 1.479402780532837\n",
      "\n",
      "Epoch 9, Batch 1450\n",
      "Training Loss: 1.7760672569274902\n",
      "\n",
      "Epoch 9, Batch 1475\n",
      "Training Loss: 1.6699309349060059\n",
      "\n",
      "Epoch 9, Batch 1500\n",
      "Training Loss: 1.4520070552825928\n",
      "\n",
      "Epoch 9, Batch 1525\n",
      "Training Loss: 1.4992830753326416\n",
      "\n",
      "Epoch 9, Batch 1550\n",
      "Training Loss: 2.0734879970550537\n",
      "\n",
      "Epoch 9, Batch 1575\n",
      "Training Loss: 1.6158267259597778\n",
      "\n",
      "Epoch 9, Batch 1600\n",
      "Training Loss: 1.7403873205184937\n",
      "\n",
      "Epoch 9, Batch 1625\n",
      "Training Loss: 1.7738157510757446\n",
      "\n",
      "Epoch 9, Batch 1650\n",
      "Training Loss: 1.7716654539108276\n",
      "\n",
      "Epoch 9, Batch 1675\n",
      "Training Loss: 1.6115922927856445\n",
      "\n",
      "Epoch 9, Batch 1700\n",
      "Training Loss: 1.8341069221496582\n",
      "\n",
      "Epoch 9, Batch 1725\n",
      "Training Loss: 1.7917697429656982\n",
      "\n",
      "Epoch 9, Batch 1750\n",
      "Training Loss: 2.294668674468994\n",
      "\n",
      "Epoch 9, Batch 1775\n",
      "Training Loss: 1.3769819736480713\n",
      "\n",
      "Epoch 9, Batch 1800\n",
      "Training Loss: 2.0340981483459473\n",
      "\n",
      "Epoch 9, Batch 1825\n",
      "Training Loss: 1.9268677234649658\n",
      "\n",
      "Epoch 9, Batch 1850\n",
      "Training Loss: 1.7538621425628662\n",
      "\n",
      "Epoch 9, Batch 1875\n",
      "Training Loss: 1.597615361213684\n",
      "\n",
      "Epoch 9, Batch 1900\n",
      "Training Loss: 1.770903468132019\n",
      "\n",
      "Epoch 9, Batch 1925\n",
      "Training Loss: 2.081162214279175\n",
      "\n",
      "Epoch 9, Batch 1950\n",
      "Training Loss: 1.5486239194869995\n",
      "\n",
      "Epoch 9, Batch 1975\n",
      "Training Loss: 1.5875129699707031\n",
      "\n",
      "Epoch 9, Batch 2000\n",
      "Training Loss: 1.8960574865341187\n",
      "\n",
      "Epoch 9, Batch 2025\n",
      "Training Loss: 1.7047860622406006\n",
      "\n",
      "Epoch 9, Batch 2050\n",
      "Training Loss: 1.814024567604065\n",
      "\n",
      "Epoch 9, Batch 2075\n",
      "Training Loss: 1.740067720413208\n",
      "\n",
      "Epoch 9, Batch 2100\n",
      "Training Loss: 1.6772525310516357\n",
      "\n",
      "Epoch 9, Batch 2125\n",
      "Training Loss: 2.015808343887329\n",
      "\n",
      "Epoch 9, Batch 2150\n",
      "Training Loss: 2.1364965438842773\n",
      "\n",
      "Epoch 9, Batch 2175\n",
      "Training Loss: 1.6836904287338257\n",
      "\n",
      "Epoch 9, Batch 2200\n",
      "Training Loss: 2.1222310066223145\n",
      "\n",
      "Epoch 9, Batch 2225\n",
      "Training Loss: 1.6968477964401245\n",
      "\n",
      "Epoch 9, Batch 2250\n",
      "Training Loss: 1.6950633525848389\n",
      "\n",
      "Epoch 9, Batch 2275\n",
      "Training Loss: 1.9828331470489502\n",
      "\n",
      "Epoch 9, Batch 2300\n",
      "Training Loss: 2.2036468982696533\n",
      "\n",
      "Epoch 9, Batch 2325\n",
      "Training Loss: 1.8369914293289185\n",
      "\n",
      "Epoch 9, Batch 2350\n",
      "Training Loss: 1.8199368715286255\n",
      "\n",
      "Epoch 9, Batch 2375\n",
      "Training Loss: 1.7496240139007568\n",
      "\n",
      "Epoch 9, Batch 2400\n",
      "Training Loss: 1.77387535572052\n",
      "\n",
      "Epoch 9, Batch 2425\n",
      "Training Loss: 2.0536913871765137\n",
      "\n",
      "Epoch 9, Batch 2450\n",
      "Training Loss: 1.6307388544082642\n",
      "\n",
      "Epoch 9, Batch 2475\n",
      "Training Loss: 1.9079139232635498\n",
      "\n",
      "Epoch 9, Batch 2500\n",
      "Training Loss: 1.7117558717727661\n",
      "\n",
      "Epoch 9, Batch 2525\n",
      "Training Loss: 1.4248684644699097\n",
      "\n",
      "Epoch 9, Batch 2550\n",
      "Training Loss: 2.0379559993743896\n",
      "\n",
      "Epoch 9, Batch 2575\n",
      "Training Loss: 1.692094087600708\n",
      "\n",
      "Epoch 9, Batch 2600\n",
      "Training Loss: 1.7979952096939087\n",
      "\n",
      "Epoch 9, Batch 2625\n",
      "Training Loss: 1.5094456672668457\n",
      "\n",
      "Epoch 9, Batch 2650\n",
      "Training Loss: 1.4742872714996338\n",
      "\n",
      "Epoch 9, Batch 2675\n",
      "Training Loss: 1.7401185035705566\n",
      "\n",
      "Epoch 9, Batch 2700\n",
      "Training Loss: 1.695578932762146\n",
      "\n",
      "Epoch 9, Batch 2725\n",
      "Training Loss: 1.588976502418518\n",
      "\n",
      "Epoch 9, Batch 2750\n",
      "Training Loss: 1.6094906330108643\n",
      "\n",
      "Epoch 9, Batch 2775\n",
      "Training Loss: 1.958325743675232\n",
      "\n",
      "Epoch 9, Batch 2800\n",
      "Training Loss: 1.9172730445861816\n",
      "\n",
      "Epoch 9, Batch 2825\n",
      "Training Loss: 1.5759503841400146\n",
      "\n",
      "Epoch 9, Batch 2850\n",
      "Training Loss: 1.382914662361145\n",
      "\n",
      "Epoch 9, Batch 2875\n",
      "Training Loss: 1.6670411825180054\n",
      "\n",
      "Epoch 9, Batch 2900\n",
      "Training Loss: 1.8040311336517334\n",
      "\n",
      "Epoch 9, Batch 2925\n",
      "Training Loss: 2.2232580184936523\n",
      "\n",
      "Epoch 9, Batch 2950\n",
      "Training Loss: 1.8865920305252075\n",
      "\n",
      "Epoch 9/20\n",
      "Training Loss: 1.7097266912460327\n",
      "\n",
      "Epoch 10, Batch 0\n",
      "Training Loss: 1.4478152990341187\n",
      "\n",
      "Epoch 10, Batch 25\n",
      "Training Loss: 1.4437755346298218\n",
      "\n",
      "Epoch 10, Batch 50\n",
      "Training Loss: 1.9170504808425903\n",
      "\n",
      "Epoch 10, Batch 75\n",
      "Training Loss: 1.616209626197815\n",
      "\n",
      "Epoch 10, Batch 100\n",
      "Training Loss: 1.5713045597076416\n",
      "\n",
      "Epoch 10, Batch 125\n",
      "Training Loss: 1.7768621444702148\n",
      "\n",
      "Epoch 10, Batch 150\n",
      "Training Loss: 1.7691633701324463\n",
      "\n",
      "Epoch 10, Batch 175\n",
      "Training Loss: 1.7050706148147583\n",
      "\n",
      "Epoch 10, Batch 200\n",
      "Training Loss: 1.7903062105178833\n",
      "\n",
      "Epoch 10, Batch 225\n",
      "Training Loss: 1.65703547000885\n",
      "\n",
      "Epoch 10, Batch 250\n",
      "Training Loss: 2.3504045009613037\n",
      "\n",
      "Epoch 10, Batch 275\n",
      "Training Loss: 1.7580691576004028\n",
      "\n",
      "Epoch 10, Batch 300\n",
      "Training Loss: 1.6522859334945679\n",
      "\n",
      "Epoch 10, Batch 325\n",
      "Training Loss: 1.5387604236602783\n",
      "\n",
      "Epoch 10, Batch 350\n",
      "Training Loss: 1.6977463960647583\n",
      "\n",
      "Epoch 10, Batch 375\n",
      "Training Loss: 1.6986478567123413\n",
      "\n",
      "Epoch 10, Batch 400\n",
      "Training Loss: 1.7405003309249878\n",
      "\n",
      "Epoch 10, Batch 425\n",
      "Training Loss: 1.6386168003082275\n",
      "\n",
      "Epoch 10, Batch 450\n",
      "Training Loss: 1.9768227338790894\n",
      "\n",
      "Epoch 10, Batch 475\n",
      "Training Loss: 1.5522748231887817\n",
      "\n",
      "Epoch 10, Batch 500\n",
      "Training Loss: 1.9024840593338013\n",
      "\n",
      "Epoch 10, Batch 525\n",
      "Training Loss: 1.5716427564620972\n",
      "\n",
      "Epoch 10, Batch 550\n",
      "Training Loss: 1.4627652168273926\n",
      "\n",
      "Epoch 10, Batch 575\n",
      "Training Loss: 1.4256539344787598\n",
      "\n",
      "Epoch 10, Batch 600\n",
      "Training Loss: 1.6502888202667236\n",
      "\n",
      "Epoch 10, Batch 625\n",
      "Training Loss: 1.5609151124954224\n",
      "\n",
      "Epoch 10, Batch 650\n",
      "Training Loss: 1.6380271911621094\n",
      "\n",
      "Epoch 10, Batch 675\n",
      "Training Loss: 1.464682698249817\n",
      "\n",
      "Epoch 10, Batch 700\n",
      "Training Loss: 1.5676896572113037\n",
      "\n",
      "Epoch 10, Batch 725\n",
      "Training Loss: 2.0399012565612793\n",
      "\n",
      "Epoch 10, Batch 750\n",
      "Training Loss: 2.0169737339019775\n",
      "\n",
      "Epoch 10, Batch 775\n",
      "Training Loss: 1.8616292476654053\n",
      "\n",
      "Epoch 10, Batch 800\n",
      "Training Loss: 1.7710570096969604\n",
      "\n",
      "Epoch 10, Batch 825\n",
      "Training Loss: 1.7337305545806885\n",
      "\n",
      "Epoch 10, Batch 850\n",
      "Training Loss: 1.4681888818740845\n",
      "\n",
      "Epoch 10, Batch 875\n",
      "Training Loss: 1.9485888481140137\n",
      "\n",
      "Epoch 10, Batch 900\n",
      "Training Loss: 1.7549126148223877\n",
      "\n",
      "Epoch 10, Batch 925\n",
      "Training Loss: 1.6867939233779907\n",
      "\n",
      "Epoch 10, Batch 950\n",
      "Training Loss: 1.8249131441116333\n",
      "\n",
      "Epoch 10, Batch 975\n",
      "Training Loss: 1.7367339134216309\n",
      "\n",
      "Epoch 10, Batch 1000\n",
      "Training Loss: 1.5532574653625488\n",
      "\n",
      "Epoch 10, Batch 1025\n",
      "Training Loss: 1.6582493782043457\n",
      "\n",
      "Epoch 10, Batch 1050\n",
      "Training Loss: 1.8032441139221191\n",
      "\n",
      "Epoch 10, Batch 1075\n",
      "Training Loss: 1.7627474069595337\n",
      "\n",
      "Epoch 10, Batch 1100\n",
      "Training Loss: 1.5797773599624634\n",
      "\n",
      "Epoch 10, Batch 1125\n",
      "Training Loss: 1.9514328241348267\n",
      "\n",
      "Epoch 10, Batch 1150\n",
      "Training Loss: 1.8815674781799316\n",
      "\n",
      "Epoch 10, Batch 1175\n",
      "Training Loss: 1.6915892362594604\n",
      "\n",
      "Epoch 10, Batch 1200\n",
      "Training Loss: 1.6065137386322021\n",
      "\n",
      "Epoch 10, Batch 1225\n",
      "Training Loss: 1.8282350301742554\n",
      "\n",
      "Epoch 10, Batch 1250\n",
      "Training Loss: 1.4195361137390137\n",
      "\n",
      "Epoch 10, Batch 1275\n",
      "Training Loss: 2.00691819190979\n",
      "\n",
      "Epoch 10, Batch 1300\n",
      "Training Loss: 1.7212845087051392\n",
      "\n",
      "Epoch 10, Batch 1325\n",
      "Training Loss: 1.6718533039093018\n",
      "\n",
      "Epoch 10, Batch 1350\n",
      "Training Loss: 1.6586564779281616\n",
      "\n",
      "Epoch 10, Batch 1375\n",
      "Training Loss: 1.7455419301986694\n",
      "\n",
      "Epoch 10, Batch 1400\n",
      "Training Loss: 1.551883339881897\n",
      "\n",
      "Epoch 10, Batch 1425\n",
      "Training Loss: 1.5564141273498535\n",
      "\n",
      "Epoch 10, Batch 1450\n",
      "Training Loss: 1.5943622589111328\n",
      "\n",
      "Epoch 10, Batch 1475\n",
      "Training Loss: 1.5963044166564941\n",
      "\n",
      "Epoch 10, Batch 1500\n",
      "Training Loss: 1.9507092237472534\n",
      "\n",
      "Epoch 10, Batch 1525\n",
      "Training Loss: 1.7597682476043701\n",
      "\n",
      "Epoch 10, Batch 1550\n",
      "Training Loss: 2.0172526836395264\n",
      "\n",
      "Epoch 10, Batch 1575\n",
      "Training Loss: 1.8035720586776733\n",
      "\n",
      "Epoch 10, Batch 1600\n",
      "Training Loss: 1.8582512140274048\n",
      "\n",
      "Epoch 10, Batch 1625\n",
      "Training Loss: 1.8666903972625732\n",
      "\n",
      "Epoch 10, Batch 1650\n",
      "Training Loss: 1.6918814182281494\n",
      "\n",
      "Epoch 10, Batch 1675\n",
      "Training Loss: 1.7772949934005737\n",
      "\n",
      "Epoch 10, Batch 1700\n",
      "Training Loss: 1.8291683197021484\n",
      "\n",
      "Epoch 10, Batch 1725\n",
      "Training Loss: 1.9939111471176147\n",
      "\n",
      "Epoch 10, Batch 1750\n",
      "Training Loss: 1.589699625968933\n",
      "\n",
      "Epoch 10, Batch 1775\n",
      "Training Loss: 1.735353708267212\n",
      "\n",
      "Epoch 10, Batch 1800\n",
      "Training Loss: 1.2660311460494995\n",
      "\n",
      "Epoch 10, Batch 1825\n",
      "Training Loss: 1.2460927963256836\n",
      "\n",
      "Epoch 10, Batch 1850\n",
      "Training Loss: 2.1955363750457764\n",
      "\n",
      "Epoch 10, Batch 1875\n",
      "Training Loss: 1.5580521821975708\n",
      "\n",
      "Epoch 10, Batch 1900\n",
      "Training Loss: 1.7196952104568481\n",
      "\n",
      "Epoch 10, Batch 1925\n",
      "Training Loss: 1.915959119796753\n",
      "\n",
      "Epoch 10, Batch 1950\n",
      "Training Loss: 1.7907572984695435\n",
      "\n",
      "Epoch 10, Batch 1975\n",
      "Training Loss: 1.5948528051376343\n",
      "\n",
      "Epoch 10, Batch 2000\n",
      "Training Loss: 1.6707247495651245\n",
      "\n",
      "Epoch 10, Batch 2025\n",
      "Training Loss: 1.7066327333450317\n",
      "\n",
      "Epoch 10, Batch 2050\n",
      "Training Loss: 1.5395315885543823\n",
      "\n",
      "Epoch 10, Batch 2075\n",
      "Training Loss: 1.5831729173660278\n",
      "\n",
      "Epoch 10, Batch 2100\n",
      "Training Loss: 1.8265511989593506\n",
      "\n",
      "Epoch 10, Batch 2125\n",
      "Training Loss: 1.701028823852539\n",
      "\n",
      "Epoch 10, Batch 2150\n",
      "Training Loss: 1.4445840120315552\n",
      "\n",
      "Epoch 10, Batch 2175\n",
      "Training Loss: 1.4674817323684692\n",
      "\n",
      "Epoch 10, Batch 2200\n",
      "Training Loss: 2.0217528343200684\n",
      "\n",
      "Epoch 10, Batch 2225\n",
      "Training Loss: 1.37906813621521\n",
      "\n",
      "Epoch 10, Batch 2250\n",
      "Training Loss: 1.4750561714172363\n",
      "\n",
      "Epoch 10, Batch 2275\n",
      "Training Loss: 1.359554409980774\n",
      "\n",
      "Epoch 10, Batch 2300\n",
      "Training Loss: 1.735278844833374\n",
      "\n",
      "Epoch 10, Batch 2325\n",
      "Training Loss: 1.9938075542449951\n",
      "\n",
      "Epoch 10, Batch 2350\n",
      "Training Loss: 2.18825101852417\n",
      "\n",
      "Epoch 10, Batch 2375\n",
      "Training Loss: 1.9012973308563232\n",
      "\n",
      "Epoch 10, Batch 2400\n",
      "Training Loss: 2.1782474517822266\n",
      "\n",
      "Epoch 10, Batch 2425\n",
      "Training Loss: 1.925296664237976\n",
      "\n",
      "Epoch 10, Batch 2450\n",
      "Training Loss: 1.6872609853744507\n",
      "\n",
      "Epoch 10, Batch 2475\n",
      "Training Loss: 1.8699970245361328\n",
      "\n",
      "Epoch 10, Batch 2500\n",
      "Training Loss: 2.2319118976593018\n",
      "\n",
      "Epoch 10, Batch 2525\n",
      "Training Loss: 1.6737322807312012\n",
      "\n",
      "Epoch 10, Batch 2550\n",
      "Training Loss: 1.6310921907424927\n",
      "\n",
      "Epoch 10, Batch 2575\n",
      "Training Loss: 1.517174243927002\n",
      "\n",
      "Epoch 10, Batch 2600\n",
      "Training Loss: 1.7399178743362427\n",
      "\n",
      "Epoch 10, Batch 2625\n",
      "Training Loss: 1.814857840538025\n",
      "\n",
      "Epoch 10, Batch 2650\n",
      "Training Loss: 1.7149771451950073\n",
      "\n",
      "Epoch 10, Batch 2675\n",
      "Training Loss: 1.5938706398010254\n",
      "\n",
      "Epoch 10, Batch 2700\n",
      "Training Loss: 1.69020414352417\n",
      "\n",
      "Epoch 10, Batch 2725\n",
      "Training Loss: 1.4869303703308105\n",
      "\n",
      "Epoch 10, Batch 2750\n",
      "Training Loss: 1.4881340265274048\n",
      "\n",
      "Epoch 10, Batch 2775\n",
      "Training Loss: 1.6807934045791626\n",
      "\n",
      "Epoch 10, Batch 2800\n",
      "Training Loss: 1.9903496503829956\n",
      "\n",
      "Epoch 10, Batch 2825\n",
      "Training Loss: 1.6888540983200073\n",
      "\n",
      "Epoch 10, Batch 2850\n",
      "Training Loss: 1.6949142217636108\n",
      "\n",
      "Epoch 10, Batch 2875\n",
      "Training Loss: 2.2247519493103027\n",
      "\n",
      "Epoch 10, Batch 2900\n",
      "Training Loss: 1.7264758348464966\n",
      "\n",
      "Epoch 10, Batch 2925\n",
      "Training Loss: 1.6014745235443115\n",
      "\n",
      "Epoch 10, Batch 2950\n",
      "Training Loss: 1.3291116952896118\n",
      "\n",
      "Epoch 10/20\n",
      "Training Loss: 1.7364373207092285\n",
      "\n",
      "Epoch 11, Batch 0\n",
      "Training Loss: 1.7291048765182495\n",
      "\n",
      "Epoch 11, Batch 25\n",
      "Training Loss: 1.6606618165969849\n",
      "\n",
      "Epoch 11, Batch 50\n",
      "Training Loss: 1.7617228031158447\n",
      "\n",
      "Epoch 11, Batch 75\n",
      "Training Loss: 1.6021106243133545\n",
      "\n",
      "Epoch 11, Batch 100\n",
      "Training Loss: 1.5453635454177856\n",
      "\n",
      "Epoch 11, Batch 125\n",
      "Training Loss: 1.9689148664474487\n",
      "\n",
      "Epoch 11, Batch 150\n",
      "Training Loss: 1.8374351263046265\n",
      "\n",
      "Epoch 11, Batch 175\n",
      "Training Loss: 1.5452463626861572\n",
      "\n",
      "Epoch 11, Batch 200\n",
      "Training Loss: 1.631269097328186\n",
      "\n",
      "Epoch 11, Batch 225\n",
      "Training Loss: 1.922253966331482\n",
      "\n",
      "Epoch 11, Batch 250\n",
      "Training Loss: 1.4330507516860962\n",
      "\n",
      "Epoch 11, Batch 275\n",
      "Training Loss: 1.6874449253082275\n",
      "\n",
      "Epoch 11, Batch 300\n",
      "Training Loss: 1.6438767910003662\n",
      "\n",
      "Epoch 11, Batch 325\n",
      "Training Loss: 1.6877847909927368\n",
      "\n",
      "Epoch 11, Batch 350\n",
      "Training Loss: 1.6165837049484253\n",
      "\n",
      "Epoch 11, Batch 375\n",
      "Training Loss: 1.9327433109283447\n",
      "\n",
      "Epoch 11, Batch 400\n",
      "Training Loss: 1.3811872005462646\n",
      "\n",
      "Epoch 11, Batch 425\n",
      "Training Loss: 1.6026194095611572\n",
      "\n",
      "Epoch 11, Batch 450\n",
      "Training Loss: 1.786020040512085\n",
      "\n",
      "Epoch 11, Batch 475\n",
      "Training Loss: 1.3553738594055176\n",
      "\n",
      "Epoch 11, Batch 500\n",
      "Training Loss: 1.8055857419967651\n",
      "\n",
      "Epoch 11, Batch 525\n",
      "Training Loss: 1.8439561128616333\n",
      "\n",
      "Epoch 11, Batch 550\n",
      "Training Loss: 1.77622389793396\n",
      "\n",
      "Epoch 11, Batch 575\n",
      "Training Loss: 1.9309592247009277\n",
      "\n",
      "Epoch 11, Batch 600\n",
      "Training Loss: 1.5403120517730713\n",
      "\n",
      "Epoch 11, Batch 625\n",
      "Training Loss: 1.955431580543518\n",
      "\n",
      "Epoch 11, Batch 650\n",
      "Training Loss: 1.6501790285110474\n",
      "\n",
      "Epoch 11, Batch 675\n",
      "Training Loss: 1.7195284366607666\n",
      "\n",
      "Epoch 11, Batch 700\n",
      "Training Loss: 1.7129491567611694\n",
      "\n",
      "Epoch 11, Batch 725\n",
      "Training Loss: 1.7666438817977905\n",
      "\n",
      "Epoch 11, Batch 750\n",
      "Training Loss: 1.750502586364746\n",
      "\n",
      "Epoch 11, Batch 775\n",
      "Training Loss: 1.8217413425445557\n",
      "\n",
      "Epoch 11, Batch 800\n",
      "Training Loss: 1.8207365274429321\n",
      "\n",
      "Epoch 11, Batch 825\n",
      "Training Loss: 1.6621568202972412\n",
      "\n",
      "Epoch 11, Batch 850\n",
      "Training Loss: 2.0313684940338135\n",
      "\n",
      "Epoch 11, Batch 875\n",
      "Training Loss: 1.5691297054290771\n",
      "\n",
      "Epoch 11, Batch 900\n",
      "Training Loss: 2.0038318634033203\n",
      "\n",
      "Epoch 11, Batch 925\n",
      "Training Loss: 1.9612818956375122\n",
      "\n",
      "Epoch 11, Batch 950\n",
      "Training Loss: 1.6188496351242065\n",
      "\n",
      "Epoch 11, Batch 975\n",
      "Training Loss: 1.30857515335083\n",
      "\n",
      "Epoch 11, Batch 1000\n",
      "Training Loss: 1.6634855270385742\n",
      "\n",
      "Epoch 11, Batch 1025\n",
      "Training Loss: 1.5224963426589966\n",
      "\n",
      "Epoch 11, Batch 1050\n",
      "Training Loss: 1.5514500141143799\n",
      "\n",
      "Epoch 11, Batch 1075\n",
      "Training Loss: 1.5374513864517212\n",
      "\n",
      "Epoch 11, Batch 1100\n",
      "Training Loss: 1.5292686223983765\n",
      "\n",
      "Epoch 11, Batch 1125\n",
      "Training Loss: 1.4816672801971436\n",
      "\n",
      "Epoch 11, Batch 1150\n",
      "Training Loss: 1.867392897605896\n",
      "\n",
      "Epoch 11, Batch 1175\n",
      "Training Loss: 1.855640172958374\n",
      "\n",
      "Epoch 11, Batch 1200\n",
      "Training Loss: 1.4533500671386719\n",
      "\n",
      "Epoch 11, Batch 1225\n",
      "Training Loss: 1.693298101425171\n",
      "\n",
      "Epoch 11, Batch 1250\n",
      "Training Loss: 1.9700599908828735\n",
      "\n",
      "Epoch 11, Batch 1275\n",
      "Training Loss: 1.526716709136963\n",
      "\n",
      "Epoch 11, Batch 1300\n",
      "Training Loss: 1.770906925201416\n",
      "\n",
      "Epoch 11, Batch 1325\n",
      "Training Loss: 1.83128023147583\n",
      "\n",
      "Epoch 11, Batch 1350\n",
      "Training Loss: 1.7195795774459839\n",
      "\n",
      "Epoch 11, Batch 1375\n",
      "Training Loss: 1.549433946609497\n",
      "\n",
      "Epoch 11, Batch 1400\n",
      "Training Loss: 1.9239957332611084\n",
      "\n",
      "Epoch 11, Batch 1425\n",
      "Training Loss: 1.7299766540527344\n",
      "\n",
      "Epoch 11, Batch 1450\n",
      "Training Loss: 1.7521055936813354\n",
      "\n",
      "Epoch 11, Batch 1475\n",
      "Training Loss: 1.7189193964004517\n",
      "\n",
      "Epoch 11, Batch 1500\n",
      "Training Loss: 1.8815726041793823\n",
      "\n",
      "Epoch 11, Batch 1525\n",
      "Training Loss: 1.6465256214141846\n",
      "\n",
      "Epoch 11, Batch 1550\n",
      "Training Loss: 2.1280176639556885\n",
      "\n",
      "Epoch 11, Batch 1575\n",
      "Training Loss: 1.6177836656570435\n",
      "\n",
      "Epoch 11, Batch 1600\n",
      "Training Loss: 1.6533913612365723\n",
      "\n",
      "Epoch 11, Batch 1625\n",
      "Training Loss: 1.9174009561538696\n",
      "\n",
      "Epoch 11, Batch 1650\n",
      "Training Loss: 1.961567997932434\n",
      "\n",
      "Epoch 11, Batch 1675\n",
      "Training Loss: 2.150416612625122\n",
      "\n",
      "Epoch 11, Batch 1700\n",
      "Training Loss: 1.6585687398910522\n",
      "\n",
      "Epoch 11, Batch 1725\n",
      "Training Loss: 2.3383281230926514\n",
      "\n",
      "Epoch 11, Batch 1750\n",
      "Training Loss: 1.299903154373169\n",
      "\n",
      "Epoch 11, Batch 1775\n",
      "Training Loss: 1.828338623046875\n",
      "\n",
      "Epoch 11, Batch 1800\n",
      "Training Loss: 1.6662994623184204\n",
      "\n",
      "Epoch 11, Batch 1825\n",
      "Training Loss: 2.010906934738159\n",
      "\n",
      "Epoch 11, Batch 1850\n",
      "Training Loss: 1.7155131101608276\n",
      "\n",
      "Epoch 11, Batch 1875\n",
      "Training Loss: 1.8716397285461426\n",
      "\n",
      "Epoch 11, Batch 1900\n",
      "Training Loss: 1.7949873208999634\n",
      "\n",
      "Epoch 11, Batch 1925\n",
      "Training Loss: 2.054356098175049\n",
      "\n",
      "Epoch 11, Batch 1950\n",
      "Training Loss: 1.5687271356582642\n",
      "\n",
      "Epoch 11, Batch 1975\n",
      "Training Loss: 1.954433560371399\n",
      "\n",
      "Epoch 11, Batch 2000\n",
      "Training Loss: 1.738520622253418\n",
      "\n",
      "Epoch 11, Batch 2025\n",
      "Training Loss: 1.699398159980774\n",
      "\n",
      "Epoch 11, Batch 2050\n",
      "Training Loss: 1.5309802293777466\n",
      "\n",
      "Epoch 11, Batch 2075\n",
      "Training Loss: 1.7224466800689697\n",
      "\n",
      "Epoch 11, Batch 2100\n",
      "Training Loss: 1.5481536388397217\n",
      "\n",
      "Epoch 11, Batch 2125\n",
      "Training Loss: 1.7066750526428223\n",
      "\n",
      "Epoch 11, Batch 2150\n",
      "Training Loss: 1.7349724769592285\n",
      "\n",
      "Epoch 11, Batch 2175\n",
      "Training Loss: 1.9365040063858032\n",
      "\n",
      "Epoch 11, Batch 2200\n",
      "Training Loss: 1.7797285318374634\n",
      "\n",
      "Epoch 11, Batch 2225\n",
      "Training Loss: 1.6109527349472046\n",
      "\n",
      "Epoch 11, Batch 2250\n",
      "Training Loss: 1.855264663696289\n",
      "\n",
      "Epoch 11, Batch 2275\n",
      "Training Loss: 1.7469202280044556\n",
      "\n",
      "Epoch 11, Batch 2300\n",
      "Training Loss: 1.9690676927566528\n",
      "\n",
      "Epoch 11, Batch 2325\n",
      "Training Loss: 1.7147976160049438\n",
      "\n",
      "Epoch 11, Batch 2350\n",
      "Training Loss: 1.5543413162231445\n",
      "\n",
      "Epoch 11, Batch 2375\n",
      "Training Loss: 1.4715626239776611\n",
      "\n",
      "Epoch 11, Batch 2400\n",
      "Training Loss: 1.5844333171844482\n",
      "\n",
      "Epoch 11, Batch 2425\n",
      "Training Loss: 2.1427478790283203\n",
      "\n",
      "Epoch 11, Batch 2450\n",
      "Training Loss: 1.6777966022491455\n",
      "\n",
      "Epoch 11, Batch 2475\n",
      "Training Loss: 1.7857439517974854\n",
      "\n",
      "Epoch 11, Batch 2500\n",
      "Training Loss: 1.8247361183166504\n",
      "\n",
      "Epoch 11, Batch 2525\n",
      "Training Loss: 1.733494758605957\n",
      "\n",
      "Epoch 11, Batch 2550\n",
      "Training Loss: 1.3906221389770508\n",
      "\n",
      "Epoch 11, Batch 2575\n",
      "Training Loss: 1.7934343814849854\n",
      "\n",
      "Epoch 11, Batch 2600\n",
      "Training Loss: 1.4998568296432495\n",
      "\n",
      "Epoch 11, Batch 2625\n",
      "Training Loss: 1.4595344066619873\n",
      "\n",
      "Epoch 11, Batch 2650\n",
      "Training Loss: 1.5487557649612427\n",
      "\n",
      "Epoch 11, Batch 2675\n",
      "Training Loss: 1.708113670349121\n",
      "\n",
      "Epoch 11, Batch 2700\n",
      "Training Loss: 1.6997251510620117\n",
      "\n",
      "Epoch 11, Batch 2725\n",
      "Training Loss: 1.4136443138122559\n",
      "\n",
      "Epoch 11, Batch 2750\n",
      "Training Loss: 1.918813705444336\n",
      "\n",
      "Epoch 11, Batch 2775\n",
      "Training Loss: 1.7247549295425415\n",
      "\n",
      "Epoch 11, Batch 2800\n",
      "Training Loss: 1.666661024093628\n",
      "\n",
      "Epoch 11, Batch 2825\n",
      "Training Loss: 1.9207470417022705\n",
      "\n",
      "Epoch 11, Batch 2850\n",
      "Training Loss: 1.8356025218963623\n",
      "\n",
      "Epoch 11, Batch 2875\n",
      "Training Loss: 1.3570011854171753\n",
      "\n",
      "Epoch 11, Batch 2900\n",
      "Training Loss: 1.8301899433135986\n",
      "\n",
      "Epoch 11, Batch 2925\n",
      "Training Loss: 1.7919354438781738\n",
      "\n",
      "Epoch 11, Batch 2950\n",
      "Training Loss: 1.7523777484893799\n",
      "\n",
      "Epoch 11/20\n",
      "Training Loss: 1.8596807718276978\n",
      "\n",
      "Epoch 12, Batch 0\n",
      "Training Loss: 1.5789705514907837\n",
      "\n",
      "Epoch 12, Batch 25\n",
      "Training Loss: 1.9733726978302002\n",
      "\n",
      "Epoch 12, Batch 50\n",
      "Training Loss: 1.5836148262023926\n",
      "\n",
      "Epoch 12, Batch 75\n",
      "Training Loss: 1.654083490371704\n",
      "\n",
      "Epoch 12, Batch 100\n",
      "Training Loss: 1.9635125398635864\n",
      "\n",
      "Epoch 12, Batch 125\n",
      "Training Loss: 1.4969370365142822\n",
      "\n",
      "Epoch 12, Batch 150\n",
      "Training Loss: 1.9970850944519043\n",
      "\n",
      "Epoch 12, Batch 175\n",
      "Training Loss: 1.7393262386322021\n",
      "\n",
      "Epoch 12, Batch 200\n",
      "Training Loss: 1.8857470750808716\n",
      "\n",
      "Epoch 12, Batch 225\n",
      "Training Loss: 1.611703634262085\n",
      "\n",
      "Epoch 12, Batch 250\n",
      "Training Loss: 1.4975014925003052\n",
      "\n",
      "Epoch 12, Batch 275\n",
      "Training Loss: 1.7347415685653687\n",
      "\n",
      "Epoch 12, Batch 300\n",
      "Training Loss: 2.0716474056243896\n",
      "\n",
      "Epoch 12, Batch 325\n",
      "Training Loss: 1.4249128103256226\n",
      "\n",
      "Epoch 12, Batch 350\n",
      "Training Loss: 1.5199156999588013\n",
      "\n",
      "Epoch 12, Batch 375\n",
      "Training Loss: 1.7835146188735962\n",
      "\n",
      "Epoch 12, Batch 400\n",
      "Training Loss: 1.8894051313400269\n",
      "\n",
      "Epoch 12, Batch 425\n",
      "Training Loss: 1.5565470457077026\n",
      "\n",
      "Epoch 12, Batch 450\n",
      "Training Loss: 2.078278064727783\n",
      "\n",
      "Epoch 12, Batch 475\n",
      "Training Loss: 1.817823052406311\n",
      "\n",
      "Epoch 12, Batch 500\n",
      "Training Loss: 1.7506698369979858\n",
      "\n",
      "Epoch 12, Batch 525\n",
      "Training Loss: 1.8869589567184448\n",
      "\n",
      "Epoch 12, Batch 550\n",
      "Training Loss: 1.7473341226577759\n",
      "\n",
      "Epoch 12, Batch 575\n",
      "Training Loss: 1.494393229484558\n",
      "\n",
      "Epoch 12, Batch 600\n",
      "Training Loss: 1.772437334060669\n",
      "\n",
      "Epoch 12, Batch 625\n",
      "Training Loss: 1.7560911178588867\n",
      "\n",
      "Epoch 12, Batch 650\n",
      "Training Loss: 1.5692297220230103\n",
      "\n",
      "Epoch 12, Batch 675\n",
      "Training Loss: 2.000728130340576\n",
      "\n",
      "Epoch 12, Batch 700\n",
      "Training Loss: 1.544863224029541\n",
      "\n",
      "Epoch 12, Batch 725\n",
      "Training Loss: 1.770716905593872\n",
      "\n",
      "Epoch 12, Batch 750\n",
      "Training Loss: 1.5057891607284546\n",
      "\n",
      "Epoch 12, Batch 775\n",
      "Training Loss: 1.6100572347640991\n",
      "\n",
      "Epoch 12, Batch 800\n",
      "Training Loss: 1.7003148794174194\n",
      "\n",
      "Epoch 12, Batch 825\n",
      "Training Loss: 1.6999123096466064\n",
      "\n",
      "Epoch 12, Batch 850\n",
      "Training Loss: 1.6462349891662598\n",
      "\n",
      "Epoch 12, Batch 875\n",
      "Training Loss: 1.486234426498413\n",
      "\n",
      "Epoch 12, Batch 900\n",
      "Training Loss: 1.746856927871704\n",
      "\n",
      "Epoch 12, Batch 925\n",
      "Training Loss: 1.9854532480239868\n",
      "\n",
      "Epoch 12, Batch 950\n",
      "Training Loss: 1.3836402893066406\n",
      "\n",
      "Epoch 12, Batch 975\n",
      "Training Loss: 1.4951462745666504\n",
      "\n",
      "Epoch 12, Batch 1000\n",
      "Training Loss: 2.0793776512145996\n",
      "\n",
      "Epoch 12, Batch 1025\n",
      "Training Loss: 1.5100582838058472\n",
      "\n",
      "Epoch 12, Batch 1050\n",
      "Training Loss: 1.4397311210632324\n",
      "\n",
      "Epoch 12, Batch 1075\n",
      "Training Loss: 1.7167588472366333\n",
      "\n",
      "Epoch 12, Batch 1100\n",
      "Training Loss: 1.8159003257751465\n",
      "\n",
      "Epoch 12, Batch 1125\n",
      "Training Loss: 1.9188828468322754\n",
      "\n",
      "Epoch 12, Batch 1150\n",
      "Training Loss: 1.654884696006775\n",
      "\n",
      "Epoch 12, Batch 1175\n",
      "Training Loss: 1.7028721570968628\n",
      "\n",
      "Epoch 12, Batch 1200\n",
      "Training Loss: 1.8476063013076782\n",
      "\n",
      "Epoch 12, Batch 1225\n",
      "Training Loss: 1.6989922523498535\n",
      "\n",
      "Epoch 12, Batch 1250\n",
      "Training Loss: 1.4793328046798706\n",
      "\n",
      "Epoch 12, Batch 1275\n",
      "Training Loss: 1.8727643489837646\n",
      "\n",
      "Epoch 12, Batch 1300\n",
      "Training Loss: 1.8367693424224854\n",
      "\n",
      "Epoch 12, Batch 1325\n",
      "Training Loss: 1.6353654861450195\n",
      "\n",
      "Epoch 12, Batch 1350\n",
      "Training Loss: 1.6328541040420532\n",
      "\n",
      "Epoch 12, Batch 1375\n",
      "Training Loss: 1.6132237911224365\n",
      "\n",
      "Epoch 12, Batch 1400\n",
      "Training Loss: 1.38899564743042\n",
      "\n",
      "Epoch 12, Batch 1425\n",
      "Training Loss: 1.8467614650726318\n",
      "\n",
      "Epoch 12, Batch 1450\n",
      "Training Loss: 1.994905710220337\n",
      "\n",
      "Epoch 12, Batch 1475\n",
      "Training Loss: 1.6677333116531372\n",
      "\n",
      "Epoch 12, Batch 1500\n",
      "Training Loss: 1.9509001970291138\n",
      "\n",
      "Epoch 12, Batch 1525\n",
      "Training Loss: 1.7887020111083984\n",
      "\n",
      "Epoch 12, Batch 1550\n",
      "Training Loss: 1.6077615022659302\n",
      "\n",
      "Epoch 12, Batch 1575\n",
      "Training Loss: 1.5739835500717163\n",
      "\n",
      "Epoch 12, Batch 1600\n",
      "Training Loss: 1.4643405675888062\n",
      "\n",
      "Epoch 12, Batch 1625\n",
      "Training Loss: 2.1371383666992188\n",
      "\n",
      "Epoch 12, Batch 1650\n",
      "Training Loss: 1.7667624950408936\n",
      "\n",
      "Epoch 12, Batch 1675\n",
      "Training Loss: 1.615972638130188\n",
      "\n",
      "Epoch 12, Batch 1700\n",
      "Training Loss: 1.9518619775772095\n",
      "\n",
      "Epoch 12, Batch 1725\n",
      "Training Loss: 1.4239789247512817\n",
      "\n",
      "Epoch 12, Batch 1750\n",
      "Training Loss: 1.8430469036102295\n",
      "\n",
      "Epoch 12, Batch 1775\n",
      "Training Loss: 1.3828446865081787\n",
      "\n",
      "Epoch 12, Batch 1800\n",
      "Training Loss: 1.6274511814117432\n",
      "\n",
      "Epoch 12, Batch 1825\n",
      "Training Loss: 2.1656506061553955\n",
      "\n",
      "Epoch 12, Batch 1850\n",
      "Training Loss: 1.7150254249572754\n",
      "\n",
      "Epoch 12, Batch 1875\n",
      "Training Loss: 1.4836875200271606\n",
      "\n",
      "Epoch 12, Batch 1900\n",
      "Training Loss: 1.58334219455719\n",
      "\n",
      "Epoch 12, Batch 1925\n",
      "Training Loss: 1.6792737245559692\n",
      "\n",
      "Epoch 12, Batch 1950\n",
      "Training Loss: 1.5227503776550293\n",
      "\n",
      "Epoch 12, Batch 1975\n",
      "Training Loss: 1.7554991245269775\n",
      "\n",
      "Epoch 12, Batch 2000\n",
      "Training Loss: 1.641367793083191\n",
      "\n",
      "Epoch 12, Batch 2025\n",
      "Training Loss: 1.4356449842453003\n",
      "\n",
      "Epoch 12, Batch 2050\n",
      "Training Loss: 1.5834336280822754\n",
      "\n",
      "Epoch 12, Batch 2075\n",
      "Training Loss: 1.7252765893936157\n",
      "\n",
      "Epoch 12, Batch 2100\n",
      "Training Loss: 1.7283076047897339\n",
      "\n",
      "Epoch 12, Batch 2125\n",
      "Training Loss: 1.8039658069610596\n",
      "\n",
      "Epoch 12, Batch 2150\n",
      "Training Loss: 1.739658236503601\n",
      "\n",
      "Epoch 12, Batch 2175\n",
      "Training Loss: 1.659131646156311\n",
      "\n",
      "Epoch 12, Batch 2200\n",
      "Training Loss: 1.5610408782958984\n",
      "\n",
      "Epoch 12, Batch 2225\n",
      "Training Loss: 1.707812786102295\n",
      "\n",
      "Epoch 12, Batch 2250\n",
      "Training Loss: 1.5550984144210815\n",
      "\n",
      "Epoch 12, Batch 2275\n",
      "Training Loss: 1.8018437623977661\n",
      "\n",
      "Epoch 12, Batch 2300\n",
      "Training Loss: 1.8229396343231201\n",
      "\n",
      "Epoch 12, Batch 2325\n",
      "Training Loss: 1.6525324583053589\n",
      "\n",
      "Epoch 12, Batch 2350\n",
      "Training Loss: 1.8594213724136353\n",
      "\n",
      "Epoch 12, Batch 2375\n",
      "Training Loss: 1.8997457027435303\n",
      "\n",
      "Epoch 12, Batch 2400\n",
      "Training Loss: 1.7269080877304077\n",
      "\n",
      "Epoch 12, Batch 2425\n",
      "Training Loss: 2.0535781383514404\n",
      "\n",
      "Epoch 12, Batch 2450\n",
      "Training Loss: 1.7193564176559448\n",
      "\n",
      "Epoch 12, Batch 2475\n",
      "Training Loss: 1.619242548942566\n",
      "\n",
      "Epoch 12, Batch 2500\n",
      "Training Loss: 1.4045493602752686\n",
      "\n",
      "Epoch 12, Batch 2525\n",
      "Training Loss: 1.6317583322525024\n",
      "\n",
      "Epoch 12, Batch 2550\n",
      "Training Loss: 1.7777036428451538\n",
      "\n",
      "Epoch 12, Batch 2575\n",
      "Training Loss: 1.76858651638031\n",
      "\n",
      "Epoch 12, Batch 2600\n",
      "Training Loss: 1.6545546054840088\n",
      "\n",
      "Epoch 12, Batch 2625\n",
      "Training Loss: 1.554608702659607\n",
      "\n",
      "Epoch 12, Batch 2650\n",
      "Training Loss: 1.8134323358535767\n",
      "\n",
      "Epoch 12, Batch 2675\n",
      "Training Loss: 1.6929025650024414\n",
      "\n",
      "Epoch 12, Batch 2700\n",
      "Training Loss: 2.2048370838165283\n",
      "\n",
      "Epoch 12, Batch 2725\n",
      "Training Loss: 1.9071764945983887\n",
      "\n",
      "Epoch 12, Batch 2750\n",
      "Training Loss: 1.7120994329452515\n",
      "\n",
      "Epoch 12, Batch 2775\n",
      "Training Loss: 2.1090662479400635\n",
      "\n",
      "Epoch 12, Batch 2800\n",
      "Training Loss: 1.5413833856582642\n",
      "\n",
      "Epoch 12, Batch 2825\n",
      "Training Loss: 1.4363794326782227\n",
      "\n",
      "Epoch 12, Batch 2850\n",
      "Training Loss: 1.8043617010116577\n",
      "\n",
      "Epoch 12, Batch 2875\n",
      "Training Loss: 1.5413415431976318\n",
      "\n",
      "Epoch 12, Batch 2900\n",
      "Training Loss: 1.9767158031463623\n",
      "\n",
      "Epoch 12, Batch 2925\n",
      "Training Loss: 1.6955047845840454\n",
      "\n",
      "Epoch 12, Batch 2950\n",
      "Training Loss: 1.3825169801712036\n",
      "\n",
      "Epoch 12/20\n",
      "Training Loss: 1.72274911403656\n",
      "\n",
      "Epoch 13, Batch 0\n",
      "Training Loss: 1.809287667274475\n",
      "\n",
      "Epoch 13, Batch 25\n",
      "Training Loss: 1.771920084953308\n",
      "\n",
      "Epoch 13, Batch 50\n",
      "Training Loss: 1.318462610244751\n",
      "\n",
      "Epoch 13, Batch 75\n",
      "Training Loss: 1.3365283012390137\n",
      "\n",
      "Epoch 13, Batch 100\n",
      "Training Loss: 1.4345951080322266\n",
      "\n",
      "Epoch 13, Batch 125\n",
      "Training Loss: 1.6774563789367676\n",
      "\n",
      "Epoch 13, Batch 150\n",
      "Training Loss: 1.6931654214859009\n",
      "\n",
      "Epoch 13, Batch 175\n",
      "Training Loss: 1.8000812530517578\n",
      "\n",
      "Epoch 13, Batch 200\n",
      "Training Loss: 1.7859108448028564\n",
      "\n",
      "Epoch 13, Batch 225\n",
      "Training Loss: 1.5302433967590332\n",
      "\n",
      "Epoch 13, Batch 250\n",
      "Training Loss: 1.674682378768921\n",
      "\n",
      "Epoch 13, Batch 275\n",
      "Training Loss: 1.4988030195236206\n",
      "\n",
      "Epoch 13, Batch 300\n",
      "Training Loss: 1.337554693222046\n",
      "\n",
      "Epoch 13, Batch 325\n",
      "Training Loss: 1.3442564010620117\n",
      "\n",
      "Epoch 13, Batch 350\n",
      "Training Loss: 1.6412622928619385\n",
      "\n",
      "Epoch 13, Batch 375\n",
      "Training Loss: 1.925367832183838\n",
      "\n",
      "Epoch 13, Batch 400\n",
      "Training Loss: 1.7928986549377441\n",
      "\n",
      "Epoch 13, Batch 425\n",
      "Training Loss: 1.9010628461837769\n",
      "\n",
      "Epoch 13, Batch 450\n",
      "Training Loss: 1.4628814458847046\n",
      "\n",
      "Epoch 13, Batch 475\n",
      "Training Loss: 1.3773974180221558\n",
      "\n",
      "Epoch 13, Batch 500\n",
      "Training Loss: 1.6893982887268066\n",
      "\n",
      "Epoch 13, Batch 525\n",
      "Training Loss: 1.9460593461990356\n",
      "\n",
      "Epoch 13, Batch 550\n",
      "Training Loss: 1.6348220109939575\n",
      "\n",
      "Epoch 13, Batch 575\n",
      "Training Loss: 1.2929891347885132\n",
      "\n",
      "Epoch 13, Batch 600\n",
      "Training Loss: 1.780186414718628\n",
      "\n",
      "Epoch 13, Batch 625\n",
      "Training Loss: 1.5687353610992432\n",
      "\n",
      "Epoch 13, Batch 650\n",
      "Training Loss: 1.8623796701431274\n",
      "\n",
      "Epoch 13, Batch 675\n",
      "Training Loss: 1.5951437950134277\n",
      "\n",
      "Epoch 13, Batch 700\n",
      "Training Loss: 1.4666156768798828\n",
      "\n",
      "Epoch 13, Batch 725\n",
      "Training Loss: 1.6914176940917969\n",
      "\n",
      "Epoch 13, Batch 750\n",
      "Training Loss: 1.8833305835723877\n",
      "\n",
      "Epoch 13, Batch 775\n",
      "Training Loss: 1.4176303148269653\n",
      "\n",
      "Epoch 13, Batch 800\n",
      "Training Loss: 1.4782137870788574\n",
      "\n",
      "Epoch 13, Batch 825\n",
      "Training Loss: 1.5042742490768433\n",
      "\n",
      "Epoch 13, Batch 850\n",
      "Training Loss: 1.9338418245315552\n",
      "\n",
      "Epoch 13, Batch 875\n",
      "Training Loss: 1.6935632228851318\n",
      "\n",
      "Epoch 13, Batch 900\n",
      "Training Loss: 1.57288658618927\n",
      "\n",
      "Epoch 13, Batch 925\n",
      "Training Loss: 1.8135489225387573\n",
      "\n",
      "Epoch 13, Batch 950\n",
      "Training Loss: 1.7967283725738525\n",
      "\n",
      "Epoch 13, Batch 975\n",
      "Training Loss: 1.6173818111419678\n",
      "\n",
      "Epoch 13, Batch 1000\n",
      "Training Loss: 1.8481979370117188\n",
      "\n",
      "Epoch 13, Batch 1025\n",
      "Training Loss: 1.6570318937301636\n",
      "\n",
      "Epoch 13, Batch 1050\n",
      "Training Loss: 1.4880245923995972\n",
      "\n",
      "Epoch 13, Batch 1075\n",
      "Training Loss: 1.2672070264816284\n",
      "\n",
      "Epoch 13, Batch 1100\n",
      "Training Loss: 1.5889886617660522\n",
      "\n",
      "Epoch 13, Batch 1125\n",
      "Training Loss: 1.7828067541122437\n",
      "\n",
      "Epoch 13, Batch 1150\n",
      "Training Loss: 1.5028899908065796\n",
      "\n",
      "Epoch 13, Batch 1175\n",
      "Training Loss: 1.8605091571807861\n",
      "\n",
      "Epoch 13, Batch 1200\n",
      "Training Loss: 1.4626518487930298\n",
      "\n",
      "Epoch 13, Batch 1225\n",
      "Training Loss: 2.060307502746582\n",
      "\n",
      "Epoch 13, Batch 1250\n",
      "Training Loss: 1.5378265380859375\n",
      "\n",
      "Epoch 13, Batch 1275\n",
      "Training Loss: 1.3704644441604614\n",
      "\n",
      "Epoch 13, Batch 1300\n",
      "Training Loss: 1.7683583498001099\n",
      "\n",
      "Epoch 13, Batch 1325\n",
      "Training Loss: 1.9542012214660645\n",
      "\n",
      "Epoch 13, Batch 1350\n",
      "Training Loss: 1.7967462539672852\n",
      "\n",
      "Epoch 13, Batch 1375\n",
      "Training Loss: 1.5687826871871948\n",
      "\n",
      "Epoch 13, Batch 1400\n",
      "Training Loss: 1.817456841468811\n",
      "\n",
      "Epoch 13, Batch 1425\n",
      "Training Loss: 1.5868149995803833\n",
      "\n",
      "Epoch 13, Batch 1450\n",
      "Training Loss: 1.7800430059432983\n",
      "\n",
      "Epoch 13, Batch 1475\n",
      "Training Loss: 1.8657164573669434\n",
      "\n",
      "Epoch 13, Batch 1500\n",
      "Training Loss: 1.7726187705993652\n",
      "\n",
      "Epoch 13, Batch 1525\n",
      "Training Loss: 1.6850099563598633\n",
      "\n",
      "Epoch 13, Batch 1550\n",
      "Training Loss: 1.584475040435791\n",
      "\n",
      "Epoch 13, Batch 1575\n",
      "Training Loss: 1.8532824516296387\n",
      "\n",
      "Epoch 13, Batch 1600\n",
      "Training Loss: 1.8686809539794922\n",
      "\n",
      "Epoch 13, Batch 1625\n",
      "Training Loss: 1.9996469020843506\n",
      "\n",
      "Epoch 13, Batch 1650\n",
      "Training Loss: 1.4897810220718384\n",
      "\n",
      "Epoch 13, Batch 1675\n",
      "Training Loss: 2.1073291301727295\n",
      "\n",
      "Epoch 13, Batch 1700\n",
      "Training Loss: 1.9402692317962646\n",
      "\n",
      "Epoch 13, Batch 1725\n",
      "Training Loss: 1.792917251586914\n",
      "\n",
      "Epoch 13, Batch 1750\n",
      "Training Loss: 1.7671209573745728\n",
      "\n",
      "Epoch 13, Batch 1775\n",
      "Training Loss: 1.7710883617401123\n",
      "\n",
      "Epoch 13, Batch 1800\n",
      "Training Loss: 1.4743335247039795\n",
      "\n",
      "Epoch 13, Batch 1825\n",
      "Training Loss: 1.9770368337631226\n",
      "\n",
      "Epoch 13, Batch 1850\n",
      "Training Loss: 1.4582455158233643\n",
      "\n",
      "Epoch 13, Batch 1875\n",
      "Training Loss: 2.014423370361328\n",
      "\n",
      "Epoch 13, Batch 1900\n",
      "Training Loss: 1.6362781524658203\n",
      "\n",
      "Epoch 13, Batch 1925\n",
      "Training Loss: 1.7803386449813843\n",
      "\n",
      "Epoch 13, Batch 1950\n",
      "Training Loss: 1.6989545822143555\n",
      "\n",
      "Epoch 13, Batch 1975\n",
      "Training Loss: 1.7379199266433716\n",
      "\n",
      "Epoch 13, Batch 2000\n",
      "Training Loss: 1.6502126455307007\n",
      "\n",
      "Epoch 13, Batch 2025\n",
      "Training Loss: 1.6615146398544312\n",
      "\n",
      "Epoch 13, Batch 2050\n",
      "Training Loss: 1.9102580547332764\n",
      "\n",
      "Epoch 13, Batch 2075\n",
      "Training Loss: 1.5304863452911377\n",
      "\n",
      "Epoch 13, Batch 2100\n",
      "Training Loss: 2.010094404220581\n",
      "\n",
      "Epoch 13, Batch 2125\n",
      "Training Loss: 1.699500322341919\n",
      "\n",
      "Epoch 13, Batch 2150\n",
      "Training Loss: 1.77378511428833\n",
      "\n",
      "Epoch 13, Batch 2175\n",
      "Training Loss: 1.8712748289108276\n",
      "\n",
      "Epoch 13, Batch 2200\n",
      "Training Loss: 1.7015725374221802\n",
      "\n",
      "Epoch 13, Batch 2225\n",
      "Training Loss: 1.6095153093338013\n",
      "\n",
      "Epoch 13, Batch 2250\n",
      "Training Loss: 1.4729492664337158\n",
      "\n",
      "Epoch 13, Batch 2275\n",
      "Training Loss: 2.074615955352783\n",
      "\n",
      "Epoch 13, Batch 2300\n",
      "Training Loss: 1.4628568887710571\n",
      "\n",
      "Epoch 13, Batch 2325\n",
      "Training Loss: 1.7697032690048218\n",
      "\n",
      "Epoch 13, Batch 2350\n",
      "Training Loss: 1.726295828819275\n",
      "\n",
      "Epoch 13, Batch 2375\n",
      "Training Loss: 1.762987494468689\n",
      "\n",
      "Epoch 13, Batch 2400\n",
      "Training Loss: 1.6336568593978882\n",
      "\n",
      "Epoch 13, Batch 2425\n",
      "Training Loss: 1.660879135131836\n",
      "\n",
      "Epoch 13, Batch 2450\n",
      "Training Loss: 1.1492074728012085\n",
      "\n",
      "Epoch 13, Batch 2475\n",
      "Training Loss: 1.7215231657028198\n",
      "\n",
      "Epoch 13, Batch 2500\n",
      "Training Loss: 1.3716020584106445\n",
      "\n",
      "Epoch 13, Batch 2525\n",
      "Training Loss: 1.963168740272522\n",
      "\n",
      "Epoch 13, Batch 2550\n",
      "Training Loss: 1.7060738801956177\n",
      "\n",
      "Epoch 13, Batch 2575\n",
      "Training Loss: 1.8847306966781616\n",
      "\n",
      "Epoch 13, Batch 2600\n",
      "Training Loss: 1.934790015220642\n",
      "\n",
      "Epoch 13, Batch 2625\n",
      "Training Loss: 1.390997052192688\n",
      "\n",
      "Epoch 13, Batch 2650\n",
      "Training Loss: 1.514237880706787\n",
      "\n",
      "Epoch 13, Batch 2675\n",
      "Training Loss: 1.5699431896209717\n",
      "\n",
      "Epoch 13, Batch 2700\n",
      "Training Loss: 1.5272105932235718\n",
      "\n",
      "Epoch 13, Batch 2725\n",
      "Training Loss: 1.7770845890045166\n",
      "\n",
      "Epoch 13, Batch 2750\n",
      "Training Loss: 1.673020362854004\n",
      "\n",
      "Epoch 13, Batch 2775\n",
      "Training Loss: 2.2273595333099365\n",
      "\n",
      "Epoch 13, Batch 2800\n",
      "Training Loss: 1.896710991859436\n",
      "\n",
      "Epoch 13, Batch 2825\n",
      "Training Loss: 1.424891710281372\n",
      "\n",
      "Epoch 13, Batch 2850\n",
      "Training Loss: 1.6530343294143677\n",
      "\n",
      "Epoch 13, Batch 2875\n",
      "Training Loss: 2.1219687461853027\n",
      "\n",
      "Epoch 13, Batch 2900\n",
      "Training Loss: 1.608743667602539\n",
      "\n",
      "Epoch 13, Batch 2925\n",
      "Training Loss: 1.6844168901443481\n",
      "\n",
      "Epoch 13, Batch 2950\n",
      "Training Loss: 1.8196803331375122\n",
      "\n",
      "Epoch 13/20\n",
      "Training Loss: 1.7159520387649536\n",
      "\n",
      "Epoch 14, Batch 0\n",
      "Training Loss: 1.4263298511505127\n",
      "\n",
      "Epoch 14, Batch 25\n",
      "Training Loss: 1.9798673391342163\n",
      "\n",
      "Epoch 14, Batch 50\n",
      "Training Loss: 1.628237009048462\n",
      "\n",
      "Epoch 14, Batch 75\n",
      "Training Loss: 1.563779354095459\n",
      "\n",
      "Epoch 14, Batch 100\n",
      "Training Loss: 1.7901835441589355\n",
      "\n",
      "Epoch 14, Batch 125\n",
      "Training Loss: 1.4271889925003052\n",
      "\n",
      "Epoch 14, Batch 150\n",
      "Training Loss: 1.6169960498809814\n",
      "\n",
      "Epoch 14, Batch 175\n",
      "Training Loss: 1.632912278175354\n",
      "\n",
      "Epoch 14, Batch 200\n",
      "Training Loss: 1.8082345724105835\n",
      "\n",
      "Epoch 14, Batch 225\n",
      "Training Loss: 1.6288890838623047\n",
      "\n",
      "Epoch 14, Batch 250\n",
      "Training Loss: 1.7484298944473267\n",
      "\n",
      "Epoch 14, Batch 275\n",
      "Training Loss: 1.6938202381134033\n",
      "\n",
      "Epoch 14, Batch 300\n",
      "Training Loss: 1.554988980293274\n",
      "\n",
      "Epoch 14, Batch 325\n",
      "Training Loss: 1.8236620426177979\n",
      "\n",
      "Epoch 14, Batch 350\n",
      "Training Loss: 2.067190170288086\n",
      "\n",
      "Epoch 14, Batch 375\n",
      "Training Loss: 1.789466381072998\n",
      "\n",
      "Epoch 14, Batch 400\n",
      "Training Loss: 1.3286348581314087\n",
      "\n",
      "Epoch 14, Batch 425\n",
      "Training Loss: 1.8584681749343872\n",
      "\n",
      "Epoch 14, Batch 450\n",
      "Training Loss: 1.5601911544799805\n",
      "\n",
      "Epoch 14, Batch 475\n",
      "Training Loss: 1.533775806427002\n",
      "\n",
      "Epoch 14, Batch 500\n",
      "Training Loss: 2.086794137954712\n",
      "\n",
      "Epoch 14, Batch 525\n",
      "Training Loss: 1.902995228767395\n",
      "\n",
      "Epoch 14, Batch 550\n",
      "Training Loss: 1.8500964641571045\n",
      "\n",
      "Epoch 14, Batch 575\n",
      "Training Loss: 1.5493370294570923\n",
      "\n",
      "Epoch 14, Batch 600\n",
      "Training Loss: 1.8602863550186157\n",
      "\n",
      "Epoch 14, Batch 625\n",
      "Training Loss: 1.823142409324646\n",
      "\n",
      "Epoch 14, Batch 650\n",
      "Training Loss: 1.499650239944458\n",
      "\n",
      "Epoch 14, Batch 675\n",
      "Training Loss: 1.657593846321106\n",
      "\n",
      "Epoch 14, Batch 700\n",
      "Training Loss: 1.4539135694503784\n",
      "\n",
      "Epoch 14, Batch 725\n",
      "Training Loss: 1.618577241897583\n",
      "\n",
      "Epoch 14, Batch 750\n",
      "Training Loss: 1.7928364276885986\n",
      "\n",
      "Epoch 14, Batch 775\n",
      "Training Loss: 1.7584683895111084\n",
      "\n",
      "Epoch 14, Batch 800\n",
      "Training Loss: 1.6756304502487183\n",
      "\n",
      "Epoch 14, Batch 825\n",
      "Training Loss: 1.867836594581604\n",
      "\n",
      "Epoch 14, Batch 850\n",
      "Training Loss: 1.3022903203964233\n",
      "\n",
      "Epoch 14, Batch 875\n",
      "Training Loss: 1.775089979171753\n",
      "\n",
      "Epoch 14, Batch 900\n",
      "Training Loss: 1.4242149591445923\n",
      "\n",
      "Epoch 14, Batch 925\n",
      "Training Loss: 1.565801739692688\n",
      "\n",
      "Epoch 14, Batch 950\n",
      "Training Loss: 2.0599467754364014\n",
      "\n",
      "Epoch 14, Batch 975\n",
      "Training Loss: 1.809324026107788\n",
      "\n",
      "Epoch 14, Batch 1000\n",
      "Training Loss: 1.7615067958831787\n",
      "\n",
      "Epoch 14, Batch 1025\n",
      "Training Loss: 1.4041410684585571\n",
      "\n",
      "Epoch 14, Batch 1050\n",
      "Training Loss: 1.6811175346374512\n",
      "\n",
      "Epoch 14, Batch 1075\n",
      "Training Loss: 1.684118390083313\n",
      "\n",
      "Epoch 14, Batch 1100\n",
      "Training Loss: 1.6852351427078247\n",
      "\n",
      "Epoch 14, Batch 1125\n",
      "Training Loss: 1.5628149509429932\n",
      "\n",
      "Epoch 14, Batch 1150\n",
      "Training Loss: 1.8006548881530762\n",
      "\n",
      "Epoch 14, Batch 1175\n",
      "Training Loss: 1.43259859085083\n",
      "\n",
      "Epoch 14, Batch 1200\n",
      "Training Loss: 1.4211785793304443\n",
      "\n",
      "Epoch 14, Batch 1225\n",
      "Training Loss: 1.3571678400039673\n",
      "\n",
      "Epoch 14, Batch 1250\n",
      "Training Loss: 1.4775042533874512\n",
      "\n",
      "Epoch 14, Batch 1275\n",
      "Training Loss: 1.5360329151153564\n",
      "\n",
      "Epoch 14, Batch 1300\n",
      "Training Loss: 1.498712420463562\n",
      "\n",
      "Epoch 14, Batch 1325\n",
      "Training Loss: 1.6249871253967285\n",
      "\n",
      "Epoch 14, Batch 1350\n",
      "Training Loss: 1.4214825630187988\n",
      "\n",
      "Epoch 14, Batch 1375\n",
      "Training Loss: 1.8029245138168335\n",
      "\n",
      "Epoch 14, Batch 1400\n",
      "Training Loss: 1.4712573289871216\n",
      "\n",
      "Epoch 14, Batch 1425\n",
      "Training Loss: 1.867710828781128\n",
      "\n",
      "Epoch 14, Batch 1450\n",
      "Training Loss: 1.8594032526016235\n",
      "\n",
      "Epoch 14, Batch 1475\n",
      "Training Loss: 1.6941266059875488\n",
      "\n",
      "Epoch 14, Batch 1500\n",
      "Training Loss: 1.7873430252075195\n",
      "\n",
      "Epoch 14, Batch 1525\n",
      "Training Loss: 1.4988752603530884\n",
      "\n",
      "Epoch 14, Batch 1550\n",
      "Training Loss: 1.692409873008728\n",
      "\n",
      "Epoch 14, Batch 1575\n",
      "Training Loss: 1.5446043014526367\n",
      "\n",
      "Epoch 14, Batch 1600\n",
      "Training Loss: 1.5157119035720825\n",
      "\n",
      "Epoch 14, Batch 1625\n",
      "Training Loss: 1.6076796054840088\n",
      "\n",
      "Epoch 14, Batch 1650\n",
      "Training Loss: 1.691622018814087\n",
      "\n",
      "Epoch 14, Batch 1675\n",
      "Training Loss: 1.456699013710022\n",
      "\n",
      "Epoch 14, Batch 1700\n",
      "Training Loss: 1.628160834312439\n",
      "\n",
      "Epoch 14, Batch 1725\n",
      "Training Loss: 1.6648056507110596\n",
      "\n",
      "Epoch 14, Batch 1750\n",
      "Training Loss: 1.400779366493225\n",
      "\n",
      "Epoch 14, Batch 1775\n",
      "Training Loss: 1.6375981569290161\n",
      "\n",
      "Epoch 14, Batch 1800\n",
      "Training Loss: 1.815808892250061\n",
      "\n",
      "Epoch 14, Batch 1825\n",
      "Training Loss: 1.761296033859253\n",
      "\n",
      "Epoch 14, Batch 1850\n",
      "Training Loss: 1.8713257312774658\n",
      "\n",
      "Epoch 14, Batch 1875\n",
      "Training Loss: 1.4127044677734375\n",
      "\n",
      "Epoch 14, Batch 1900\n",
      "Training Loss: 1.6950294971466064\n",
      "\n",
      "Epoch 14, Batch 1925\n",
      "Training Loss: 1.5680800676345825\n",
      "\n",
      "Epoch 14, Batch 1950\n",
      "Training Loss: 1.916882872581482\n",
      "\n",
      "Epoch 14, Batch 1975\n",
      "Training Loss: 1.4260435104370117\n",
      "\n",
      "Epoch 14, Batch 2000\n",
      "Training Loss: 1.7464711666107178\n",
      "\n",
      "Epoch 14, Batch 2025\n",
      "Training Loss: 1.8943345546722412\n",
      "\n",
      "Epoch 14, Batch 2050\n",
      "Training Loss: 1.9207627773284912\n",
      "\n",
      "Epoch 14, Batch 2075\n",
      "Training Loss: 1.6888841390609741\n",
      "\n",
      "Epoch 14, Batch 2100\n",
      "Training Loss: 1.8451299667358398\n",
      "\n",
      "Epoch 14, Batch 2125\n",
      "Training Loss: 1.4217206239700317\n",
      "\n",
      "Epoch 14, Batch 2150\n",
      "Training Loss: 1.4108186960220337\n",
      "\n",
      "Epoch 14, Batch 2175\n",
      "Training Loss: 1.562665581703186\n",
      "\n",
      "Epoch 14, Batch 2200\n",
      "Training Loss: 1.6453317403793335\n",
      "\n",
      "Epoch 14, Batch 2225\n",
      "Training Loss: 1.6237002611160278\n",
      "\n",
      "Epoch 14, Batch 2250\n",
      "Training Loss: 1.5086363554000854\n",
      "\n",
      "Epoch 14, Batch 2275\n",
      "Training Loss: 1.3747624158859253\n",
      "\n",
      "Epoch 14, Batch 2300\n",
      "Training Loss: 1.7250949144363403\n",
      "\n",
      "Epoch 14, Batch 2325\n",
      "Training Loss: 1.4959914684295654\n",
      "\n",
      "Epoch 14, Batch 2350\n",
      "Training Loss: 1.3659420013427734\n",
      "\n",
      "Epoch 14, Batch 2375\n",
      "Training Loss: 1.5890775918960571\n",
      "\n",
      "Epoch 14, Batch 2400\n",
      "Training Loss: 1.6928154230117798\n",
      "\n",
      "Epoch 14, Batch 2425\n",
      "Training Loss: 1.6366147994995117\n",
      "\n",
      "Epoch 14, Batch 2450\n",
      "Training Loss: 1.7705243825912476\n",
      "\n",
      "Epoch 14, Batch 2475\n",
      "Training Loss: 1.835128664970398\n",
      "\n",
      "Epoch 14, Batch 2500\n",
      "Training Loss: 1.5282738208770752\n",
      "\n",
      "Epoch 14, Batch 2525\n",
      "Training Loss: 1.2664817571640015\n",
      "\n",
      "Epoch 14, Batch 2550\n",
      "Training Loss: 1.5341280698776245\n",
      "\n",
      "Epoch 14, Batch 2575\n",
      "Training Loss: 1.637040615081787\n",
      "\n",
      "Epoch 14, Batch 2600\n",
      "Training Loss: 1.5383957624435425\n",
      "\n",
      "Epoch 14, Batch 2625\n",
      "Training Loss: 1.9188950061798096\n",
      "\n",
      "Epoch 14, Batch 2650\n",
      "Training Loss: 1.6736623048782349\n",
      "\n",
      "Epoch 14, Batch 2675\n",
      "Training Loss: 1.6524853706359863\n",
      "\n",
      "Epoch 14, Batch 2700\n",
      "Training Loss: 2.033822536468506\n",
      "\n",
      "Epoch 14, Batch 2725\n",
      "Training Loss: 1.521105170249939\n",
      "\n",
      "Epoch 14, Batch 2750\n",
      "Training Loss: 1.566529393196106\n",
      "\n",
      "Epoch 14, Batch 2775\n",
      "Training Loss: 1.7690192461013794\n",
      "\n",
      "Epoch 14, Batch 2800\n",
      "Training Loss: 1.888535976409912\n",
      "\n",
      "Epoch 14, Batch 2825\n",
      "Training Loss: 1.706986427307129\n",
      "\n",
      "Epoch 14, Batch 2850\n",
      "Training Loss: 1.4600030183792114\n",
      "\n",
      "Epoch 14, Batch 2875\n",
      "Training Loss: 1.8009446859359741\n",
      "\n",
      "Epoch 14, Batch 2900\n",
      "Training Loss: 1.7751672267913818\n",
      "\n",
      "Epoch 14, Batch 2925\n",
      "Training Loss: 1.3409640789031982\n",
      "\n",
      "Epoch 14, Batch 2950\n",
      "Training Loss: 1.7572708129882812\n",
      "\n",
      "Epoch 14/20\n",
      "Training Loss: 2.0062930583953857\n",
      "\n",
      "Epoch 15, Batch 0\n",
      "Training Loss: 1.790766954421997\n",
      "\n",
      "Epoch 15, Batch 25\n",
      "Training Loss: 1.77150297164917\n",
      "\n",
      "Epoch 15, Batch 50\n",
      "Training Loss: 1.5212841033935547\n",
      "\n",
      "Epoch 15, Batch 75\n",
      "Training Loss: 1.7658530473709106\n",
      "\n",
      "Epoch 15, Batch 100\n",
      "Training Loss: 1.6814355850219727\n",
      "\n",
      "Epoch 15, Batch 125\n",
      "Training Loss: 2.112480878829956\n",
      "\n",
      "Epoch 15, Batch 150\n",
      "Training Loss: 1.126792311668396\n",
      "\n",
      "Epoch 15, Batch 175\n",
      "Training Loss: 1.570003628730774\n",
      "\n",
      "Epoch 15, Batch 200\n",
      "Training Loss: 1.4243015050888062\n",
      "\n",
      "Epoch 15, Batch 225\n",
      "Training Loss: 1.7260791063308716\n",
      "\n",
      "Epoch 15, Batch 250\n",
      "Training Loss: 1.419629693031311\n",
      "\n",
      "Epoch 15, Batch 275\n",
      "Training Loss: 1.5621949434280396\n",
      "\n",
      "Epoch 15, Batch 300\n",
      "Training Loss: 1.6739140748977661\n",
      "\n",
      "Epoch 15, Batch 325\n",
      "Training Loss: 1.5026367902755737\n",
      "\n",
      "Epoch 15, Batch 350\n",
      "Training Loss: 1.7430437803268433\n",
      "\n",
      "Epoch 15, Batch 375\n",
      "Training Loss: 1.641133189201355\n",
      "\n",
      "Epoch 15, Batch 400\n",
      "Training Loss: 1.3209296464920044\n",
      "\n",
      "Epoch 15, Batch 425\n",
      "Training Loss: 1.402778148651123\n",
      "\n",
      "Epoch 15, Batch 450\n",
      "Training Loss: 1.57106614112854\n",
      "\n",
      "Epoch 15, Batch 475\n",
      "Training Loss: 1.7040237188339233\n",
      "\n",
      "Epoch 15, Batch 500\n",
      "Training Loss: 1.76035737991333\n",
      "\n",
      "Epoch 15, Batch 525\n",
      "Training Loss: 1.7117024660110474\n",
      "\n",
      "Epoch 15, Batch 550\n",
      "Training Loss: 1.3643749952316284\n",
      "\n",
      "Epoch 15, Batch 575\n",
      "Training Loss: 1.3739173412322998\n",
      "\n",
      "Epoch 15, Batch 600\n",
      "Training Loss: 1.501617193222046\n",
      "\n",
      "Epoch 15, Batch 625\n",
      "Training Loss: 1.6305729150772095\n",
      "\n",
      "Epoch 15, Batch 650\n",
      "Training Loss: 1.6123372316360474\n",
      "\n",
      "Epoch 15, Batch 675\n",
      "Training Loss: 2.1456525325775146\n",
      "\n",
      "Epoch 15, Batch 700\n",
      "Training Loss: 1.3413952589035034\n",
      "\n",
      "Epoch 15, Batch 725\n",
      "Training Loss: 1.164218544960022\n",
      "\n",
      "Epoch 15, Batch 750\n",
      "Training Loss: 1.5551187992095947\n",
      "\n",
      "Epoch 15, Batch 775\n",
      "Training Loss: 1.9440323114395142\n",
      "\n",
      "Epoch 15, Batch 800\n",
      "Training Loss: 1.837247610092163\n",
      "\n",
      "Epoch 15, Batch 825\n",
      "Training Loss: 1.5422427654266357\n",
      "\n",
      "Epoch 15, Batch 850\n",
      "Training Loss: 1.9655907154083252\n",
      "\n",
      "Epoch 15, Batch 875\n",
      "Training Loss: 1.6988697052001953\n",
      "\n",
      "Epoch 15, Batch 900\n",
      "Training Loss: 1.5624698400497437\n",
      "\n",
      "Epoch 15, Batch 925\n",
      "Training Loss: 1.8333371877670288\n",
      "\n",
      "Epoch 15, Batch 950\n",
      "Training Loss: 1.7194386720657349\n",
      "\n",
      "Epoch 15, Batch 975\n",
      "Training Loss: 1.6194531917572021\n",
      "\n",
      "Epoch 15, Batch 1000\n",
      "Training Loss: 1.7678755521774292\n",
      "\n",
      "Epoch 15, Batch 1025\n",
      "Training Loss: 2.3886566162109375\n",
      "\n",
      "Epoch 15, Batch 1050\n",
      "Training Loss: 1.7055199146270752\n",
      "\n",
      "Epoch 15, Batch 1075\n",
      "Training Loss: 1.6587916612625122\n",
      "\n",
      "Epoch 15, Batch 1100\n",
      "Training Loss: 1.4689185619354248\n",
      "\n",
      "Epoch 15, Batch 1125\n",
      "Training Loss: 1.7752822637557983\n",
      "\n",
      "Epoch 15, Batch 1150\n",
      "Training Loss: 1.4834213256835938\n",
      "\n",
      "Epoch 15, Batch 1175\n",
      "Training Loss: 1.6709516048431396\n",
      "\n",
      "Epoch 15, Batch 1200\n",
      "Training Loss: 1.927950143814087\n",
      "\n",
      "Epoch 15, Batch 1225\n",
      "Training Loss: 1.8790491819381714\n",
      "\n",
      "Epoch 15, Batch 1250\n",
      "Training Loss: 1.832342505455017\n",
      "\n",
      "Epoch 15, Batch 1275\n",
      "Training Loss: 1.5608347654342651\n",
      "\n",
      "Epoch 15, Batch 1300\n",
      "Training Loss: 1.3918941020965576\n",
      "\n",
      "Epoch 15, Batch 1325\n",
      "Training Loss: 1.319401741027832\n",
      "\n",
      "Epoch 15, Batch 1350\n",
      "Training Loss: 1.7220780849456787\n",
      "\n",
      "Epoch 15, Batch 1375\n",
      "Training Loss: 1.6696211099624634\n",
      "\n",
      "Epoch 15, Batch 1400\n",
      "Training Loss: 1.6644636392593384\n",
      "\n",
      "Epoch 15, Batch 1425\n",
      "Training Loss: 1.6594948768615723\n",
      "\n",
      "Epoch 15, Batch 1450\n",
      "Training Loss: 1.5894486904144287\n",
      "\n",
      "Epoch 15, Batch 1475\n",
      "Training Loss: 1.4879975318908691\n",
      "\n",
      "Epoch 15, Batch 1500\n",
      "Training Loss: 1.4763797521591187\n",
      "\n",
      "Epoch 15, Batch 1525\n",
      "Training Loss: 1.6315511465072632\n",
      "\n",
      "Epoch 15, Batch 1550\n",
      "Training Loss: 1.4982717037200928\n",
      "\n",
      "Epoch 15, Batch 1575\n",
      "Training Loss: 1.6918190717697144\n",
      "\n",
      "Epoch 15, Batch 1600\n",
      "Training Loss: 1.6281927824020386\n",
      "\n",
      "Epoch 15, Batch 1625\n",
      "Training Loss: 1.510098934173584\n",
      "\n",
      "Epoch 15, Batch 1650\n",
      "Training Loss: 1.609466791152954\n",
      "\n",
      "Epoch 15, Batch 1675\n",
      "Training Loss: 1.4404003620147705\n",
      "\n",
      "Epoch 15, Batch 1700\n",
      "Training Loss: 1.6294357776641846\n",
      "\n",
      "Epoch 15, Batch 1725\n",
      "Training Loss: 1.5442423820495605\n",
      "\n",
      "Epoch 15, Batch 1750\n",
      "Training Loss: 1.7116076946258545\n",
      "\n",
      "Epoch 15, Batch 1775\n",
      "Training Loss: 1.8263006210327148\n",
      "\n",
      "Epoch 15, Batch 1800\n",
      "Training Loss: 1.9772212505340576\n",
      "\n",
      "Epoch 15, Batch 1825\n",
      "Training Loss: 1.732267141342163\n",
      "\n",
      "Epoch 15, Batch 1850\n",
      "Training Loss: 1.562443494796753\n",
      "\n",
      "Epoch 15, Batch 1875\n",
      "Training Loss: 1.562037706375122\n",
      "\n",
      "Epoch 15, Batch 1900\n",
      "Training Loss: 1.4776074886322021\n",
      "\n",
      "Epoch 15, Batch 1925\n",
      "Training Loss: 1.6850457191467285\n",
      "\n",
      "Epoch 15, Batch 1950\n",
      "Training Loss: 1.7344223260879517\n",
      "\n",
      "Epoch 15, Batch 1975\n",
      "Training Loss: 1.569016456604004\n",
      "\n",
      "Epoch 15, Batch 2000\n",
      "Training Loss: 1.5206252336502075\n",
      "\n",
      "Epoch 15, Batch 2025\n",
      "Training Loss: 1.7634073495864868\n",
      "\n",
      "Epoch 15, Batch 2050\n",
      "Training Loss: 1.9864624738693237\n",
      "\n",
      "Epoch 15, Batch 2075\n",
      "Training Loss: 1.7511917352676392\n",
      "\n",
      "Epoch 15, Batch 2100\n",
      "Training Loss: 1.8207802772521973\n",
      "\n",
      "Epoch 15, Batch 2125\n",
      "Training Loss: 1.606547236442566\n",
      "\n",
      "Epoch 15, Batch 2150\n",
      "Training Loss: 1.7934471368789673\n",
      "\n",
      "Epoch 15, Batch 2175\n",
      "Training Loss: 1.9353574514389038\n",
      "\n",
      "Epoch 15, Batch 2200\n",
      "Training Loss: 1.6584399938583374\n",
      "\n",
      "Epoch 15, Batch 2225\n",
      "Training Loss: 1.3999844789505005\n",
      "\n",
      "Epoch 15, Batch 2250\n",
      "Training Loss: 1.9861334562301636\n",
      "\n",
      "Epoch 15, Batch 2275\n",
      "Training Loss: 1.7325752973556519\n",
      "\n",
      "Epoch 15, Batch 2300\n",
      "Training Loss: 1.854748249053955\n",
      "\n",
      "Epoch 15, Batch 2325\n",
      "Training Loss: 1.7291327714920044\n",
      "\n",
      "Epoch 15, Batch 2350\n",
      "Training Loss: 1.7767138481140137\n",
      "\n",
      "Epoch 15, Batch 2375\n",
      "Training Loss: 1.7763028144836426\n",
      "\n",
      "Epoch 15, Batch 2400\n",
      "Training Loss: 1.5175632238388062\n",
      "\n",
      "Epoch 15, Batch 2425\n",
      "Training Loss: 1.7186967134475708\n",
      "\n",
      "Epoch 15, Batch 2450\n",
      "Training Loss: 1.5156581401824951\n",
      "\n",
      "Epoch 15, Batch 2475\n",
      "Training Loss: 1.8426775932312012\n",
      "\n",
      "Epoch 15, Batch 2500\n",
      "Training Loss: 1.9911905527114868\n",
      "\n",
      "Epoch 15, Batch 2525\n",
      "Training Loss: 1.451745629310608\n",
      "\n",
      "Epoch 15, Batch 2550\n",
      "Training Loss: 1.5975358486175537\n",
      "\n",
      "Epoch 15, Batch 2575\n",
      "Training Loss: 1.8848915100097656\n",
      "\n",
      "Epoch 15, Batch 2600\n",
      "Training Loss: 1.6153684854507446\n",
      "\n",
      "Epoch 15, Batch 2625\n",
      "Training Loss: 1.5606929063796997\n",
      "\n",
      "Epoch 15, Batch 2650\n",
      "Training Loss: 1.7856194972991943\n",
      "\n",
      "Epoch 15, Batch 2675\n",
      "Training Loss: 1.422001600265503\n",
      "\n",
      "Epoch 15, Batch 2700\n",
      "Training Loss: 1.8558824062347412\n",
      "\n",
      "Epoch 15, Batch 2725\n",
      "Training Loss: 1.6563575267791748\n",
      "\n",
      "Epoch 15, Batch 2750\n",
      "Training Loss: 1.8395593166351318\n",
      "\n",
      "Epoch 15, Batch 2775\n",
      "Training Loss: 1.461722731590271\n",
      "\n",
      "Epoch 15, Batch 2800\n",
      "Training Loss: 1.524896502494812\n",
      "\n",
      "Epoch 15, Batch 2825\n",
      "Training Loss: 1.4715452194213867\n",
      "\n",
      "Epoch 15, Batch 2850\n",
      "Training Loss: 1.6100687980651855\n",
      "\n",
      "Epoch 15, Batch 2875\n",
      "Training Loss: 1.5218017101287842\n",
      "\n",
      "Epoch 15, Batch 2900\n",
      "Training Loss: 1.6896584033966064\n",
      "\n",
      "Epoch 15, Batch 2925\n",
      "Training Loss: 2.0035746097564697\n",
      "\n",
      "Epoch 15, Batch 2950\n",
      "Training Loss: 1.5668706893920898\n",
      "\n",
      "Epoch 15/20\n",
      "Training Loss: 1.6898380517959595\n",
      "\n",
      "Epoch 16, Batch 0\n",
      "Training Loss: 1.9980504512786865\n",
      "\n",
      "Epoch 16, Batch 25\n",
      "Training Loss: 1.9108026027679443\n",
      "\n",
      "Epoch 16, Batch 50\n",
      "Training Loss: 1.6205154657363892\n",
      "\n",
      "Epoch 16, Batch 75\n",
      "Training Loss: 1.5669203996658325\n",
      "\n",
      "Epoch 16, Batch 100\n",
      "Training Loss: 1.5071065425872803\n",
      "\n",
      "Epoch 16, Batch 125\n",
      "Training Loss: 1.385460376739502\n",
      "\n",
      "Epoch 16, Batch 150\n",
      "Training Loss: 1.5326749086380005\n",
      "\n",
      "Epoch 16, Batch 175\n",
      "Training Loss: 1.4290071725845337\n",
      "\n",
      "Epoch 16, Batch 200\n",
      "Training Loss: 1.5272579193115234\n",
      "\n",
      "Epoch 16, Batch 225\n",
      "Training Loss: 1.526197910308838\n",
      "\n",
      "Epoch 16, Batch 250\n",
      "Training Loss: 1.4015891551971436\n",
      "\n",
      "Epoch 16, Batch 275\n",
      "Training Loss: 1.5891118049621582\n",
      "\n",
      "Epoch 16, Batch 300\n",
      "Training Loss: 1.6852498054504395\n",
      "\n",
      "Epoch 16, Batch 325\n",
      "Training Loss: 1.7420024871826172\n",
      "\n",
      "Epoch 16, Batch 350\n",
      "Training Loss: 2.2032155990600586\n",
      "\n",
      "Epoch 16, Batch 375\n",
      "Training Loss: 1.4952484369277954\n",
      "\n",
      "Epoch 16, Batch 400\n",
      "Training Loss: 1.3209590911865234\n",
      "\n",
      "Epoch 16, Batch 425\n",
      "Training Loss: 2.125584363937378\n",
      "\n",
      "Epoch 16, Batch 450\n",
      "Training Loss: 1.8855183124542236\n",
      "\n",
      "Epoch 16, Batch 475\n",
      "Training Loss: 1.828223705291748\n",
      "\n",
      "Epoch 16, Batch 500\n",
      "Training Loss: 1.705988883972168\n",
      "\n",
      "Epoch 16, Batch 525\n",
      "Training Loss: 1.6800729036331177\n",
      "\n",
      "Epoch 16, Batch 550\n",
      "Training Loss: 1.676530122756958\n",
      "\n",
      "Epoch 16, Batch 575\n",
      "Training Loss: 1.4660711288452148\n",
      "\n",
      "Epoch 16, Batch 600\n",
      "Training Loss: 1.6284916400909424\n",
      "\n",
      "Epoch 16, Batch 625\n",
      "Training Loss: 1.5110459327697754\n",
      "\n",
      "Epoch 16, Batch 650\n",
      "Training Loss: 1.5587036609649658\n",
      "\n",
      "Epoch 16, Batch 675\n",
      "Training Loss: 1.5158296823501587\n",
      "\n",
      "Epoch 16, Batch 700\n",
      "Training Loss: 1.564690113067627\n",
      "\n",
      "Epoch 16, Batch 725\n",
      "Training Loss: 1.7591806650161743\n",
      "\n",
      "Epoch 16, Batch 750\n",
      "Training Loss: 1.6142126321792603\n",
      "\n",
      "Epoch 16, Batch 775\n",
      "Training Loss: 1.442894697189331\n",
      "\n",
      "Epoch 16, Batch 800\n",
      "Training Loss: 1.6414858102798462\n",
      "\n",
      "Epoch 16, Batch 825\n",
      "Training Loss: 1.743168592453003\n",
      "\n",
      "Epoch 16, Batch 850\n",
      "Training Loss: 1.7219642400741577\n",
      "\n",
      "Epoch 16, Batch 875\n",
      "Training Loss: 1.5952470302581787\n",
      "\n",
      "Epoch 16, Batch 900\n",
      "Training Loss: 1.5530251264572144\n",
      "\n",
      "Epoch 16, Batch 925\n",
      "Training Loss: 1.57802152633667\n",
      "\n",
      "Epoch 16, Batch 950\n",
      "Training Loss: 1.4343441724777222\n",
      "\n",
      "Epoch 16, Batch 975\n",
      "Training Loss: 2.009187936782837\n",
      "\n",
      "Epoch 16, Batch 1000\n",
      "Training Loss: 1.664156198501587\n",
      "\n",
      "Epoch 16, Batch 1025\n",
      "Training Loss: 1.5706852674484253\n",
      "\n",
      "Epoch 16, Batch 1050\n",
      "Training Loss: 1.5139710903167725\n",
      "\n",
      "Epoch 16, Batch 1075\n",
      "Training Loss: 1.5994795560836792\n",
      "\n",
      "Epoch 16, Batch 1100\n",
      "Training Loss: 1.613175868988037\n",
      "\n",
      "Epoch 16, Batch 1125\n",
      "Training Loss: 1.4872355461120605\n",
      "\n",
      "Epoch 16, Batch 1150\n",
      "Training Loss: 1.4699106216430664\n",
      "\n",
      "Epoch 16, Batch 1175\n",
      "Training Loss: 1.8718788623809814\n",
      "\n",
      "Epoch 16, Batch 1200\n",
      "Training Loss: 1.4896506071090698\n",
      "\n",
      "Epoch 16, Batch 1225\n",
      "Training Loss: 1.623380184173584\n",
      "\n",
      "Epoch 16, Batch 1250\n",
      "Training Loss: 1.7676280736923218\n",
      "\n",
      "Epoch 16, Batch 1275\n",
      "Training Loss: 1.5772576332092285\n",
      "\n",
      "Epoch 16, Batch 1300\n",
      "Training Loss: 1.4266548156738281\n",
      "\n",
      "Epoch 16, Batch 1325\n",
      "Training Loss: 1.3267433643341064\n",
      "\n",
      "Epoch 16, Batch 1350\n",
      "Training Loss: 1.6615992784500122\n",
      "\n",
      "Epoch 16, Batch 1375\n",
      "Training Loss: 1.650874137878418\n",
      "\n",
      "Epoch 16, Batch 1400\n",
      "Training Loss: 1.8109182119369507\n",
      "\n",
      "Epoch 16, Batch 1425\n",
      "Training Loss: 2.077357053756714\n",
      "\n",
      "Epoch 16, Batch 1450\n",
      "Training Loss: 1.842393398284912\n",
      "\n",
      "Epoch 16, Batch 1475\n",
      "Training Loss: 1.7547510862350464\n",
      "\n",
      "Epoch 16, Batch 1500\n",
      "Training Loss: 1.6650748252868652\n",
      "\n",
      "Epoch 16, Batch 1525\n",
      "Training Loss: 1.8033034801483154\n",
      "\n",
      "Epoch 16, Batch 1550\n",
      "Training Loss: 1.507757306098938\n",
      "\n",
      "Epoch 16, Batch 1575\n",
      "Training Loss: 1.5043247938156128\n",
      "\n",
      "Epoch 16, Batch 1600\n",
      "Training Loss: 1.9692009687423706\n",
      "\n",
      "Epoch 16, Batch 1625\n",
      "Training Loss: 1.4205152988433838\n",
      "\n",
      "Epoch 16, Batch 1650\n",
      "Training Loss: 1.9132108688354492\n",
      "\n",
      "Epoch 16, Batch 1675\n",
      "Training Loss: 1.3337361812591553\n",
      "\n",
      "Epoch 16, Batch 1700\n",
      "Training Loss: 1.5003052949905396\n",
      "\n",
      "Epoch 16, Batch 1725\n",
      "Training Loss: 1.6531727313995361\n",
      "\n",
      "Epoch 16, Batch 1750\n",
      "Training Loss: 1.9411824941635132\n",
      "\n",
      "Epoch 16, Batch 1775\n",
      "Training Loss: 1.309322476387024\n",
      "\n",
      "Epoch 16, Batch 1800\n",
      "Training Loss: 2.0207509994506836\n",
      "\n",
      "Epoch 16, Batch 1825\n",
      "Training Loss: 1.7683607339859009\n",
      "\n",
      "Epoch 16, Batch 1850\n",
      "Training Loss: 1.349992275238037\n",
      "\n",
      "Epoch 16, Batch 1875\n",
      "Training Loss: 1.6339644193649292\n",
      "\n",
      "Epoch 16, Batch 1900\n",
      "Training Loss: 1.7309823036193848\n",
      "\n",
      "Epoch 16, Batch 1925\n",
      "Training Loss: 1.582908272743225\n",
      "\n",
      "Epoch 16, Batch 1950\n",
      "Training Loss: 1.5720727443695068\n",
      "\n",
      "Epoch 16, Batch 1975\n",
      "Training Loss: 1.7971336841583252\n",
      "\n",
      "Epoch 16, Batch 2000\n",
      "Training Loss: 1.6869193315505981\n",
      "\n",
      "Epoch 16, Batch 2025\n",
      "Training Loss: 1.8409783840179443\n",
      "\n",
      "Epoch 16, Batch 2050\n",
      "Training Loss: 1.7058005332946777\n",
      "\n",
      "Epoch 16, Batch 2075\n",
      "Training Loss: 1.5806996822357178\n",
      "\n",
      "Epoch 16, Batch 2100\n",
      "Training Loss: 1.4752435684204102\n",
      "\n",
      "Epoch 16, Batch 2125\n",
      "Training Loss: 1.9960746765136719\n",
      "\n",
      "Epoch 16, Batch 2150\n",
      "Training Loss: 1.8222683668136597\n",
      "\n",
      "Epoch 16, Batch 2175\n",
      "Training Loss: 1.9784106016159058\n",
      "\n",
      "Epoch 16, Batch 2200\n",
      "Training Loss: 1.5583184957504272\n",
      "\n",
      "Epoch 16, Batch 2225\n",
      "Training Loss: 1.6015658378601074\n",
      "\n",
      "Epoch 16, Batch 2250\n",
      "Training Loss: 1.576783299446106\n",
      "\n",
      "Epoch 16, Batch 2275\n",
      "Training Loss: 1.7145564556121826\n",
      "\n",
      "Epoch 16, Batch 2300\n",
      "Training Loss: 1.617967963218689\n",
      "\n",
      "Epoch 16, Batch 2325\n",
      "Training Loss: 1.256116271018982\n",
      "\n",
      "Epoch 16, Batch 2350\n",
      "Training Loss: 1.8901402950286865\n",
      "\n",
      "Epoch 16, Batch 2375\n",
      "Training Loss: 1.6440463066101074\n",
      "\n",
      "Epoch 16, Batch 2400\n",
      "Training Loss: 1.9255743026733398\n",
      "\n",
      "Epoch 16, Batch 2425\n",
      "Training Loss: 1.247463345527649\n",
      "\n",
      "Epoch 16, Batch 2450\n",
      "Training Loss: 1.9195340871810913\n",
      "\n",
      "Epoch 16, Batch 2475\n",
      "Training Loss: 1.7041206359863281\n",
      "\n",
      "Epoch 16, Batch 2500\n",
      "Training Loss: 1.6995635032653809\n",
      "\n",
      "Epoch 16, Batch 2525\n",
      "Training Loss: 1.6317921876907349\n",
      "\n",
      "Epoch 16, Batch 2550\n",
      "Training Loss: 1.909717321395874\n",
      "\n",
      "Epoch 16, Batch 2575\n",
      "Training Loss: 1.903899908065796\n",
      "\n",
      "Epoch 16, Batch 2600\n",
      "Training Loss: 1.547265648841858\n",
      "\n",
      "Epoch 16, Batch 2625\n",
      "Training Loss: 1.729689598083496\n",
      "\n",
      "Epoch 16, Batch 2650\n",
      "Training Loss: 1.636534333229065\n",
      "\n",
      "Epoch 16, Batch 2675\n",
      "Training Loss: 1.5899673700332642\n",
      "\n",
      "Epoch 16, Batch 2700\n",
      "Training Loss: 1.7142441272735596\n",
      "\n",
      "Epoch 16, Batch 2725\n",
      "Training Loss: 1.5719327926635742\n",
      "\n",
      "Epoch 16, Batch 2750\n",
      "Training Loss: 1.7374478578567505\n",
      "\n",
      "Epoch 16, Batch 2775\n",
      "Training Loss: 2.0833182334899902\n",
      "\n",
      "Epoch 16, Batch 2800\n",
      "Training Loss: 1.5219188928604126\n",
      "\n",
      "Epoch 16, Batch 2825\n",
      "Training Loss: 1.6258503198623657\n",
      "\n",
      "Epoch 16, Batch 2850\n",
      "Training Loss: 1.580116629600525\n",
      "\n",
      "Epoch 16, Batch 2875\n",
      "Training Loss: 1.4446098804473877\n",
      "\n",
      "Epoch 16, Batch 2900\n",
      "Training Loss: 2.193692922592163\n",
      "\n",
      "Epoch 16, Batch 2925\n",
      "Training Loss: 1.348397970199585\n",
      "\n",
      "Epoch 16, Batch 2950\n",
      "Training Loss: 1.7485387325286865\n",
      "\n",
      "Epoch 16/20\n",
      "Training Loss: 1.6682817935943604\n",
      "\n",
      "Epoch 17, Batch 0\n",
      "Training Loss: 1.5944828987121582\n",
      "\n",
      "Epoch 17, Batch 25\n",
      "Training Loss: 1.6419861316680908\n",
      "\n",
      "Epoch 17, Batch 50\n",
      "Training Loss: 1.697274088859558\n",
      "\n",
      "Epoch 17, Batch 75\n",
      "Training Loss: 1.3977644443511963\n",
      "\n",
      "Epoch 17, Batch 100\n",
      "Training Loss: 1.7506622076034546\n",
      "\n",
      "Epoch 17, Batch 125\n",
      "Training Loss: 1.5250898599624634\n",
      "\n",
      "Epoch 17, Batch 150\n",
      "Training Loss: 1.6737937927246094\n",
      "\n",
      "Epoch 17, Batch 175\n",
      "Training Loss: 1.6140352487564087\n",
      "\n",
      "Epoch 17, Batch 200\n",
      "Training Loss: 1.7068068981170654\n",
      "\n",
      "Epoch 17, Batch 225\n",
      "Training Loss: 1.68532133102417\n",
      "\n",
      "Epoch 17, Batch 250\n",
      "Training Loss: 1.5314821004867554\n",
      "\n",
      "Epoch 17, Batch 275\n",
      "Training Loss: 1.7997207641601562\n",
      "\n",
      "Epoch 17, Batch 300\n",
      "Training Loss: 1.648937702178955\n",
      "\n",
      "Epoch 17, Batch 325\n",
      "Training Loss: 1.5876474380493164\n",
      "\n",
      "Epoch 17, Batch 350\n",
      "Training Loss: 1.5535274744033813\n",
      "\n",
      "Epoch 17, Batch 375\n",
      "Training Loss: 1.4139673709869385\n",
      "\n",
      "Epoch 17, Batch 400\n",
      "Training Loss: 1.8592121601104736\n",
      "\n",
      "Epoch 17, Batch 425\n",
      "Training Loss: 1.4836763143539429\n",
      "\n",
      "Epoch 17, Batch 450\n",
      "Training Loss: 1.4613637924194336\n",
      "\n",
      "Epoch 17, Batch 475\n",
      "Training Loss: 1.479204773902893\n",
      "\n",
      "Epoch 17, Batch 500\n",
      "Training Loss: 1.8466476202011108\n",
      "\n",
      "Epoch 17, Batch 525\n",
      "Training Loss: 2.032783269882202\n",
      "\n",
      "Epoch 17, Batch 550\n",
      "Training Loss: 1.273840069770813\n",
      "\n",
      "Epoch 17, Batch 575\n",
      "Training Loss: 1.8035680055618286\n",
      "\n",
      "Epoch 17, Batch 600\n",
      "Training Loss: 1.6739298105239868\n",
      "\n",
      "Epoch 17, Batch 625\n",
      "Training Loss: 1.6106404066085815\n",
      "\n",
      "Epoch 17, Batch 650\n",
      "Training Loss: 1.5019781589508057\n",
      "\n",
      "Epoch 17, Batch 675\n",
      "Training Loss: 1.7286807298660278\n",
      "\n",
      "Epoch 17, Batch 700\n",
      "Training Loss: 1.510275959968567\n",
      "\n",
      "Epoch 17, Batch 725\n",
      "Training Loss: 2.0623939037323\n",
      "\n",
      "Epoch 17, Batch 750\n",
      "Training Loss: 1.5686898231506348\n",
      "\n",
      "Epoch 17, Batch 775\n",
      "Training Loss: 1.69644033908844\n",
      "\n",
      "Epoch 17, Batch 800\n",
      "Training Loss: 1.801658034324646\n",
      "\n",
      "Epoch 17, Batch 825\n",
      "Training Loss: 1.9006235599517822\n",
      "\n",
      "Epoch 17, Batch 850\n",
      "Training Loss: 1.6409956216812134\n",
      "\n",
      "Epoch 17, Batch 875\n",
      "Training Loss: 1.328162670135498\n",
      "\n",
      "Epoch 17, Batch 900\n",
      "Training Loss: 1.4607181549072266\n",
      "\n",
      "Epoch 17, Batch 925\n",
      "Training Loss: 1.305533528327942\n",
      "\n",
      "Epoch 17, Batch 950\n",
      "Training Loss: 1.379150629043579\n",
      "\n",
      "Epoch 17, Batch 975\n",
      "Training Loss: 2.193071126937866\n",
      "\n",
      "Epoch 17, Batch 1000\n",
      "Training Loss: 2.0140626430511475\n",
      "\n",
      "Epoch 17, Batch 1025\n",
      "Training Loss: 1.800330400466919\n",
      "\n",
      "Epoch 17, Batch 1050\n",
      "Training Loss: 1.5823078155517578\n",
      "\n",
      "Epoch 17, Batch 1075\n",
      "Training Loss: 1.7362420558929443\n",
      "\n",
      "Epoch 17, Batch 1100\n",
      "Training Loss: 1.6358115673065186\n",
      "\n",
      "Epoch 17, Batch 1125\n",
      "Training Loss: 1.8490004539489746\n",
      "\n",
      "Epoch 17, Batch 1150\n",
      "Training Loss: 1.4926668405532837\n",
      "\n",
      "Epoch 17, Batch 1175\n",
      "Training Loss: 1.663001537322998\n",
      "\n",
      "Epoch 17, Batch 1200\n",
      "Training Loss: 1.786969542503357\n",
      "\n",
      "Epoch 17, Batch 1225\n",
      "Training Loss: 1.955364465713501\n",
      "\n",
      "Epoch 17, Batch 1250\n",
      "Training Loss: 1.7042052745819092\n",
      "\n",
      "Epoch 17, Batch 1275\n",
      "Training Loss: 1.282117247581482\n",
      "\n",
      "Epoch 17, Batch 1300\n",
      "Training Loss: 1.6682795286178589\n",
      "\n",
      "Epoch 17, Batch 1325\n",
      "Training Loss: 1.8646748065948486\n",
      "\n",
      "Epoch 17, Batch 1350\n",
      "Training Loss: 1.3438959121704102\n",
      "\n",
      "Epoch 17, Batch 1375\n",
      "Training Loss: 1.4875890016555786\n",
      "\n",
      "Epoch 17, Batch 1400\n",
      "Training Loss: 1.8912218809127808\n",
      "\n",
      "Epoch 17, Batch 1425\n",
      "Training Loss: 1.5659151077270508\n",
      "\n",
      "Epoch 17, Batch 1450\n",
      "Training Loss: 1.5875184535980225\n",
      "\n",
      "Epoch 17, Batch 1475\n",
      "Training Loss: 1.6758430004119873\n",
      "\n",
      "Epoch 17, Batch 1500\n",
      "Training Loss: 1.5247759819030762\n",
      "\n",
      "Epoch 17, Batch 1525\n",
      "Training Loss: 1.4737716913223267\n",
      "\n",
      "Epoch 17, Batch 1550\n",
      "Training Loss: 1.37824547290802\n",
      "\n",
      "Epoch 17, Batch 1575\n",
      "Training Loss: 1.3996211290359497\n",
      "\n",
      "Epoch 17, Batch 1600\n",
      "Training Loss: 1.3785312175750732\n",
      "\n",
      "Epoch 17, Batch 1625\n",
      "Training Loss: 1.5807257890701294\n",
      "\n",
      "Epoch 17, Batch 1650\n",
      "Training Loss: 1.438870906829834\n",
      "\n",
      "Epoch 17, Batch 1675\n",
      "Training Loss: 1.3842097520828247\n",
      "\n",
      "Epoch 17, Batch 1700\n",
      "Training Loss: 1.7547874450683594\n",
      "\n",
      "Epoch 17, Batch 1725\n",
      "Training Loss: 1.7818421125411987\n",
      "\n",
      "Epoch 17, Batch 1750\n",
      "Training Loss: 1.435116171836853\n",
      "\n",
      "Epoch 17, Batch 1775\n",
      "Training Loss: 1.5637969970703125\n",
      "\n",
      "Epoch 17, Batch 1800\n",
      "Training Loss: 1.447613000869751\n",
      "\n",
      "Epoch 17, Batch 1825\n",
      "Training Loss: 1.621758222579956\n",
      "\n",
      "Epoch 17, Batch 1850\n",
      "Training Loss: 1.56692373752594\n",
      "\n",
      "Epoch 17, Batch 1875\n",
      "Training Loss: 1.421842098236084\n",
      "\n",
      "Epoch 17, Batch 1900\n",
      "Training Loss: 1.5395606756210327\n",
      "\n",
      "Epoch 17, Batch 1925\n",
      "Training Loss: 1.8406906127929688\n",
      "\n",
      "Epoch 17, Batch 1950\n",
      "Training Loss: 1.742136001586914\n",
      "\n",
      "Epoch 17, Batch 1975\n",
      "Training Loss: 1.4948140382766724\n",
      "\n",
      "Epoch 17, Batch 2000\n",
      "Training Loss: 1.5352503061294556\n",
      "\n",
      "Epoch 17, Batch 2025\n",
      "Training Loss: 1.959817886352539\n",
      "\n",
      "Epoch 17, Batch 2050\n",
      "Training Loss: 1.5855621099472046\n",
      "\n",
      "Epoch 17, Batch 2075\n",
      "Training Loss: 1.7161062955856323\n",
      "\n",
      "Epoch 17, Batch 2100\n",
      "Training Loss: 1.9963549375534058\n",
      "\n",
      "Epoch 17, Batch 2125\n",
      "Training Loss: 1.6108906269073486\n",
      "\n",
      "Epoch 17, Batch 2150\n",
      "Training Loss: 1.7763322591781616\n",
      "\n",
      "Epoch 17, Batch 2175\n",
      "Training Loss: 1.683789610862732\n",
      "\n",
      "Epoch 17, Batch 2200\n",
      "Training Loss: 1.6436411142349243\n",
      "\n",
      "Epoch 17, Batch 2225\n",
      "Training Loss: 1.8534220457077026\n",
      "\n",
      "Epoch 17, Batch 2250\n",
      "Training Loss: 1.558046579360962\n",
      "\n",
      "Epoch 17, Batch 2275\n",
      "Training Loss: 1.7948795557022095\n",
      "\n",
      "Epoch 17, Batch 2300\n",
      "Training Loss: 1.4477386474609375\n",
      "\n",
      "Epoch 17, Batch 2325\n",
      "Training Loss: 1.7376880645751953\n",
      "\n",
      "Epoch 17, Batch 2350\n",
      "Training Loss: 1.3638414144515991\n",
      "\n",
      "Epoch 17, Batch 2375\n",
      "Training Loss: 1.8868921995162964\n",
      "\n",
      "Epoch 17, Batch 2400\n",
      "Training Loss: 1.659400463104248\n",
      "\n",
      "Epoch 17, Batch 2425\n",
      "Training Loss: 1.6970925331115723\n",
      "\n",
      "Epoch 17, Batch 2450\n",
      "Training Loss: 1.828403353691101\n",
      "\n",
      "Epoch 17, Batch 2475\n",
      "Training Loss: 1.4129034280776978\n",
      "\n",
      "Epoch 17, Batch 2500\n",
      "Training Loss: 1.5927495956420898\n",
      "\n",
      "Epoch 17, Batch 2525\n",
      "Training Loss: 1.2980067729949951\n",
      "\n",
      "Epoch 17, Batch 2550\n",
      "Training Loss: 2.023336172103882\n",
      "\n",
      "Epoch 17, Batch 2575\n",
      "Training Loss: 1.672538161277771\n",
      "\n",
      "Epoch 17, Batch 2600\n",
      "Training Loss: 1.5742005109786987\n",
      "\n",
      "Epoch 17, Batch 2625\n",
      "Training Loss: 1.6989320516586304\n",
      "\n",
      "Epoch 17, Batch 2650\n",
      "Training Loss: 1.4118199348449707\n",
      "\n",
      "Epoch 17, Batch 2675\n",
      "Training Loss: 1.798016905784607\n",
      "\n",
      "Epoch 17, Batch 2700\n",
      "Training Loss: 1.8314242362976074\n",
      "\n",
      "Epoch 17, Batch 2725\n",
      "Training Loss: 1.8304166793823242\n",
      "\n",
      "Epoch 17, Batch 2750\n",
      "Training Loss: 1.6589173078536987\n",
      "\n",
      "Epoch 17, Batch 2775\n",
      "Training Loss: 1.776383399963379\n",
      "\n",
      "Epoch 17, Batch 2800\n",
      "Training Loss: 1.856557011604309\n",
      "\n",
      "Epoch 17, Batch 2825\n",
      "Training Loss: 1.8132644891738892\n",
      "\n",
      "Epoch 17, Batch 2850\n",
      "Training Loss: 1.6656004190444946\n",
      "\n",
      "Epoch 17, Batch 2875\n",
      "Training Loss: 2.147664785385132\n",
      "\n",
      "Epoch 17, Batch 2900\n",
      "Training Loss: 1.7448159456253052\n",
      "\n",
      "Epoch 17, Batch 2925\n",
      "Training Loss: 1.7016322612762451\n",
      "\n",
      "Epoch 17, Batch 2950\n",
      "Training Loss: 1.833556890487671\n",
      "\n",
      "Epoch 17/20\n",
      "Training Loss: 1.813761591911316\n",
      "\n",
      "Epoch 18, Batch 0\n",
      "Training Loss: 1.217983603477478\n",
      "\n",
      "Epoch 18, Batch 25\n",
      "Training Loss: 2.173424243927002\n",
      "\n",
      "Epoch 18, Batch 50\n",
      "Training Loss: 1.3148304224014282\n",
      "\n",
      "Epoch 18, Batch 75\n",
      "Training Loss: 1.7744852304458618\n",
      "\n",
      "Epoch 18, Batch 100\n",
      "Training Loss: 1.64479660987854\n",
      "\n",
      "Epoch 18, Batch 125\n",
      "Training Loss: 1.5687787532806396\n",
      "\n",
      "Epoch 18, Batch 150\n",
      "Training Loss: 1.6355112791061401\n",
      "\n",
      "Epoch 18, Batch 175\n",
      "Training Loss: 1.6188238859176636\n",
      "\n",
      "Epoch 18, Batch 200\n",
      "Training Loss: 1.7136529684066772\n",
      "\n",
      "Epoch 18, Batch 225\n",
      "Training Loss: 1.3521349430084229\n",
      "\n",
      "Epoch 18, Batch 250\n",
      "Training Loss: 1.6542937755584717\n",
      "\n",
      "Epoch 18, Batch 275\n",
      "Training Loss: 1.4206746816635132\n",
      "\n",
      "Epoch 18, Batch 300\n",
      "Training Loss: 1.5533946752548218\n",
      "\n",
      "Epoch 18, Batch 325\n",
      "Training Loss: 1.4538376331329346\n",
      "\n",
      "Epoch 18, Batch 350\n",
      "Training Loss: 1.5150244235992432\n",
      "\n",
      "Epoch 18, Batch 375\n",
      "Training Loss: 1.5444221496582031\n",
      "\n",
      "Epoch 18, Batch 400\n",
      "Training Loss: 1.5847867727279663\n",
      "\n",
      "Epoch 18, Batch 425\n",
      "Training Loss: 1.730570912361145\n",
      "\n",
      "Epoch 18, Batch 450\n",
      "Training Loss: 1.7397209405899048\n",
      "\n",
      "Epoch 18, Batch 475\n",
      "Training Loss: 1.780707597732544\n",
      "\n",
      "Epoch 18, Batch 500\n",
      "Training Loss: 1.791574478149414\n",
      "\n",
      "Epoch 18, Batch 525\n",
      "Training Loss: 1.6249066591262817\n",
      "\n",
      "Epoch 18, Batch 550\n",
      "Training Loss: 1.3381379842758179\n",
      "\n",
      "Epoch 18, Batch 575\n",
      "Training Loss: 1.6181858777999878\n",
      "\n",
      "Epoch 18, Batch 600\n",
      "Training Loss: 1.6232085227966309\n",
      "\n",
      "Epoch 18, Batch 625\n",
      "Training Loss: 1.8358674049377441\n",
      "\n",
      "Epoch 18, Batch 650\n",
      "Training Loss: 1.684838056564331\n",
      "\n",
      "Epoch 18, Batch 675\n",
      "Training Loss: 1.531498670578003\n",
      "\n",
      "Epoch 18, Batch 700\n",
      "Training Loss: 1.2818104028701782\n",
      "\n",
      "Epoch 18, Batch 725\n",
      "Training Loss: 1.461035966873169\n",
      "\n",
      "Epoch 18, Batch 750\n",
      "Training Loss: 1.6456162929534912\n",
      "\n",
      "Epoch 18, Batch 775\n",
      "Training Loss: 1.4527366161346436\n",
      "\n",
      "Epoch 18, Batch 800\n",
      "Training Loss: 1.5562807321548462\n",
      "\n",
      "Epoch 18, Batch 825\n",
      "Training Loss: 1.8633182048797607\n",
      "\n",
      "Epoch 18, Batch 850\n",
      "Training Loss: 1.4131755828857422\n",
      "\n",
      "Epoch 18, Batch 875\n",
      "Training Loss: 1.7103625535964966\n",
      "\n",
      "Epoch 18, Batch 900\n",
      "Training Loss: 1.6769248247146606\n",
      "\n",
      "Epoch 18, Batch 925\n",
      "Training Loss: 1.641065001487732\n",
      "\n",
      "Epoch 18, Batch 950\n",
      "Training Loss: 1.4571099281311035\n",
      "\n",
      "Epoch 18, Batch 975\n",
      "Training Loss: 1.5367811918258667\n",
      "\n",
      "Epoch 18, Batch 1000\n",
      "Training Loss: 1.950931191444397\n",
      "\n",
      "Epoch 18, Batch 1025\n",
      "Training Loss: 1.8408119678497314\n",
      "\n",
      "Epoch 18, Batch 1050\n",
      "Training Loss: 1.544411540031433\n",
      "\n",
      "Epoch 18, Batch 1075\n",
      "Training Loss: 1.3716413974761963\n",
      "\n",
      "Epoch 18, Batch 1100\n",
      "Training Loss: 1.525728464126587\n",
      "\n",
      "Epoch 18, Batch 1125\n",
      "Training Loss: 1.6803419589996338\n",
      "\n",
      "Epoch 18, Batch 1150\n",
      "Training Loss: 1.6778737306594849\n",
      "\n",
      "Epoch 18, Batch 1175\n",
      "Training Loss: 1.3658274412155151\n",
      "\n",
      "Epoch 18, Batch 1200\n",
      "Training Loss: 1.6432396173477173\n",
      "\n",
      "Epoch 18, Batch 1225\n",
      "Training Loss: 1.376023292541504\n",
      "\n",
      "Epoch 18, Batch 1250\n",
      "Training Loss: 1.9859564304351807\n",
      "\n",
      "Epoch 18, Batch 1275\n",
      "Training Loss: 1.3675466775894165\n",
      "\n",
      "Epoch 18, Batch 1300\n",
      "Training Loss: 1.9428588151931763\n",
      "\n",
      "Epoch 18, Batch 1325\n",
      "Training Loss: 1.704482078552246\n",
      "\n",
      "Epoch 18, Batch 1350\n",
      "Training Loss: 1.3617790937423706\n",
      "\n",
      "Epoch 18, Batch 1375\n",
      "Training Loss: 1.701172113418579\n",
      "\n",
      "Epoch 18, Batch 1400\n",
      "Training Loss: 1.594396710395813\n",
      "\n",
      "Epoch 18, Batch 1425\n",
      "Training Loss: 1.4116898775100708\n",
      "\n",
      "Epoch 18, Batch 1450\n",
      "Training Loss: 1.7356420755386353\n",
      "\n",
      "Epoch 18, Batch 1475\n",
      "Training Loss: 1.7581491470336914\n",
      "\n",
      "Epoch 18, Batch 1500\n",
      "Training Loss: 1.5036864280700684\n",
      "\n",
      "Epoch 18, Batch 1525\n",
      "Training Loss: 2.00007963180542\n",
      "\n",
      "Epoch 18, Batch 1550\n",
      "Training Loss: 1.6650607585906982\n",
      "\n",
      "Epoch 18, Batch 1575\n",
      "Training Loss: 1.5184326171875\n",
      "\n",
      "Epoch 18, Batch 1600\n",
      "Training Loss: 1.9971877336502075\n",
      "\n",
      "Epoch 18, Batch 1625\n",
      "Training Loss: 1.4758564233779907\n",
      "\n",
      "Epoch 18, Batch 1650\n",
      "Training Loss: 1.8768701553344727\n",
      "\n",
      "Epoch 18, Batch 1675\n",
      "Training Loss: 1.4109222888946533\n",
      "\n",
      "Epoch 18, Batch 1700\n",
      "Training Loss: 1.5858851671218872\n",
      "\n",
      "Epoch 18, Batch 1725\n",
      "Training Loss: 1.5174027681350708\n",
      "\n",
      "Epoch 18, Batch 1750\n",
      "Training Loss: 1.8394452333450317\n",
      "\n",
      "Epoch 18, Batch 1775\n",
      "Training Loss: 1.4664949178695679\n",
      "\n",
      "Epoch 18, Batch 1800\n",
      "Training Loss: 1.8214205503463745\n",
      "\n",
      "Epoch 18, Batch 1825\n",
      "Training Loss: 1.4551007747650146\n",
      "\n",
      "Epoch 18, Batch 1850\n",
      "Training Loss: 1.7908931970596313\n",
      "\n",
      "Epoch 18, Batch 1875\n",
      "Training Loss: 2.0884125232696533\n",
      "\n",
      "Epoch 18, Batch 1900\n",
      "Training Loss: 1.8483920097351074\n",
      "\n",
      "Epoch 18, Batch 1925\n",
      "Training Loss: 1.3401447534561157\n",
      "\n",
      "Epoch 18, Batch 1950\n",
      "Training Loss: 1.6233839988708496\n",
      "\n",
      "Epoch 18, Batch 1975\n",
      "Training Loss: 1.5862692594528198\n",
      "\n",
      "Epoch 18, Batch 2000\n",
      "Training Loss: 1.6910641193389893\n",
      "\n",
      "Epoch 18, Batch 2025\n",
      "Training Loss: 1.8763091564178467\n",
      "\n",
      "Epoch 18, Batch 2050\n",
      "Training Loss: 1.6577357053756714\n",
      "\n",
      "Epoch 18, Batch 2075\n",
      "Training Loss: 1.6979589462280273\n",
      "\n",
      "Epoch 18, Batch 2100\n",
      "Training Loss: 1.5347611904144287\n",
      "\n",
      "Epoch 18, Batch 2125\n",
      "Training Loss: 1.7117947340011597\n",
      "\n",
      "Epoch 18, Batch 2150\n",
      "Training Loss: 1.2552638053894043\n",
      "\n",
      "Epoch 18, Batch 2175\n",
      "Training Loss: 1.8662000894546509\n",
      "\n",
      "Epoch 18, Batch 2200\n",
      "Training Loss: 1.6893237829208374\n",
      "\n",
      "Epoch 18, Batch 2225\n",
      "Training Loss: 1.6893203258514404\n",
      "\n",
      "Epoch 18, Batch 2250\n",
      "Training Loss: 1.3339793682098389\n",
      "\n",
      "Epoch 18, Batch 2275\n",
      "Training Loss: 1.4406085014343262\n",
      "\n",
      "Epoch 18, Batch 2300\n",
      "Training Loss: 1.4564130306243896\n",
      "\n",
      "Epoch 18, Batch 2325\n",
      "Training Loss: 1.5368781089782715\n",
      "\n",
      "Epoch 18, Batch 2350\n",
      "Training Loss: 1.4304331541061401\n",
      "\n",
      "Epoch 18, Batch 2375\n",
      "Training Loss: 1.616320252418518\n",
      "\n",
      "Epoch 18, Batch 2400\n",
      "Training Loss: 1.7032479047775269\n",
      "\n",
      "Epoch 18, Batch 2425\n",
      "Training Loss: 1.6990376710891724\n",
      "\n",
      "Epoch 18, Batch 2450\n",
      "Training Loss: 1.3792401552200317\n",
      "\n",
      "Epoch 18, Batch 2475\n",
      "Training Loss: 1.534448266029358\n",
      "\n",
      "Epoch 18, Batch 2500\n",
      "Training Loss: 1.6018446683883667\n",
      "\n",
      "Epoch 18, Batch 2525\n",
      "Training Loss: 1.6735321283340454\n",
      "\n",
      "Epoch 18, Batch 2550\n",
      "Training Loss: 1.517325758934021\n",
      "\n",
      "Epoch 18, Batch 2575\n",
      "Training Loss: 1.8613872528076172\n",
      "\n",
      "Epoch 18, Batch 2600\n",
      "Training Loss: 1.311646580696106\n",
      "\n",
      "Epoch 18, Batch 2625\n",
      "Training Loss: 2.1409595012664795\n",
      "\n",
      "Epoch 18, Batch 2650\n",
      "Training Loss: 1.743987798690796\n",
      "\n",
      "Epoch 18, Batch 2675\n",
      "Training Loss: 1.4577550888061523\n",
      "\n",
      "Epoch 18, Batch 2700\n",
      "Training Loss: 1.4401129484176636\n",
      "\n",
      "Epoch 18, Batch 2725\n",
      "Training Loss: 1.8811755180358887\n",
      "\n",
      "Epoch 18, Batch 2750\n",
      "Training Loss: 1.3923399448394775\n",
      "\n",
      "Epoch 18, Batch 2775\n",
      "Training Loss: 1.6762874126434326\n",
      "\n",
      "Epoch 18, Batch 2800\n",
      "Training Loss: 1.34998619556427\n",
      "\n",
      "Epoch 18, Batch 2825\n",
      "Training Loss: 1.69798743724823\n",
      "\n",
      "Epoch 18, Batch 2850\n",
      "Training Loss: 1.5506315231323242\n",
      "\n",
      "Epoch 18, Batch 2875\n",
      "Training Loss: 1.6064107418060303\n",
      "\n",
      "Epoch 18, Batch 2900\n",
      "Training Loss: 1.3279081583023071\n",
      "\n",
      "Epoch 18, Batch 2925\n",
      "Training Loss: 1.514852523803711\n",
      "\n",
      "Epoch 18, Batch 2950\n",
      "Training Loss: 1.608231782913208\n",
      "\n",
      "Epoch 18/20\n",
      "Training Loss: 1.591049075126648\n",
      "\n",
      "Epoch 19, Batch 0\n",
      "Training Loss: 1.7688746452331543\n",
      "\n",
      "Epoch 19, Batch 25\n",
      "Training Loss: 1.5766427516937256\n",
      "\n",
      "Epoch 19, Batch 50\n",
      "Training Loss: 1.6006066799163818\n",
      "\n",
      "Epoch 19, Batch 75\n",
      "Training Loss: 1.5211639404296875\n",
      "\n",
      "Epoch 19, Batch 100\n",
      "Training Loss: 1.5625355243682861\n",
      "\n",
      "Epoch 19, Batch 125\n",
      "Training Loss: 1.6050987243652344\n",
      "\n",
      "Epoch 19, Batch 150\n",
      "Training Loss: 1.7847793102264404\n",
      "\n",
      "Epoch 19, Batch 175\n",
      "Training Loss: 1.798974871635437\n",
      "\n",
      "Epoch 19, Batch 200\n",
      "Training Loss: 1.8616281747817993\n",
      "\n",
      "Epoch 19, Batch 225\n",
      "Training Loss: 1.9736143350601196\n",
      "\n",
      "Epoch 19, Batch 250\n",
      "Training Loss: 1.7797199487686157\n",
      "\n",
      "Epoch 19, Batch 275\n",
      "Training Loss: 1.54539155960083\n",
      "\n",
      "Epoch 19, Batch 300\n",
      "Training Loss: 1.779311180114746\n",
      "\n",
      "Epoch 19, Batch 325\n",
      "Training Loss: 1.6051920652389526\n",
      "\n",
      "Epoch 19, Batch 350\n",
      "Training Loss: 1.3161300420761108\n",
      "\n",
      "Epoch 19, Batch 375\n",
      "Training Loss: 1.6069843769073486\n",
      "\n",
      "Epoch 19, Batch 400\n",
      "Training Loss: 1.6204122304916382\n",
      "\n",
      "Epoch 19, Batch 425\n",
      "Training Loss: 1.6851564645767212\n",
      "\n",
      "Epoch 19, Batch 450\n",
      "Training Loss: 1.6237895488739014\n",
      "\n",
      "Epoch 19, Batch 475\n",
      "Training Loss: 1.382562518119812\n",
      "\n",
      "Epoch 19, Batch 500\n",
      "Training Loss: 1.4014719724655151\n",
      "\n",
      "Epoch 19, Batch 525\n",
      "Training Loss: 1.3929513692855835\n",
      "\n",
      "Epoch 19, Batch 550\n",
      "Training Loss: 1.4576354026794434\n",
      "\n",
      "Epoch 19, Batch 575\n",
      "Training Loss: 1.5184924602508545\n",
      "\n",
      "Epoch 19, Batch 600\n",
      "Training Loss: 1.2558261156082153\n",
      "\n",
      "Epoch 19, Batch 625\n",
      "Training Loss: 1.5220245122909546\n",
      "\n",
      "Epoch 19, Batch 650\n",
      "Training Loss: 1.5652952194213867\n",
      "\n",
      "Epoch 19, Batch 675\n",
      "Training Loss: 1.3787224292755127\n",
      "\n",
      "Epoch 19, Batch 700\n",
      "Training Loss: 1.4678571224212646\n",
      "\n",
      "Epoch 19, Batch 725\n",
      "Training Loss: 1.6682478189468384\n",
      "\n",
      "Epoch 19, Batch 750\n",
      "Training Loss: 1.3869017362594604\n",
      "\n",
      "Epoch 19, Batch 775\n",
      "Training Loss: 1.6116113662719727\n",
      "\n",
      "Epoch 19, Batch 800\n",
      "Training Loss: 1.6942270994186401\n",
      "\n",
      "Epoch 19, Batch 825\n",
      "Training Loss: 1.3118137121200562\n",
      "\n",
      "Epoch 19, Batch 850\n",
      "Training Loss: 1.5062683820724487\n",
      "\n",
      "Epoch 19, Batch 875\n",
      "Training Loss: 1.4545003175735474\n",
      "\n",
      "Epoch 19, Batch 900\n",
      "Training Loss: 1.3213627338409424\n",
      "\n",
      "Epoch 19, Batch 925\n",
      "Training Loss: 1.7121543884277344\n",
      "\n",
      "Epoch 19, Batch 950\n",
      "Training Loss: 1.7495979070663452\n",
      "\n",
      "Epoch 19, Batch 975\n",
      "Training Loss: 1.392648696899414\n",
      "\n",
      "Epoch 19, Batch 1000\n",
      "Training Loss: 1.510438084602356\n",
      "\n",
      "Epoch 19, Batch 1025\n",
      "Training Loss: 1.987450122833252\n",
      "\n",
      "Epoch 19, Batch 1050\n",
      "Training Loss: 1.5324695110321045\n",
      "\n",
      "Epoch 19, Batch 1075\n",
      "Training Loss: 1.5825653076171875\n",
      "\n",
      "Epoch 19, Batch 1100\n",
      "Training Loss: 1.4500732421875\n",
      "\n",
      "Epoch 19, Batch 1125\n",
      "Training Loss: 1.5569477081298828\n",
      "\n",
      "Epoch 19, Batch 1150\n",
      "Training Loss: 1.6453355550765991\n",
      "\n",
      "Epoch 19, Batch 1175\n",
      "Training Loss: 1.5143402814865112\n",
      "\n",
      "Epoch 19, Batch 1200\n",
      "Training Loss: 1.7218940258026123\n",
      "\n",
      "Epoch 19, Batch 1225\n",
      "Training Loss: 1.4836766719818115\n",
      "\n",
      "Epoch 19, Batch 1250\n",
      "Training Loss: 1.6978346109390259\n",
      "\n",
      "Epoch 19, Batch 1275\n",
      "Training Loss: 1.7958259582519531\n",
      "\n",
      "Epoch 19, Batch 1300\n",
      "Training Loss: 1.6009674072265625\n",
      "\n",
      "Epoch 19, Batch 1325\n",
      "Training Loss: 1.6354870796203613\n",
      "\n",
      "Epoch 19, Batch 1350\n",
      "Training Loss: 1.851036548614502\n",
      "\n",
      "Epoch 19, Batch 1375\n",
      "Training Loss: 1.4845775365829468\n",
      "\n",
      "Epoch 19, Batch 1400\n",
      "Training Loss: 1.3380131721496582\n",
      "\n",
      "Epoch 19, Batch 1425\n",
      "Training Loss: 1.5820248126983643\n",
      "\n",
      "Epoch 19, Batch 1450\n",
      "Training Loss: 1.7705219984054565\n",
      "\n",
      "Epoch 19, Batch 1475\n",
      "Training Loss: 1.4607958793640137\n",
      "\n",
      "Epoch 19, Batch 1500\n",
      "Training Loss: 1.8345803022384644\n",
      "\n",
      "Epoch 19, Batch 1525\n",
      "Training Loss: 1.6855286359786987\n",
      "\n",
      "Epoch 19, Batch 1550\n",
      "Training Loss: 1.7705655097961426\n",
      "\n",
      "Epoch 19, Batch 1575\n",
      "Training Loss: 1.7565503120422363\n",
      "\n",
      "Epoch 19, Batch 1600\n",
      "Training Loss: 1.6001710891723633\n",
      "\n",
      "Epoch 19, Batch 1625\n",
      "Training Loss: 1.3142549991607666\n",
      "\n",
      "Epoch 19, Batch 1650\n",
      "Training Loss: 1.505358338356018\n",
      "\n",
      "Epoch 19, Batch 1675\n",
      "Training Loss: 1.507444143295288\n",
      "\n",
      "Epoch 19, Batch 1700\n",
      "Training Loss: 1.5778422355651855\n",
      "\n",
      "Epoch 19, Batch 1725\n",
      "Training Loss: 1.3869123458862305\n",
      "\n",
      "Epoch 19, Batch 1750\n",
      "Training Loss: 1.5349799394607544\n",
      "\n",
      "Epoch 19, Batch 1775\n",
      "Training Loss: 1.543020248413086\n",
      "\n",
      "Epoch 19, Batch 1800\n",
      "Training Loss: 1.2201004028320312\n",
      "\n",
      "Epoch 19, Batch 1825\n",
      "Training Loss: 1.2696641683578491\n",
      "\n",
      "Epoch 19, Batch 1850\n",
      "Training Loss: 1.7337058782577515\n",
      "\n",
      "Epoch 19, Batch 1875\n",
      "Training Loss: 1.7762713432312012\n",
      "\n",
      "Epoch 19, Batch 1900\n",
      "Training Loss: 1.2529491186141968\n",
      "\n",
      "Epoch 19, Batch 1925\n",
      "Training Loss: 1.8096848726272583\n",
      "\n",
      "Epoch 19, Batch 1950\n",
      "Training Loss: 1.562740683555603\n",
      "\n",
      "Epoch 19, Batch 1975\n",
      "Training Loss: 1.5903418064117432\n",
      "\n",
      "Epoch 19, Batch 2000\n",
      "Training Loss: 1.631516933441162\n",
      "\n",
      "Epoch 19, Batch 2025\n",
      "Training Loss: 1.6624281406402588\n",
      "\n",
      "Epoch 19, Batch 2050\n",
      "Training Loss: 1.5790458917617798\n",
      "\n",
      "Epoch 19, Batch 2075\n",
      "Training Loss: 1.5117907524108887\n",
      "\n",
      "Epoch 19, Batch 2100\n",
      "Training Loss: 1.6452302932739258\n",
      "\n",
      "Epoch 19, Batch 2125\n",
      "Training Loss: 1.7964651584625244\n",
      "\n",
      "Epoch 19, Batch 2150\n",
      "Training Loss: 1.4170693159103394\n",
      "\n",
      "Epoch 19, Batch 2175\n",
      "Training Loss: 1.2838820219039917\n",
      "\n",
      "Epoch 19, Batch 2200\n",
      "Training Loss: 2.048161506652832\n",
      "\n",
      "Epoch 19, Batch 2225\n",
      "Training Loss: 1.5899112224578857\n",
      "\n",
      "Epoch 19, Batch 2250\n",
      "Training Loss: 1.5382386445999146\n",
      "\n",
      "Epoch 19, Batch 2275\n",
      "Training Loss: 1.5485763549804688\n",
      "\n",
      "Epoch 19, Batch 2300\n",
      "Training Loss: 1.4063374996185303\n",
      "\n",
      "Epoch 19, Batch 2325\n",
      "Training Loss: 1.33927583694458\n",
      "\n",
      "Epoch 19, Batch 2350\n",
      "Training Loss: 1.8915187120437622\n",
      "\n",
      "Epoch 19, Batch 2375\n",
      "Training Loss: 1.7225338220596313\n",
      "\n",
      "Epoch 19, Batch 2400\n",
      "Training Loss: 1.8160032033920288\n",
      "\n",
      "Epoch 19, Batch 2425\n",
      "Training Loss: 1.3724770545959473\n",
      "\n",
      "Epoch 19, Batch 2450\n",
      "Training Loss: 1.6128156185150146\n",
      "\n",
      "Epoch 19, Batch 2475\n",
      "Training Loss: 1.3955726623535156\n",
      "\n",
      "Epoch 19, Batch 2500\n",
      "Training Loss: 1.5687445402145386\n",
      "\n",
      "Epoch 19, Batch 2525\n",
      "Training Loss: 1.4720420837402344\n",
      "\n",
      "Epoch 19, Batch 2550\n",
      "Training Loss: 1.5247994661331177\n",
      "\n",
      "Epoch 19, Batch 2575\n",
      "Training Loss: 1.3838894367218018\n",
      "\n",
      "Epoch 19, Batch 2600\n",
      "Training Loss: 1.8714640140533447\n",
      "\n",
      "Epoch 19, Batch 2625\n",
      "Training Loss: 1.811203122138977\n",
      "\n",
      "Epoch 19, Batch 2650\n",
      "Training Loss: 1.5047204494476318\n",
      "\n",
      "Epoch 19, Batch 2675\n",
      "Training Loss: 1.4483815431594849\n",
      "\n",
      "Epoch 19, Batch 2700\n",
      "Training Loss: 1.357318639755249\n",
      "\n",
      "Epoch 19, Batch 2725\n",
      "Training Loss: 1.5574471950531006\n",
      "\n",
      "Epoch 19, Batch 2750\n",
      "Training Loss: 1.918402075767517\n",
      "\n",
      "Epoch 19, Batch 2775\n",
      "Training Loss: 1.6735506057739258\n",
      "\n",
      "Epoch 19, Batch 2800\n",
      "Training Loss: 1.7212492227554321\n",
      "\n",
      "Epoch 19, Batch 2825\n",
      "Training Loss: 1.7727519273757935\n",
      "\n",
      "Epoch 19, Batch 2850\n",
      "Training Loss: 1.6383732557296753\n",
      "\n",
      "Epoch 19, Batch 2875\n",
      "Training Loss: 1.4805257320404053\n",
      "\n",
      "Epoch 19, Batch 2900\n",
      "Training Loss: 1.477859616279602\n",
      "\n",
      "Epoch 19, Batch 2925\n",
      "Training Loss: 1.361030101776123\n",
      "\n",
      "Epoch 19, Batch 2950\n",
      "Training Loss: 1.4736347198486328\n",
      "\n",
      "Epoch 19/20\n",
      "Training Loss: 1.5765583515167236\n",
      "\n",
      "Epoch 20, Batch 0\n",
      "Training Loss: 1.6819343566894531\n",
      "\n",
      "Epoch 20, Batch 25\n",
      "Training Loss: 1.564173936843872\n",
      "\n",
      "Epoch 20, Batch 50\n",
      "Training Loss: 1.6619101762771606\n",
      "\n",
      "Epoch 20, Batch 75\n",
      "Training Loss: 1.6525706052780151\n",
      "\n",
      "Epoch 20, Batch 100\n",
      "Training Loss: 1.7954822778701782\n",
      "\n",
      "Epoch 20, Batch 125\n",
      "Training Loss: 1.1937181949615479\n",
      "\n",
      "Epoch 20, Batch 150\n",
      "Training Loss: 1.3866448402404785\n",
      "\n",
      "Epoch 20, Batch 175\n",
      "Training Loss: 2.03294038772583\n",
      "\n",
      "Epoch 20, Batch 200\n",
      "Training Loss: 1.4624723196029663\n",
      "\n",
      "Epoch 20, Batch 225\n",
      "Training Loss: 1.2632981538772583\n",
      "\n",
      "Epoch 20, Batch 250\n",
      "Training Loss: 1.9450632333755493\n",
      "\n",
      "Epoch 20, Batch 275\n",
      "Training Loss: 1.746822476387024\n",
      "\n",
      "Epoch 20, Batch 300\n",
      "Training Loss: 1.483945369720459\n",
      "\n",
      "Epoch 20, Batch 325\n",
      "Training Loss: 1.8035582304000854\n",
      "\n",
      "Epoch 20, Batch 350\n",
      "Training Loss: 1.3212453126907349\n",
      "\n",
      "Epoch 20, Batch 375\n",
      "Training Loss: 1.6594798564910889\n",
      "\n",
      "Epoch 20, Batch 400\n",
      "Training Loss: 1.4813066720962524\n",
      "\n",
      "Epoch 20, Batch 425\n",
      "Training Loss: 1.3875023126602173\n",
      "\n",
      "Epoch 20, Batch 450\n",
      "Training Loss: 1.7060798406600952\n",
      "\n",
      "Epoch 20, Batch 475\n",
      "Training Loss: 1.5283998250961304\n",
      "\n",
      "Epoch 20, Batch 500\n",
      "Training Loss: 1.33582603931427\n",
      "\n",
      "Epoch 20, Batch 525\n",
      "Training Loss: 1.4531407356262207\n",
      "\n",
      "Epoch 20, Batch 550\n",
      "Training Loss: 1.5602167844772339\n",
      "\n",
      "Epoch 20, Batch 575\n",
      "Training Loss: 1.8122868537902832\n",
      "\n",
      "Epoch 20, Batch 600\n",
      "Training Loss: 1.342956781387329\n",
      "\n",
      "Epoch 20, Batch 625\n",
      "Training Loss: 1.7246417999267578\n",
      "\n",
      "Epoch 20, Batch 650\n",
      "Training Loss: 1.5346620082855225\n",
      "\n",
      "Epoch 20, Batch 675\n",
      "Training Loss: 1.7729582786560059\n",
      "\n",
      "Epoch 20, Batch 700\n",
      "Training Loss: 1.565330982208252\n",
      "\n",
      "Epoch 20, Batch 725\n",
      "Training Loss: 1.6238210201263428\n",
      "\n",
      "Epoch 20, Batch 750\n",
      "Training Loss: 1.4630870819091797\n",
      "\n",
      "Epoch 20, Batch 775\n",
      "Training Loss: 1.3644253015518188\n",
      "\n",
      "Epoch 20, Batch 800\n",
      "Training Loss: 2.000793695449829\n",
      "\n",
      "Epoch 20, Batch 825\n",
      "Training Loss: 1.6695197820663452\n",
      "\n",
      "Epoch 20, Batch 850\n",
      "Training Loss: 1.659300684928894\n",
      "\n",
      "Epoch 20, Batch 875\n",
      "Training Loss: 1.5530716180801392\n",
      "\n",
      "Epoch 20, Batch 900\n",
      "Training Loss: 1.787457823753357\n",
      "\n",
      "Epoch 20, Batch 925\n",
      "Training Loss: 1.7783184051513672\n",
      "\n",
      "Epoch 20, Batch 950\n",
      "Training Loss: 1.6747552156448364\n",
      "\n",
      "Epoch 20, Batch 975\n",
      "Training Loss: 1.6570476293563843\n",
      "\n",
      "Epoch 20, Batch 1000\n",
      "Training Loss: 1.8026288747787476\n",
      "\n",
      "Epoch 20, Batch 1025\n",
      "Training Loss: 1.2622019052505493\n",
      "\n",
      "Epoch 20, Batch 1050\n",
      "Training Loss: 1.5251096487045288\n",
      "\n",
      "Epoch 20, Batch 1075\n",
      "Training Loss: 1.833473563194275\n",
      "\n",
      "Epoch 20, Batch 1100\n",
      "Training Loss: 1.369397521018982\n",
      "\n",
      "Epoch 20, Batch 1125\n",
      "Training Loss: 1.4846200942993164\n",
      "\n",
      "Epoch 20, Batch 1150\n",
      "Training Loss: 1.340847134590149\n",
      "\n",
      "Epoch 20, Batch 1175\n",
      "Training Loss: 1.4540534019470215\n",
      "\n",
      "Epoch 20, Batch 1200\n",
      "Training Loss: 1.5043758153915405\n",
      "\n",
      "Epoch 20, Batch 1225\n",
      "Training Loss: 1.8565934896469116\n",
      "\n",
      "Epoch 20, Batch 1250\n",
      "Training Loss: 1.6338258981704712\n",
      "\n",
      "Epoch 20, Batch 1275\n",
      "Training Loss: 1.769129753112793\n",
      "\n",
      "Epoch 20, Batch 1300\n",
      "Training Loss: 1.6778984069824219\n",
      "\n",
      "Epoch 20, Batch 1325\n",
      "Training Loss: 1.454733967781067\n",
      "\n",
      "Epoch 20, Batch 1350\n",
      "Training Loss: 1.7010726928710938\n",
      "\n",
      "Epoch 20, Batch 1375\n",
      "Training Loss: 1.5307509899139404\n",
      "\n",
      "Epoch 20, Batch 1400\n",
      "Training Loss: 1.5839998722076416\n",
      "\n",
      "Epoch 20, Batch 1425\n",
      "Training Loss: 1.5177429914474487\n",
      "\n",
      "Epoch 20, Batch 1450\n",
      "Training Loss: 1.4451299905776978\n",
      "\n",
      "Epoch 20, Batch 1475\n",
      "Training Loss: 1.4538671970367432\n",
      "\n",
      "Epoch 20, Batch 1500\n",
      "Training Loss: 1.4997715950012207\n",
      "\n",
      "Epoch 20, Batch 1525\n",
      "Training Loss: 1.37861168384552\n",
      "\n",
      "Epoch 20, Batch 1550\n",
      "Training Loss: 1.5050289630889893\n",
      "\n",
      "Epoch 20, Batch 1575\n",
      "Training Loss: 1.3361809253692627\n",
      "\n",
      "Epoch 20, Batch 1600\n",
      "Training Loss: 1.8404793739318848\n",
      "\n",
      "Epoch 20, Batch 1625\n",
      "Training Loss: 1.4516925811767578\n",
      "\n",
      "Epoch 20, Batch 1650\n",
      "Training Loss: 1.6229212284088135\n",
      "\n",
      "Epoch 20, Batch 1675\n",
      "Training Loss: 1.5327144861221313\n",
      "\n",
      "Epoch 20, Batch 1700\n",
      "Training Loss: 1.52753746509552\n",
      "\n",
      "Epoch 20, Batch 1725\n",
      "Training Loss: 1.4843699932098389\n",
      "\n",
      "Epoch 20, Batch 1750\n",
      "Training Loss: 1.9646399021148682\n",
      "\n",
      "Epoch 20, Batch 1775\n",
      "Training Loss: 1.3291212320327759\n",
      "\n",
      "Epoch 20, Batch 1800\n",
      "Training Loss: 1.5513262748718262\n",
      "\n",
      "Epoch 20, Batch 1825\n",
      "Training Loss: 1.5865923166275024\n",
      "\n",
      "Epoch 20, Batch 1850\n",
      "Training Loss: 1.730636715888977\n",
      "\n",
      "Epoch 20, Batch 1875\n",
      "Training Loss: 1.643129825592041\n",
      "\n",
      "Epoch 20, Batch 1900\n",
      "Training Loss: 1.7085692882537842\n",
      "\n",
      "Epoch 20, Batch 1925\n",
      "Training Loss: 1.6197534799575806\n",
      "\n",
      "Epoch 20, Batch 1950\n",
      "Training Loss: 1.6064990758895874\n",
      "\n",
      "Epoch 20, Batch 1975\n",
      "Training Loss: 1.5267637968063354\n",
      "\n",
      "Epoch 20, Batch 2000\n",
      "Training Loss: 1.5506370067596436\n",
      "\n",
      "Epoch 20, Batch 2025\n",
      "Training Loss: 1.6464765071868896\n",
      "\n",
      "Epoch 20, Batch 2050\n",
      "Training Loss: 1.426362156867981\n",
      "\n",
      "Epoch 20, Batch 2075\n",
      "Training Loss: 1.6955897808074951\n",
      "\n",
      "Epoch 20, Batch 2100\n",
      "Training Loss: 1.3815070390701294\n",
      "\n",
      "Epoch 20, Batch 2125\n",
      "Training Loss: 1.2404019832611084\n",
      "\n",
      "Epoch 20, Batch 2150\n",
      "Training Loss: 1.9780195951461792\n",
      "\n",
      "Epoch 20, Batch 2175\n",
      "Training Loss: 1.6533607244491577\n",
      "\n",
      "Epoch 20, Batch 2200\n",
      "Training Loss: 1.8464734554290771\n",
      "\n",
      "Epoch 20, Batch 2225\n",
      "Training Loss: 1.487173080444336\n",
      "\n",
      "Epoch 20, Batch 2250\n",
      "Training Loss: 1.5757217407226562\n",
      "\n",
      "Epoch 20, Batch 2275\n",
      "Training Loss: 1.6364637613296509\n",
      "\n",
      "Epoch 20, Batch 2300\n",
      "Training Loss: 1.684688687324524\n",
      "\n",
      "Epoch 20, Batch 2325\n",
      "Training Loss: 1.6381217241287231\n",
      "\n",
      "Epoch 20, Batch 2350\n",
      "Training Loss: 1.4186244010925293\n",
      "\n",
      "Epoch 20, Batch 2375\n",
      "Training Loss: 1.6751221418380737\n",
      "\n",
      "Epoch 20, Batch 2400\n",
      "Training Loss: 1.5593960285186768\n",
      "\n",
      "Epoch 20, Batch 2425\n",
      "Training Loss: 1.8426252603530884\n",
      "\n",
      "Epoch 20, Batch 2450\n",
      "Training Loss: 1.9305779933929443\n",
      "\n",
      "Epoch 20, Batch 2475\n",
      "Training Loss: 2.155937910079956\n",
      "\n",
      "Epoch 20, Batch 2500\n",
      "Training Loss: 1.3688223361968994\n",
      "\n",
      "Epoch 20, Batch 2525\n",
      "Training Loss: 1.72366464138031\n",
      "\n",
      "Epoch 20, Batch 2550\n",
      "Training Loss: 1.9119298458099365\n",
      "\n",
      "Epoch 20, Batch 2575\n",
      "Training Loss: 2.0326406955718994\n",
      "\n",
      "Epoch 20, Batch 2600\n",
      "Training Loss: 1.5053539276123047\n",
      "\n",
      "Epoch 20, Batch 2625\n",
      "Training Loss: 1.8443433046340942\n",
      "\n",
      "Epoch 20, Batch 2650\n",
      "Training Loss: 1.3420511484146118\n",
      "\n",
      "Epoch 20, Batch 2675\n",
      "Training Loss: 1.527683138847351\n",
      "\n",
      "Epoch 20, Batch 2700\n",
      "Training Loss: 1.2759848833084106\n",
      "\n",
      "Epoch 20, Batch 2725\n",
      "Training Loss: 1.5187331438064575\n",
      "\n",
      "Epoch 20, Batch 2750\n",
      "Training Loss: 1.818440556526184\n",
      "\n",
      "Epoch 20, Batch 2775\n",
      "Training Loss: 1.6701223850250244\n",
      "\n",
      "Epoch 20, Batch 2800\n",
      "Training Loss: 1.4433246850967407\n",
      "\n",
      "Epoch 20, Batch 2825\n",
      "Training Loss: 2.0138823986053467\n",
      "\n",
      "Epoch 20, Batch 2850\n",
      "Training Loss: 1.7823268175125122\n",
      "\n",
      "Epoch 20, Batch 2875\n",
      "Training Loss: 1.425519585609436\n",
      "\n",
      "Epoch 20, Batch 2900\n",
      "Training Loss: 1.631500482559204\n",
      "\n",
      "Epoch 20, Batch 2925\n",
      "Training Loss: 1.9029752016067505\n",
      "\n",
      "Epoch 20, Batch 2950\n",
      "Training Loss: 1.558463454246521\n",
      "\n",
      "Epoch 20/20\n",
      "Training Loss: 1.7553753852844238\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "params = {\n",
    "    'batch_size' : 16,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 0,\n",
    "    'drop_last' : True\n",
    "}\n",
    "\n",
    "# NOTE: max_trg_len <= max_src_len otherwise side asset error is triggered\n",
    "max_trg_len = 500 # length of all target note sequences, holds 99 notes max\n",
    "max_src_len = 500\n",
    "pad_idx = 434\n",
    "\n",
    "# Define data loaders\n",
    "train_path = Path(r'X:\\Training Data\\Model 1 Training\\train')\n",
    "train_data = LazierDataset(train_path, max_src_len, max_trg_len, pad_idx)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **params)\n",
    "val_path = Path(r'X:\\Training Data\\Model 1 Training\\val')\n",
    "val_data = LazierDataset(val_path, max_src_len, max_trg_len, pad_idx)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **params)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "# Model hyperparameters\n",
    "trg_vocab_size = 435  # <output length>\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dropout = 0.1\n",
    "max_len = max_src_len\n",
    "forward_expansion = 2048\n",
    "\n",
    "# Tensorboard for nice plots\n",
    "writer = SummaryWriter('runs/model5')\n",
    "step = 8885  # how many times the model has gone through some input\n",
    "\n",
    "# Define model\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    trg_vocab_size,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\ewais\\Documents\\GitHub\\tensor-hero\\Model_1\\model5.pt'))\n",
    "\n",
    "# torch.save(model.state_dict(), 'model.pt')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() # Multi-class loss, when you have a many class prediction problem\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=434)\n",
    "\n",
    "start_epoch = 3\n",
    "num_extra_epochs = 17\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+num_extra_epochs):\n",
    "    model.train() # Put model in training mode, so that it knows it's parameters should be updated\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Batches come through as a tuple defined in the return statement __getitem__ in the Dataset\n",
    "        spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # forward prop\n",
    "        output = model(spec, notes[..., :-1]) # Don't pass the last element into the decoder, want it to be predicted\n",
    "        # print('output shape : {}'.format(output.shape))\n",
    "        # output = output.reshape(-1, output.shape[2]) # Reshape the output for use by criterion\n",
    "        notes = notes[..., 1:] # .reshape(-1)           # Same for the notes\n",
    "        # print('notes shape 2 {}'.format(notes.shape))\n",
    "        optimizer.zero_grad()                        # Zero out the gradient so it doesn't accumulate\n",
    "\n",
    "        loss = criterion(output.permute(0,2,1), notes)     # Calculate loss, this is output vs ground truth\n",
    "        loss.backward()                     # Compute loss for every node in the computation graph\n",
    "\n",
    "        # This line to avoid the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()    # Update model parameters\n",
    "        writer.add_scalar(\"Training Loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "        if batch_idx%25 == 0:\n",
    "            print('\\nEpoch {}, Batch {}'.format(epoch+1,batch_idx))\n",
    "            print('Training Loss: {}'.format(loss.item()))\n",
    "        \n",
    "        # if batch_idx%100 == 0:\n",
    "            # print('Ground Truth (sample) : {}'.format(notes[0]))\n",
    "            # print('Canidate (sample)     : {}'.format(torch.argmax(output[0], dim=1)))\n",
    "\n",
    "    print(f'\\nEpoch {epoch+1}/{start_epoch+num_extra_epochs}')\n",
    "    print(f'Training Loss: {loss.item()}')\n",
    "    torch.save(model.state_dict(), 'model5.pt')\n",
    "    # # Evaluate on validation set\n",
    "    # model.eval()\n",
    "    # for batch_idx, batch in enumerate(val_loader):\n",
    "        # spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # # forward prop\n",
    "        # output = model(spec, notes[..., :-1]) # Don't pass the last element into the decoder, want it to be predicted\n",
    "\n",
    "        # # output = output.reshape(-1, output.shape[2]) # Reshape the output for use by criterion\n",
    "        # notes = notes[..., 1:] # .reshape(-1)           # Same for the notes\n",
    "        \n",
    "        # loss = criterion(output.permute(0,2,1), notes)     # Calculate loss, this is output vs ground truth\n",
    "\n",
    "        # writer.add_scalar(\"Validation Loss\", loss, global_step=step)\n",
    "        # step += 1\n",
    "    # print('Validation Loss: {}'.format(loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model4.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
