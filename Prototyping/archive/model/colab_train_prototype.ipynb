{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Colab Lazy Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "from tensor_hero.model import ColabLazyDataset\n",
    "import torch\n",
    "\n",
    "dl_params = {\n",
    "            'batch_size' : 12,\n",
    "            'shuffle' : True,\n",
    "            'num_workers' : 0,\n",
    "            'drop_last' : True\n",
    "        }\n",
    "\n",
    "train_path = Path(r'X:\\Training Data\\training_ready\\train')\n",
    "val_path = Path(r'X:\\Training Data\\training_ready\\val')\n",
    "\n",
    "# Define data loaders\n",
    "train_data = ColabLazyDataset(train_path, max_src_len=500, max_trg_len=500, pad_idx=434)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dl_params)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f'batch idx: {batch_idx} | shape: {batch}')\n",
    "    break\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "song_paths = [train_path / x for x in os.listdir(train_path)]\n",
    "notes_dirs = [x / 'notes' for x in song_paths]\n",
    "specs_dirs = [x / 'spectrograms' for x in song_paths]\n",
    "\n",
    "specs_lists = []\n",
    "for dir_ in specs_dirs:\n",
    "    for specs_dir, _, specs in os.walk(dir_):\n",
    "        if not specs:\n",
    "            continue\n",
    "        specs_lists.append([Path(specs_dir) / spec for spec in specs])\n",
    "    # print(f'x: {x}\\ny: {y}\\nz: {z}\\n\\n')\n",
    "def __note_dirs_from_spec_dirs(spec_file):\n",
    "    '''\n",
    "    Finds the note files corresponding to the spectrogram in spec_dir\n",
    "    Helper function for ColabLazyDataset __init__()\n",
    "\n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    - spec_file (Path): \n",
    "        - single path to a spectrogram in colab transformer training data\n",
    "        - assumes file structure defined in tensor_hero/preprocessing/data.py\n",
    "            -> preprocess_transformer_data() w/ COLAB=True\n",
    "    \n",
    "    ~~~~ RETURNS ~~~~\n",
    "    Path: Path to notes array corresponding to spec in spec_file\n",
    "    '''\n",
    "    return Path(str(spec_file).replace('spectrograms', 'notes'))\n",
    "    \n",
    "specs_lists = [spec for spec_list in specs_lists for spec in spec_list]  # Flatten\n",
    "notes_lists = [__note_dirs_from_spec_dirs(x) for x in specs_lists]\n",
    "print(specs_lists)\n",
    "# print(notes_lists[-10:])\n",
    "\n",
    "len(specs_lists)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Colab Memory Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "sys.path.insert(1, str(Path.cwd().parent.parent))\n",
    "from tensor_hero.model import ColabMemoryDataset\n",
    "import torch\n",
    "\n",
    "dl_params = {\n",
    "            'batch_size' : 32,\n",
    "            'shuffle' : True,\n",
    "            'num_workers' : 4,\n",
    "            'drop_last' : True\n",
    "        }\n",
    "\n",
    "train_path = Path.cwd().parent.parent / 'Training_Data' / 'colab_training_data' / 'train'\n",
    "\n",
    "max_examples = 1000\n",
    "\n",
    "# Define data loaders\n",
    "train_data = ColabMemoryDataset(train_path, \n",
    "                                max_src_len=500, \n",
    "                                max_trg_len=500, \n",
    "                                pad_idx=434,\n",
    "                                max_examples=max_examples)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dl_params)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f'batch idx: {batch_idx} | shape: {batch[0].shape}')\n",
    "    if batch_idx > 5:\n",
    "        break\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking length of spectrograms and notes...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9722/9722 [00:02<00:00, 4820.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 datapoints removed due to exceeding maximum length\n",
      "Populating 1000 samples into memory\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 449.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "self.specs is taking up 1953.13 MB\n",
      "self.notes is taking up 3.81 MB\n",
      "batch idx: 0 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 1 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 2 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 3 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 4 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 5 | shape: torch.Size([32, 512, 500])\n",
      "batch idx: 6 | shape: torch.Size([32, 512, 500])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}