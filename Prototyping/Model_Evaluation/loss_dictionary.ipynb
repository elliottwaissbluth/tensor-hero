{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-77a27646156f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-77a27646156f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    lossloss_types =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#dictionary of types of errors\n",
    "\n",
    "def comparison(truth_array, pred_array, loss_dict):\n",
    "    #imput truth and prediction array, output an array of loss values that is pulled from the dictionary for each comparison\n",
    "\n",
    "    if truth_array.shape[0] != pred_arry.shape[0]:\n",
    "        return 'Arrays must be same shape'\n",
    "\n",
    "    \n",
    "\n",
    "    losses = truth - pred\n",
    "    loss_types = (pd.Categorical(pd.cut(losses, \n",
    "               [-1,1, 5, 15, 25, 30, 31]))\n",
    "           .rename_categories(['exact','single', 'double', 'triple', 'quad', 'quint']))\n",
    "    \n",
    "    loss_arrays = [loss_dict[i] for i in buckets]\n",
    "\n",
    "    loss = np.average(loss_arrays)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_penalty_dict(weights = [0,1, 2, 3, 4, 5]):\n",
    "    #creates dict for specific weights for each type of error\n",
    "    #weights array is for exact, single, double, triple, quad, quint\n",
    "    d = {}\n",
    "    d['exact'] = weights[0]\n",
    "    d['single'] = weights[1]\n",
    "    d['double'] = weights[2]\n",
    "    d['triple'] = weights[3]\n",
    "    d['quad'] = weights[4]\n",
    "    d['quint'] = weights[5]\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = generate_penalty_dict()\n",
    "\n",
    "arr = np.array([0,3,4,5, 6, 10, 30, 31])\n",
    "\n",
    "buckets = (pd.Categorical(pd.cut(arr, \n",
    "               [-1,1, 5, 15, 25, 30, 31]))\n",
    "           .rename_categories(['exact','single', 'double', 'triple', 'quad', 'quint']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2, 2, 4, 5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d[i] for i in buckets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 1], (1, 5], (1, 5], (1, 5], (5, 15], (5, 15], (25, 30], (30, 31]]\n",
       "Categories (6, interval[int64]): [(-1, 1] < (1, 5] < (5, 15] < (15, 25] < (25, 30] < (30, 31]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Categorical(pd.cut(arr, [-1,1, 5, 15, 25, 30, 31])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_matrix(time_note_weight = 1.5, weights = [1.2,1.3,1.4,1.5]):\n",
    "    \"\"\"\n",
    "    #returns  weight matrix needed for multiplication against output\n",
    "    #weights incorporate punishment for predicting timestep on note, note on timestep\n",
    "    #punishes wrong note-type prediction\n",
    "\n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    - weights : list of weights\n",
    "        - length 4\n",
    "        - for single,double,triple, and quad degree incorrectness\n",
    "    \n",
    "    ~~~~ RETURNS ~~~~\n",
    "    - 431x431 weights matrix\n",
    "    \"\"\"\n",
    "    w = np.ones((431,431))\n",
    "\n",
    "    #punish timesteps that were predicted as notes\n",
    "    w[0:400, 400:] = time_note_weight\n",
    "    \n",
    "    #punish notes that were predicted as timesteps\n",
    "    w[400:, 0:400] = time_note_weight\n",
    "\n",
    "    #punish wrong notes\n",
    "    #single degree\n",
    "    w[400:405, 405:415] = weights[0]\n",
    "    w[405:415, 400:405] = weights[0]\n",
    "    w[405:415, 415:425] = weights[0]\n",
    "    w[415:425, 405:415] = weights[0]\n",
    "    w[415:425, 425:430] = weights[0]\n",
    "    w[425:430, 415:425] = weights[0]\n",
    "    w[430,     425:430] = weights[0]\n",
    "    w[425:430,     430] = weights[0]    \n",
    "\n",
    "    #second degree\n",
    "    w[400:405, 415:425] = weights[1]\n",
    "    w[415:425, 400:405] = weights[1]\n",
    "    w[405:415, 425:430] = weights[1]\n",
    "    w[425:430, 405:415] = weights[1]\n",
    "    w[415:425,     430] = weights[1]\n",
    "    w[430,     415:425] = weights[1]\n",
    "\n",
    "    #third degree\n",
    "    w[400:405, 425:430] = weights[2]\n",
    "    w[425:430, 400:405] = weights[2]\n",
    "    w[430,     405:415] = weights[2]\n",
    "    w[405:415,     430] = weights[2]\n",
    "\n",
    "    #fourth degree\n",
    "    w[430,     400:405] = weights[3]\n",
    "    w[400:405,     430] = weights[3]\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = weights_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.3,\n",
       "       1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.2, 1.2, 1.2, 1.2,\n",
       "       1.2, 1. ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[430,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
