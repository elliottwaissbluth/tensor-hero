{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader prototype\n",
    "\n",
    "The dataloader will inheret from LazierDataset and ColabMemoryDataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 230.75it/s]\n",
      " 21%|██        | 418/2008 [00:00<00:00, 4138.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading list of notes and spectrogram files\n",
      "Checking length of spectrograms and notes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [00:00<00:00, 4365.26it/s]\n",
      " 14%|█▎        | 27/200 [00:00<00:00, 264.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 datapoints removed due to exceeding maximum length\n",
      "Populating data into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 237.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.specs (shape = (200, 512, 500)) is taking up 390.63 MB\n",
      "self.notes (shape = (200, 160)) is taking up 0.24 MB\n",
      "batch_idx: 0\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n",
      "batch_idx: 1\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n",
      "batch_idx: 2\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n",
      "batch_idx: 3\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n",
      "batch_idx: 4\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n",
      "batch_idx: 5\n",
      "batch[0] shape: torch.Size([8, 512, 7, 50])\n",
      "batch[1] shape: torch.Size([8, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent.parent))\n",
    "from tensor_hero.model import ColabMemoryDataset, LazierDataset, \\\n",
    "                              note_dirs_from_spec_dirs, check_notes_length\n",
    "from tensor_hero.inference import __single_prediction_to_notes_array\n",
    "from tensor_hero.preprocessing.data import encode_contour, notes_array_time_adjust\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa.display import specshow\n",
    "\n",
    "def contour_vector_from_notes(notes, tbps, include_time=True):\n",
    "    '''Captures original transformer output notes arrays and translates them to\n",
    "    contour vectors\n",
    "\n",
    "    Args:\n",
    "        notes (1D numpy array): original transformer output formatted notes\n",
    "        tbps (int): time bins per second represented in output array\n",
    "    Returns:\n",
    "        contour_vector (1D numpy array): transformer formatted contour array\n",
    "            - [time, note plurality, motion, time, note plurality, motion, ...]\n",
    "    '''\n",
    "    notes_array = __single_prediction_to_notes_array(notes)\n",
    "\n",
    "    # Reduce time bins per second from 100 to tbps\n",
    "    notes_array, _ = notes_array_time_adjust(notes_array, time_bins_per_second=tbps)\n",
    "    \n",
    "    # Create contour\n",
    "    contour = encode_contour(notes_array)\n",
    "    \n",
    "    # Convert to vector representation\n",
    "    #      index         information\n",
    "    #  0            | <sos> \n",
    "    #  1            | <eos> \n",
    "    #  2            | <pad> \n",
    "    #  3-15         | <note pluralities 0-13>\n",
    "    #  16-24        | <motion [-4, 4]>\n",
    "    #  25-(tbps+25) | <time bin 1-tbps>\n",
    "    contour_vector = contour_to_transformer_output(contour, tbps, include_time)\n",
    "    \n",
    "    return contour_vector\n",
    "\n",
    "def contour_to_transformer_output(contour, tbps, include_time=True):\n",
    "    '''Generates transformer output version of contour array\n",
    "    \n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "        contour (2D numpy array): contour array, note plurality is first row, motion\n",
    "                                    is second row \n",
    "        tbps (int): time bins per second. Determines dimensionality of output_vector\n",
    "        include_time (bool): Determines whether onset times are included in output dimension\n",
    "            if True, contour_vectors are in following format:\n",
    "                [<sos>, <time 0>, <note plurality 0>, <motion 0>, <time 1>, ..., <eos>]\n",
    "            if False, \n",
    "                [<sos>, <note plurality 0>, <motion 0>, <note plurality 1>, ..., <eos>]\n",
    "    ~~~~ RETURNS ~~~~\n",
    "        contour_vector (1D numpy array):\n",
    "            [time, note plurality, motion, time, note plurality, motion, ...]\n",
    "\n",
    "        The values of contour_vector are detailed below\n",
    "\n",
    "            value           information\n",
    "        ____________________________________\n",
    "        0            | <sos> \n",
    "        1            | <eos> \n",
    "        2            | <pad>\n",
    "        3-15         | <note pluralities    contour (_type_): _description_\n",
    "        16-24        | <motion [-4, 4]>    tbps (_type_): _description_\n",
    "        25-(tbps+24) | <time bin 1-tbps>\n",
    "    '''\n",
    "    # These lambda functions translate contour encoded notes, motions, and times into their\n",
    "    # respective vector indices\n",
    "    motion_idx = lambda motion: motion + 20     # motion in [-4, 4] -> [16, 24]\n",
    "    time_idx = lambda time: time + 25           # time bin in [0,tbps*4] -> [25, tbps*4+24]\n",
    "    np_idx = lambda note_p: note_p + 2          # note plurality in [1, 13] -> [3, 15]\n",
    "\n",
    "    # Find indices with note events and create empty vector for contour\n",
    "    note_events = np.where(contour[0,:] > 0)[0].astype(int)\n",
    "    if include_time:\n",
    "        contour_vector = np.zeros(shape=(2+note_events.shape[0]*3))\n",
    "    else:\n",
    "        contour_vector = np.zeros(shape=(2+note_events.shape[0]*2))\n",
    "    \n",
    "    for idx, ne in enumerate(list(note_events)):\n",
    "        if include_time:\n",
    "            contour_vector[1+(3*idx)] = time_idx(ne)\n",
    "            contour_vector[2+(3*idx)] = np_idx(contour[0, ne])\n",
    "            contour_vector[3+(3*idx)] = motion_idx(contour[1, ne])\n",
    "        else:\n",
    "            contour_vector[1+(2*idx)] = np_idx(contour[0, ne])\n",
    "            contour_vector[2+(2*idx)] = motion_idx(contour[1, ne])\n",
    "    \n",
    "    # Populate eos\n",
    "    contour_vector[-1] = 1\n",
    "    return contour_vector\n",
    "\n",
    "def check_notes_length_and_emptiness(notes_path, max_len):\n",
    "    '''Opens the processed notes array at notes_path and checks whether or not it is larger than max_len\n",
    "       and whether the contour notes are empty\n",
    "    Args:\n",
    "        notes_path (Path): path to notes array\n",
    "        max_len (int): maximum length of notes\n",
    "        \n",
    "    Returns:\n",
    "        bool: Whether the notes array at notes_path is >= max_len\n",
    "    '''\n",
    "    good = True\n",
    "    notes_array = np.load(notes_path)\n",
    "    if (notes_array.shape[0] > max_len) or (notes_array.shape[0] == 2):\n",
    "        good=False\n",
    "    return good\n",
    "\n",
    "class ContourOnsetMemoryDataset(ColabMemoryDataset):\n",
    "    '''Implementation of ColabMemoryDataset but transforms output into contour_vectors\n",
    "    '''\n",
    "    def __init__(self, partition_path, max_src_len, max_trg_len, max_examples, \n",
    "                 pad_idx, CHECK_LENGTH=False, tbps=25, include_time=True, remove_empty=False):\n",
    "        '''\n",
    "\n",
    "        Args:\n",
    "            partition_path (Path): _description_\n",
    "            max_src_len (int): _description_\n",
    "            max_trg_len (int): _description_\n",
    "            max_examples (int): _description_\n",
    "            pad_idx (int): _description_\n",
    "            CHECK_LENGTH (bool, optional): _description_. Defaults to False.\n",
    "            tbps (int, optional): _description_. Defaults to 25.\n",
    "            include_time (bool, optional): Determines format of transformer output,\n",
    "                if True:\n",
    "                    contour_vectors include onset times\n",
    "                if False:\n",
    "                    contour_vectors are only note pluralities and motions\n",
    "        '''\n",
    "        self.max_trg_len = max_trg_len\n",
    "        max_trg_len_with_time = 160\n",
    "        self.max_src_len = max_src_len\n",
    "        self.pad_idx = pad_idx\n",
    "        self.tbps = 25\n",
    "        \n",
    "        # Construct list of spectrogram file paths and list of note file paths\n",
    "        song_paths = [partition_path / x for x in os.listdir(partition_path)]\n",
    "        specs_dirs = [x / 'spectrograms' for x in song_paths]\n",
    "        specs_lists = []\n",
    "        print('Loading list of notes and spectrogram files')\n",
    "        for dir_ in tqdm(specs_dirs):\n",
    "            for specs_dir, _, specs in os.walk(dir_):\n",
    "                if not specs:\n",
    "                    continue\n",
    "                specs_lists.append([Path(specs_dir) / spec for spec in specs])\n",
    "        specs_lists = [spec for spec_list in specs_lists for spec in spec_list]  # Flatten\n",
    "        notes_lists = [note_dirs_from_spec_dirs(x) for x in specs_lists]\n",
    "        \n",
    "        # Construct dictionary where key:value is <path to spec>:<path to notes array>\n",
    "        l = {}  # labels\n",
    "        for i in range(len(specs_lists)):\n",
    "            l[specs_lists[i]] = notes_lists[i]\n",
    "            \n",
    "        # Weed out bits of data that exceed the maximum length\n",
    "        self.labels = {}        # holds spec paths as keys, note paths as values\n",
    "        self.data_paths = []    # list of spec paths\n",
    "        too_long = 0            # how many of the notes have more elements than max_trg_len\n",
    "        print('Checking length of spectrograms and notes...')\n",
    "        for x in tqdm(specs_lists):\n",
    "            if check_notes_length_and_emptiness(l[x], max_trg_len):\n",
    "                self.data_paths.append(x)\n",
    "                self.labels[x] = l[x]\n",
    "            else:\n",
    "                too_long += 1 # NOTE: This includes too short now too\n",
    "        print(f'{too_long} datapoints removed due to exceeding maximum length')\n",
    "        \n",
    "        # Restrict max samples in Dataset to min(max_examples, num_samples)        \n",
    "        self.num_samples = len(self.labels)  # This could be lower than max_samples\n",
    "        self.max_examples = max_examples if max_examples > 0 else self.num_samples\n",
    "        self.max_examples = min(self.max_examples, self.num_samples)\n",
    "        del too_long, l, song_paths, specs_dirs, specs_lists, notes_lists\n",
    "        \n",
    "        # Create and empty data matrix\n",
    "        spec = np.load(self.data_paths[0])  # Load single examples to get shape\n",
    "        notes = np.load(self.labels[self.data_paths[0]])\n",
    "        # Shape for self.specs = [max_examples, 512, max_src_len]\n",
    "        # Shape for self.notes = [max_examples, max_trg_len]\n",
    "        self.specs = np.empty(shape=(self.max_examples, spec.shape[0], max_src_len))\n",
    "        self.notes = np.empty(shape=(self.max_examples, max_trg_len_with_time))\n",
    "        self.notes_no_time = np.empty(shape=(self.max_examples, max_trg_len))\n",
    "        \n",
    "        # Populate data into memory\n",
    "        print('Populating data into memory')\n",
    "        for idx in tqdm(range(self.max_examples)):\n",
    "            spec = self.pad_spec(np.load(self.data_paths[idx]))\n",
    "            # Transform notes into contour_vectors\n",
    "            # contour_vectors are formatted to be transformer output\n",
    "            notes = np.load(self.labels[self.data_paths[idx]])\n",
    "            notes_no_time = contour_vector_from_notes(notes, tbps, include_time=False)\n",
    "            notes = contour_vector_from_notes(notes, tbps, include_time=True)\n",
    "\n",
    "            # To be used to extract slices from spectrograms\n",
    "            notes = self.pad_notes(notes, max_trg_len_with_time)\n",
    "            self.notes[idx,...] = notes     # Final data\n",
    "\n",
    "            # To be used for actual training\n",
    "            notes_no_time = self.pad_notes(notes_no_time, self.max_trg_len)\n",
    "            self.notes_no_time[idx,...] = notes_no_time\n",
    "\n",
    "            self.specs[idx,...] = spec      # Final data\n",
    "\n",
    "        print(f'self.specs (shape = {self.specs.shape}) is taking up {sys.getsizeof(self.specs) / (1024**2):.2f} MB')\n",
    "        print(f'self.notes (shape = {self.notes.shape}) is taking up {sys.getsizeof(self.notes) / (1024**2):.2f} MB')\n",
    "        del spec, notes\n",
    "    \n",
    "    def pad_notes(self, notes, max_len):\n",
    "        '''pads notes with pad_idx to length max_trg_len'''\n",
    "        notes = np.pad(notes, \n",
    "                    (0, max_len-notes.shape[0]),\n",
    "                    'constant',\n",
    "                    constant_values=self.pad_idx)\n",
    "        return notes\n",
    "\n",
    "    def spec_slices_from_notes(self, spec, notes, window=3):\n",
    "        '''Takes timesteps where notes are present and returns a tensor of spectrogram\n",
    "        slices at those timesteps, plus window frames on either side\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor, dtype=torch.float, shape=(512,400)):\n",
    "                - log-mel spectrogram, 512 frequency bins, 400 time bins\n",
    "            notes (1D numpy array): \n",
    "                - Contains the contour encoded notes from contour_vector_from_notes()\n",
    "                - Assumes 25 tbps\n",
    "            window (int):\n",
    "                - The number of time frames on each side of an onset to pull from the spectrogram\n",
    "                - Default is 3, 30ms on each side.\n",
    "        \n",
    "        Returns:\n",
    "            spec (torch.Tensor, dtype=torch.float, shape = (512, 2*window+1, (max_trg_len-3)/2)):\n",
    "                - (max_trg_len-3)/2 is the maximum number of notes in a sequence\n",
    "                \n",
    "        '''\n",
    "        # Get onsets by leveraging fact that onsets are encoded between [25, tbps*4+25] \n",
    "        onsets = np.delete(notes, np.argwhere(notes < 25))\n",
    "        onsets -= 25  # Subtract 25 to get exact time index\n",
    "        onsets *= 4   # multiply by 4 to get corresponding indices\n",
    "        \n",
    "        # Pad spec on either side to avoid window overflow\n",
    "        big_spec = torch.zeros(size=(512, 400+2*window+1))\n",
    "        big_spec[:,window:(400+window)] = spec[:,:400]\n",
    "        \n",
    "        # Create empty tensor for spectrogram slices\n",
    "        specs = torch.zeros(size=(512, 2*window+1, self.max_src_len), dtype=torch.float)\n",
    "        for idx, onset in enumerate(onsets):\n",
    "            onset = int(onset+window)  # To index properly in big_spec\n",
    "            spec_slice = torch.zeros(size=(512, 2*window+1), dtype=torch.float)\n",
    "            spec_slice[:,:window] = big_spec[:,(onset-window):onset]\n",
    "            spec_slice[:,(window+1):] = big_spec[:,(onset+1):(onset+window+1)]\n",
    "            spec_slice[:,window] = big_spec[:,onset]\n",
    "            specs[:,:,idx] = spec_slice \n",
    "        \n",
    "        return specs\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get spectrogram slices\n",
    "        notes = self.notes[idx]\n",
    "        spec = torch.tensor(self.specs[idx], dtype=torch.float)\n",
    "        spec = self.spec_slices_from_notes(spec, notes)\n",
    "\n",
    "        # print(f'spec size: {spec.size()}')\n",
    "        notes = list(notes)\n",
    "        # if 125 in [int(x) for x in notes]:\n",
    "            # for i in range(spec.size()[2]):\n",
    "                # plt.figure()\n",
    "                # specshow(spec[:,:,i].numpy())\n",
    "            \n",
    "\n",
    "        return spec, torch.tensor(self.notes_no_time[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "       \n",
    "# Define data loaders\n",
    "train_path = Path.cwd().parent.parent / 'Training_Data' / 'training_ready' / 'train'\n",
    "train_data = ContourOnsetMemoryDataset(partition_path = train_path, \n",
    "                                  max_src_len = 500, \n",
    "                                  max_trg_len= 103,\n",
    "                                  max_examples = 200,\n",
    "                                  pad_idx = 2,\n",
    "                                  include_time=True,\n",
    "                                  CHECK_LENGTH=True,\n",
    "                                  remove_empty=True)\n",
    "\n",
    "dl_params = {\n",
    "    'batch_size' : 8,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 0,\n",
    "    'drop_last' : True,\n",
    "}\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dl_params)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f'batch_idx: {batch_idx}')\n",
    "    print(f'batch[0] shape: {batch[0].shape}')\n",
    "    print(f'batch[1] shape: {batch[1].shape}')\n",
    "    contour_vectors = batch[1]\n",
    "    # for i in range(contour_vectors.shape[0]):\n",
    "        # print(f'contour_vector {i}: {contour_vectors[i]}')\n",
    "        # break\n",
    "    # print('\\n')\n",
    "    if batch_idx > 4:\n",
    "        break \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3584, 50])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(batch[0], (8, 512*7, 50)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    '''\n",
    "    This is a custom class which can be used in place of nn.Embedding for input embeddings.\n",
    "    We can't use nn.Embedding for our input because our input is continuous, and nn.Embedding\n",
    "    only works with discrete vocabularies such as words.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "    ):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        # Take the spectrogram frames and pass them through a FC layer\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, src):\n",
    "        return self.linear(src)\n",
    "\n",
    "class OnsetInputEmbedding(nn.Module):\n",
    "    '''\n",
    "    This is a custom class which can be used in place of nn.Embedding for input embeddings.\n",
    "    We can't use nn.Embedding for our input because our input is continuous, and nn.Embedding\n",
    "    only works with discrete vocabularies such as words.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "    ):\n",
    "        super(OnsetInputEmbedding, self).__init__()\n",
    "\n",
    "        # Take the spectrogram frames and pass them through a FC layer\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512*7, embedding_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, src):\n",
    "        return self.linear(src)\n",
    "    \n",
    "class OnsetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,   # 512\n",
    "        trg_vocab_size,   # 434\n",
    "        num_heads,        # 8\n",
    "        num_encoder_layers,  # 3\n",
    "        num_decoder_layers,  # 3\n",
    "        forward_expansion,   # 2048\n",
    "        dropout,             # 0.1\n",
    "        max_len,    # 400\n",
    "        device,     # GPU or CPU?\n",
    "    ):\n",
    "        super(OnsetTransformer, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        # src_position_embedding and trg_position_embedding work the same, we can use nn.Embedding for both\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        # trg_word_embeddings can also leverage nn.Embedding, since the target values come from a\n",
    "        # discrete vocabulary of note events (unlike the continuous spectrogram input)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "\n",
    "        # continuous inputs of dim 512, so we can't feed nn.Embedding an index\n",
    "        self.src_spec_embedding = OnsetInputEmbedding(embedding_size)\n",
    "\n",
    "        self.device = device    # device will be used to move tensors from CPU to GPU \n",
    "        self.transformer = nn.Transformer(  # Define transformer\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)     # Output linear layer\n",
    "        self.dropout = nn.Dropout(dropout)      # Dropout to avoid overfitting\n",
    "\n",
    "    def make_src_mask(self, src, trg):\n",
    "        '''\n",
    "        Finds the non-padded indices of trg and creates a mask to that length\n",
    "        '''\n",
    "        src_len = self.max_len\n",
    "        num_trg = (torch.sum(trg != 2, dim=1)-2)/2  # -2 to remove <sos>, <eos>\n",
    "                                                    # /2 because trg encodes each note as two indices\n",
    "        src_mask = torch.arange(src_len).expand(num_trg.size()[0], src_len) < num_trg.unsqueeze(1)\n",
    "        \n",
    "        return src_mask\n",
    "    \n",
    "    def make_embed_src(self, src):\n",
    "        '''\n",
    "        This function passes the spectrogram frames through the input embedding layer. Notice how it does\n",
    "        so one layer at a time in a for loop. It may be possible to increase the speed of this and parallelize\n",
    "        by doing a lil trick with nn.Conv1D. We'll investigate this in the future.\n",
    "        '''\n",
    "        # out = torch.zeros_like(src, requires_grad=True).to(self.device)\n",
    "        out_list = []\n",
    "\n",
    "        for idx in range(src.shape[2]): # For each spectrogram frame of width 7\n",
    "            if idx < self.max_len: # 50\n",
    "                src = torch.reshape(src, (src.shape[0], 512*7, self.max_len))\n",
    "                out_list.append(self.src_spec_embedding(src[...,idx]).unsqueeze(-1))\n",
    "            else:\n",
    "                out_list.append(torch.zeros_like(out_list[0]))\n",
    "        return out_list\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape[-1], src.shape[0]  # Should be 50, N\n",
    "        trg_seq_length, N = trg.shape[1], trg.shape[0]  # The target sequence length is 51 for our toy training example\n",
    "                                                        # Originally 52, but we shift the target to mask the last value\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src, trg).to(self.device)\n",
    "        \n",
    "        # src_positions is just a sequence of increasing ints, expanded to the same shape as the batch of inputs used for\n",
    "        # positional embedding calculation\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "            )\n",
    "        # [0, 1, 2, 3, ..., 399]\n",
    "\n",
    "        # trg_positions is the same thing as src_positions, but with a slightly different shape since the trg and src inputs are\n",
    "        # formatted in different ways (src is spectrogram frames, trg is a series of indices corresponding to note events)\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).permute(1,0)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # [0, 1, 2, ..., 50] shape = (51) to shape = (N, 51)\n",
    "        # if N = 2, AKA 2 spectrograms per batch\n",
    "        # trg_positions = [[0, 1, 2, ..., 50], [0, 1, 2, ..., 50]]\n",
    "        # trg without padding = [433, 70, 0, 300, 3, 434], Len = 5\n",
    "        # trg with padding = [433, 70, 0, 300, 3, 434, 435, 435, 435, 435, ..., 435] Len = max_trg_seq_len\n",
    "\n",
    "        # The permutations are just to get the embeddings into the right shape for the encoder\n",
    "        # Notice how make_embed_src() is called, this is our custom function that passes the input through the parallel dense layers\n",
    "        out_list = self.make_embed_src(src)\n",
    "        src_embed = torch.cat(out_list, dim=2)\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (src_embed + self.src_position_embedding(src_positions).permute(1,2,0))\n",
    "            .to(self.device)\n",
    "        ).permute(0,2,1)\n",
    "        # This is going into the transformer (final input after the pink blocks)\n",
    "        \n",
    "        # embed_trg uses \"word\" embeddings since trg is just a list of indices corresponding to \"words\" i.e. note events.\n",
    "        # Positional embeddings are summed at this stage.\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "            .to(self.device)\n",
    "        )\n",
    "        # This is going into the decoder\n",
    "\n",
    "        # This target mask ensures the decoder doesn't take context from future input while making predictions.\n",
    "        # That would be useless for inference since our output is sampled autoregressively.\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask = src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "\n",
    "        # Pass the transformer output through a linear layer for a final prediction\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20, 10, 20, 14, 13, 14,  1,  2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 0, 24, 10, 20, 11,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2]])\n",
      "tensor([6, 4])\n",
      "tensor([[ True,  True,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "trg = [\n",
    "    [0, 20, 10, 20, 14, 13, 14, 1, 2, 2, 2, 2, 2, 2, 2],\n",
    "    [0, 24, 10, 20, 11, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    ]\n",
    "\n",
    "trg = torch.tensor(trg)\n",
    "print(trg)\n",
    "print(torch.sum(trg != 2, dim=1)-2)\n",
    "num_trg = (torch.sum(trg != 2, dim=1)-2)/2\n",
    "\n",
    "src_mask = torch.zeros(size=(2, 50))\n",
    "\n",
    "src_mask = torch.arange(50).expand(num_trg.size()[0], 50) < num_trg.unsqueeze(1)\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "tensor([ 0, 81, 14, 20,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2])\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from tensor_hero.preprocessing.data import decode_contour\n",
    "\n",
    "def __contour_prediction_to_notes_array(prediction, tbps=25):\n",
    "    '''\n",
    "    Takes a single contour prediction from the transformer and translates it to a notes array\n",
    "    of length 400.\n",
    "\n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    -   prediction (numpy Array, shape=(<max_trg_len>,)):\n",
    "            -   Prediction from the transformer, should be a single list of max indices\n",
    "                from transformer prediction. \n",
    "            -   Expected to be formatted as\n",
    "                [<sos>, time, note plurality, motion, etc., <eos>, <pad>, <pad>, etc.]\n",
    "    -   tbps (int): time bins per second of predicted notes\n",
    "        \n",
    "    ~~~~ RETURNS ~~~~\n",
    "    -   notes_array (Numpy Array, shape = (400,)):\n",
    "            -   The prediction\n",
    "    '''\n",
    "    if type(prediction) == torch.Tensor:\n",
    "        prediction = prediction.detach().cpu().numpy()\n",
    "    #     value           information\n",
    "    # ____________________________________\n",
    "    # 0            | <sos> \n",
    "    # 1            | <eos> \n",
    "    # 2            | <pad>\n",
    "    # 3-15         | <note pluralities>    \n",
    "    # 16-24        | <motion [-4, 4]>    \n",
    "    # 25-(tbps*4+24) | <time bin 1-tbps>\n",
    "\n",
    "    note_vals = list(range(3, 16))            # Note pluralities\n",
    "    time_vals = list(range(25, tbps*4+24))    # Corresponding to times\n",
    "    motion_vals = list(range(16, 25))         # Motion in [-4,4]\n",
    "    print(tbps*4 + 24)\n",
    "    # Loop through the array 3 elements at a time\n",
    "    pairs = []\n",
    "    for i in range(prediction.shape[0]-2):\n",
    "        pair = (prediction[i], prediction[i+1], prediction[i+2]) # Take predicted notes as couples\n",
    "        if pair[0] in time_vals and pair[1] in note_vals and pair[2] in motion_vals:\n",
    "            pairs.append(pair)  # Append if pair follows (time, note) pattern\n",
    "\n",
    "    # Create contour from pairs\n",
    "    expansion_factor = 100/tbps\n",
    "    contour = np.zeros(shape=(2, 400))\n",
    "    for pair in pairs:\n",
    "        index = min(round((pair[0]-25)*expansion_factor), 400)\n",
    "        contour[0, index] = pair[1]-2    # note plurality\n",
    "        contour[1, index] = pair[2]-20   # motion\n",
    "    \n",
    "    # Create notes array from contour\n",
    "    notes_array = decode_contour(contour)\n",
    "     \n",
    "    return notes_array\n",
    "\n",
    "notes_array = __contour_prediction_to_notes_array(contour_vectors[7])\n",
    "print(contour_vectors[7])\n",
    "print(notes_array)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4bace46d75acd3815648dc6b648dfb793a73e8151710a171cc803cab380864b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
