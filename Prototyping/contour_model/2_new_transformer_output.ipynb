{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor the transformer model to produce \n",
    "1. (1) time (2) note plurality (3) motion\n",
    "2. (1) note plurality (2) motion\n",
    "\n",
    "First we'll try including the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector representation of an output space detailing note categories as well as motions\n",
    "\n",
    "Rewrite preprocess_transformer_data to process data in this fashion, perhaps automatically insert the contour arrays (or transformer representation of contour arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: let's replace \"note category\" with \"note plurality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battalion.n.02 ['battalion', 'large_number', 'multitude', 'plurality', 'pack']\n",
      "multitude.n.02 ['multitude', 'throng', 'concourse']\n",
      "multitude.n.03 ['multitude', 'masses', 'mass', 'hoi_polloi', 'people', 'the_great_unwashed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for ss in wn.synsets('multitude'):\n",
    "    print(ss.name(), ss.lemma_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader prototype\n",
    "\n",
    "The dataloader will inheret from LazierDataset and ColabMemoryDataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-f62202a540dd>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-f62202a540dd>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    def __getitem__(self, idx):\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent.parent))\n",
    "from tensor_hero.model import ColabMemoryDataset, LazierDataset, \\\n",
    "                              note_dirs_from_spec_dirs, check_notes_length\n",
    "from tensor_hero.inference import __single_prediction_to_notes_array\n",
    "from tensor_hero.preprocessing.data import encode_contour, notes_array_time_adjust\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContourMemoryDataset(ColabMemoryDataset):\n",
    "    def __init__(self, partition_path, max_src_len, max_trg_len, max_examples, \n",
    "                 pad_idx, CHECK_LENGTH=False, tbps=25):\n",
    "        self.max_trg_len = max_trg_len\n",
    "        self.max_src_len = max_src_len\n",
    "        self.pad_idx = pad_idx\n",
    "        self.tbps = 25\n",
    "        \n",
    "        # Construct list of spectrogram file paths and list of note file paths\n",
    "        song_paths = [partition_path / x for x in os.listdir(partition_path)]\n",
    "        specs_dirs = [x / 'spectrograms' for x in song_paths]\n",
    "        specs_lists = []\n",
    "        for dir_ in specs_dirs:\n",
    "            for specs_dir, _, specs in os.walk(dir_):\n",
    "                if not specs:\n",
    "                    continue\n",
    "                specs_lists.append([Path(specs_dir) / spec for spec in specs])\n",
    "        specs_lists = [spec for spec_list in specs_lists for spec in spec_list]  # Flatten\n",
    "        notes_lists = [note_dirs_from_spec_dirs(x) for x in specs_lists]\n",
    "        \n",
    "        # Construct dictionary where key:value is <path to spec>:<path to notes array>\n",
    "        l = {}  # labels\n",
    "        for i in range(len(specs_lists)):\n",
    "            l[specs_lists[i]] = notes_lists[i]\n",
    "            \n",
    "        # Weed out bits of data that exceed the maximum length\n",
    "        self.labels = {}        # holds spec paths as keys, note paths as values\n",
    "        self.data_paths = []    # list of spec paths\n",
    "        too_long = 0            # how many of the notes have more elements than max_trg_len\n",
    "        if CHECK_LENGTH:\n",
    "            print('Checking length of spectrograms and notes...')\n",
    "            for x in tqdm(specs_lists):\n",
    "                if check_notes_length(l[x], max_trg_len):\n",
    "                    self.data_paths.append(x)\n",
    "                    self.labels[x] = l[x]\n",
    "                else:\n",
    "                    too_long += 1\n",
    "                print(f'{too_long} datapoints removed due to exceeding maximum length')\n",
    "        else:\n",
    "            self.data_paths = specs_lists\n",
    "            self.labels = l\n",
    "            print('Notes were not checked against max_trg_len')\n",
    "        \n",
    "        # Restrict max samples in Dataset to min(max_examples, num_samples)        \n",
    "        self.num_samples = len(self.labels)  # This could be lower than max_samples\n",
    "        self.max_examples = max_examples if max_examples > 0 else self.num_samples\n",
    "        self.max_examples = min(self.max_examples, self.num_samples)\n",
    "        del too_long, l, song_paths, specs_dirs, specs_lists, notes_lists\n",
    "        \n",
    "        # Create and empty data matrix\n",
    "        spec = np.load(self.data_paths[0])  # Load single examples to get shape\n",
    "        notes = np.load(self.labels[self.data_paths[0]])\n",
    "        # Shape for self.specs = [max_examples, 512, max_src_len]\n",
    "        # Shape for self.notes = [max_examples, max_trg_len]\n",
    "        self.specs = np.empty(shape=(self.max_examples, spec.shape[0], max_src_len))\n",
    "        self.notes = np.empty(shape=(self.max_examples, max_trg_len))\n",
    "        \n",
    "        # Populate data into memory\n",
    "        for idx in tqdm(range(self.max_examples)):\n",
    "            spec = self.pad_spec(np.load(self.data_paths[idx]))\n",
    "            # Transform notes into contour_vectors\n",
    "            # contour_vectors are formatted to be transformer output\n",
    "            notes = np.load(self.labels[self.data_paths[idx]])\n",
    "            notes = self.contour_vector_from_notes(notes, tbps)\n",
    "            notes = self.pad_notes(notes)\n",
    "            self.specs[idx,...] = spec      # Final data\n",
    "            self.notes[idx,...] = notes     # Final data\n",
    "        print(f'self.specs (shape = {self.specs.shape}) is taking up {sys.getsizeof(self.specs) / (1024**2):.2f} MB')\n",
    "        print(f'self.notes (shape = {self.notes.shape}) is taking up {sys.getsizeof(self.notes) / (1024**2):.2f} MB')\n",
    "        del spec, notes\n",
    "       \n",
    "    def contour_vector_from_notes(self, notes, tbps):\n",
    "        '''Captures original transformer output notes arrays and translates them to\n",
    "        contour vectors\n",
    "\n",
    "        Args:\n",
    "            notes (1D numpy array): original transformer output formatted notes\n",
    "            tbps (int): time bins per second represented in output array\n",
    "        Returns:\n",
    "            contour_vector (1D numpy array): transformer formatted contour array\n",
    "                - [time, note plurality, motion, time, note plurality, motion, ...]\n",
    "        '''\n",
    "        notes_array = __single_prediction_to_notes_array(notes)\n",
    "\n",
    "        # Reduce time bins per second from 100 to tbps\n",
    "        notes_array, _ = notes_array_time_adjust(notes_array, time_bins_per_second=tbps)\n",
    "        \n",
    "        # Create contour\n",
    "        contour = encode_contour(notes_array)\n",
    "        \n",
    "        # Convert to vector representation\n",
    "        #      index         information\n",
    "        #  0            | <sos> \n",
    "        #  1            | <eos> \n",
    "        #  2            | <pad> \n",
    "        #  3-15         | <note pluralities 0-13>\n",
    "        #  16-24        | <motion [-4, 4]>\n",
    "        #  25-(tbps+25) | <time bin 1-tbps>\n",
    "\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
