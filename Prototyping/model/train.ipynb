{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 400])\n",
      "torch.Size([1, 52])\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_path = Path.cwd() / 'toy training data' / 'preprocessed'\n",
    "# Define dataset\n",
    "train_data = Dataset(train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    # input = spec, output = notes\n",
    "    spec, notes = batch[0], batch[1]\n",
    "    print(spec.shape)\n",
    "    print(notes.shape)\n",
    "    print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_path = Path.cwd() / 'toy training data' / 'preprocessed'\n",
    "# Define dataset\n",
    "train_data = Dataset(train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Model hyperparameters\n",
    "trg_vocab_size = 434  # <output length>\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "max_len = 400\n",
    "forward_expansion = 2048\n",
    "\n",
    "# Tensorboard for nice plots\n",
    "writer = SummaryWriter('runs/loss_plot')\n",
    "step = 0  # how many times the model has gone through some input\n",
    "\n",
    "# Define model\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    trg_vocab_size,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Multi-class loss, when you have a many class prediction problem\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Put model in training mode, so that it knows it's parameters should be updated\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Batches come through as a tuple defined in the return statement __getitem__ in the Dataset\n",
    "        spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # forward prop\n",
    "        output = model(spec, notes[..., :-1]) # Don't pass the last element into the decoder, want it to be predicted\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2]) # Reshape the output for use by criterion\n",
    "        notes = notes[..., 1:].reshape(-1)           # Same for the notes\n",
    "        optimizer.zero_grad()                        # Zero out the gradient so it doesn't accumulate\n",
    "\n",
    "        loss = criterion(output, notes)     # Calculate loss, this is output vs ground truth\n",
    "        loss.backward()                     # Compute loss for every node in the computation graph\n",
    "\n",
    "        # This line to avoid the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()    # Update model parameters\n",
    "        writer.add_scalar(\"Training Loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "        # Let's print the output vs the ground truth every 5 epochs for the first 15 epochs\n",
    "        if epoch in [0, 5, 10, 15, 20]:\n",
    "            print('\\nEpoch {}/{}'.format(epoch, num_epochs))\n",
    "            print('Loss : {}'.format(loss.item()))\n",
    "            print('Ground Truth : {}'.format(notes))\n",
    "            print('Model Output : {}'.format(torch.argmax(output, dim=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,  47,   2,  60,   2,  99,   1, 110,   0, 122,   3, 133,   1, 145,\n",
      "          2, 157,   4, 171,   3, 183,   1, 195,   3, 221,   1, 233,   0, 245,\n",
      "          2, 259,   2, 297,   1, 309,   0, 322,   3, 334,   1, 346,   2, 359,\n",
      "          4, 371,   3, 395,   2, 419,   3, 431, 433], device='cuda:0')\n",
      "tensor([  0,  47,   2,  60,   2,  99,   1, 110,   0, 122,   3, 133,   1, 145,\n",
      "          2, 157,   4, 171,   3, 183,   1, 195,   3, 221,   1, 233,   0, 245,\n",
      "          2, 259,   2, 297,   1, 309,   0, 322,   3, 334,   1, 346,   2, 359,\n",
      "          4, 371,   3, 395,   2, 419,   3, 431, 433], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(output, dim=1))\n",
    "print(notes)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
