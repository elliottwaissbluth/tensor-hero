{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Function Definitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "from tensor_hero.model import Transformer, ColabLazyDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def __load_model(model_directory):\n",
    "    '''\n",
    "    Loads model and param dict from model_directory. useful for continuing training\n",
    "    Helper function for initialize_params()\n",
    "    \n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    - model_directory (Path): Folder containing model weights and params\n",
    "        - probably ./model/saved_models/<model name>\n",
    "    \n",
    "    ~~~~ RETURNS ~~~~\n",
    "    - dict: params loaded from model directory\n",
    "    '''\n",
    "    with open(str(model_directory / 'params.pkl'), 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return params\n",
    "\n",
    "def initialize_params(params):\n",
    "    '''\n",
    "    Takes the original params and modifies them to match the current training objective.\n",
    "        - Will load params from model directory if desired\n",
    "        - Initializes new params for training a new model\n",
    "        - Asks for user input regarding descriptions of new models\n",
    "\n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    - params (dict): Dictionary containing relevant model information. See definition in main() for more information.\n",
    "\n",
    "    ~~~~ RAISES ~~~~\n",
    "    - SystemExit: If the experiment is to be aborted.\n",
    "\n",
    "    ~~~~ RETURNS ~~~~\n",
    "    - dict: model and training parameters\n",
    "    '''\n",
    "    # Ask user whether they will load a pretrained model to continue training or initialize a new model\n",
    "    response = str(input('Load from pretrained model? (y/n): ')).lower()\n",
    "    while response not in ['y', 'n']:\n",
    "        response = str(input('invalid input\\nAre the parameters correct (y/n)?: ')).lower()\n",
    "        \n",
    "    if response == 'y':  # If loading from pretrained model, load its weights from its directory\n",
    "        response = str(input('Enter name of model to load: '))  # This will be the name of the directory under ./model/saved_models\n",
    "        while not os.path.isdir(Path.cwd() / 'model' / 'saved_models' / response):\n",
    "            print('Error: {} is not a valid directory'.format(response))\n",
    "            response = str(input('Enter name of model to load: '))\n",
    "        model_directory = Path.cwd() / 'model' / 'saved_models' / response\n",
    "        params = __load_model(model_directory)\n",
    "        params['LOAD'] = True\n",
    "\n",
    "    else:  # If initializing new model, create a new directory for it\n",
    "        while os.path.isdir(Path.cwd() / 'model' / 'saved_models' / params['model_name']):\n",
    "            new_name = input('Directory already exists.\\nEnter new model name: ')\n",
    "            params['model_name'] = str(new_name)\n",
    "        os.mkdir(Path.cwd() / 'saved_models' / params['model_name'])\n",
    "        params['LOAD'] = False\n",
    "\n",
    "    params['model_file_name'] = params['model_name'] + '.pt'  # Holds weights of model\n",
    "    params['model_outfile'] = str(Path.cwd() / 'model' / 'saved_models' / params['model_name'] / params['model_file_name'])  # model directory\n",
    "\n",
    "    # Validate parameters\n",
    "    print(json.dumps(params, indent=4))\n",
    "    response = str(input('Are the parameters correct (y/n)?: ')).lower()\n",
    "    while response not in ['y', 'n']:\n",
    "        response = str(input('invalid input\\nAre the parameters correct (y/n)?: ')).lower()\n",
    "    if response == 'n':\n",
    "        raise SystemExit(0)\n",
    "\n",
    "    # Gather description of experiment from user\n",
    "    if not 'experiment_description' in params.keys():\n",
    "        experiment_description = input('Enter experiment description: ')\n",
    "        params['experiment_description'] = experiment_description\n",
    "\n",
    "    # Save parameters\n",
    "    with open(str(Path.cwd() / 'saved_models' / params['model_name'] / 'params.pkl'), 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "    f.close()\n",
    "    \n",
    "    print('parameters saved\\n')\n",
    "    \n",
    "    return params\n",
    "\n",
    "def initialize_model(params, device):\n",
    "    '''\n",
    "    Takes params and the device (CUDA or CPU) and initializes a transformer model, as defined in ./tensor_hero/model.py\n",
    "    \n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    - params (dict): Model and training parameters. Should be the output from initialize_params()\n",
    "    - device (str): \"CUDA\" or \"CPU\"\n",
    "    \n",
    "    ~~~~ RETURNS ~~~~\n",
    "        PyTorch model: Transformer model initialized with params and sent to device. Defined in ./tensor_hero/model.py\n",
    "    '''\n",
    "    model = Transformer(\n",
    "            embedding_size = params['embedding_size'],\n",
    "            trg_vocab_size = params['trg_vocab_size'],\n",
    "            num_heads = params['num_heads'],\n",
    "            num_encoder_layers = params['num_encoder_layers'],\n",
    "            num_decoder_layers = params['num_decoder_layers'],\n",
    "            forward_expansion = params['embedding_size']*params['forward_expansion'],\n",
    "            dropout = params['dropout'],\n",
    "            max_len = params['max_src_len'],\n",
    "            device = device,\n",
    "        ).to(device)\n",
    "\n",
    "    return model\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model and Training Initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "params = {\n",
    "    'training_data' : 'train separated',     # CHANGEME (these parameters must be changed each experiment)\n",
    "    'model_name' : 'loss_function_test2',     # CHANGEME\n",
    "    'optimizer' : 'Adam',                    # CHANGEME (maybe not this one, but you do have to fill it in manually)\n",
    "    'train_path' : Path.cwd().parent / 'Training_Data' / 'training_ready' / 'train',\n",
    "\n",
    "    'num_epochs' : 500,\n",
    "    'batch_size' : 12,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 4,\n",
    "    'drop_last' : True,\n",
    "    'last_global_step' : 0,\n",
    "\n",
    "    'max_trg_len' : 500, # NOTE: max_trg_len <= max_src_len otherwise side asset error is triggered\n",
    "    'max_src_len' : 500,\n",
    "    'trg_vocab_size' : 435,\n",
    "    'pad_idx' : 434,\n",
    "    'embedding_size' : 512,\n",
    "\n",
    "    'lr' : 1e-4,\n",
    "    'num_heads' : 8,\n",
    "    'num_encoder_layers' : 2,\n",
    "    'num_decoder_layers' : 2,\n",
    "    'dropout' : 0.1,\n",
    "    'forward_expansion' : 4,\n",
    "\n",
    "    'date' : datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "}\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                                  DATALOADER                                  #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "dl_params = {\n",
    "    'batch_size' : params['batch_size'],\n",
    "    'shuffle' : params['shuffle'],\n",
    "    'num_workers' : params['num_workers'],\n",
    "    'drop_last' : params['drop_last'],\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "train_data = ColabLazyDataset(Path(params['train_path']), params['max_src_len'], params['max_trg_len'], params['pad_idx'])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dl_params)\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                              TRAINING PARAMETERS                             #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = params['lr']\n",
    "num_epochs = params['num_epochs']\n",
    "\n",
    "model = initialize_model(params, device) \n",
    "\n",
    "# torch.save(model.state_dict(), 'model.pt')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss() # Multi-class loss, when you have a many class prediction problem\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=params['pad_idx'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2008/2008 [00:00<00:00, 6456.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 datapoints removed due to exceeding maximum length\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's use this loop to parse out the shape of all the inputs and the outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model.train() # Put model in training mode, so that it knows it's parameters should be updated\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    # Batches come through as a tuple defined in the return statement __getitem__ in the Dataset\n",
    "    spec, notes = batch[0].to(device), batch[1].to(device)\n",
    "    \n",
    "    print('MODEL INPUTS  (the notes are input for training purposes)\\n~~~~~~~~~~~~~~~~')\n",
    "    print(f'spec shape: {spec.shape}')\n",
    "    print(f'notes shape: {notes.shape}')\n",
    "\n",
    "    # forward prop\n",
    "    output = model(spec, notes[..., :-1])           # Don't pass the last element into the decoder, want it to be predicted\n",
    "\n",
    "    print('\\nMODEL OUTPUT\\n~~~~~~~~~~~~~~~')\n",
    "    print(f'output shape: {output.shape}')\n",
    "    # output = output.reshape(-1, output.shape[2])  # Reshape the output for use by criterion\n",
    "    notes = notes[..., 1:] # .reshape(-1)           # Same for the notes\n",
    "    print(f'notes shape: {notes.shape}')\n",
    "    optimizer.zero_grad()                           # Zero out the gradient so it doesn't accumulate\n",
    "\n",
    "    loss = criterion(output.permute(0,2,1), notes)  # Calculate loss, this is output vs ground truth\n",
    "    \n",
    "    print('\\nLOSS FUNCTION INPUT SHAPE\\n~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(f'Ground Truth Notes shape: {notes.shape}')\n",
    "    print(f'Candidate Notes shape: {output.permute(0,2,1).shape}')\n",
    "    print('\\nIt looks like the ground truth is input as notes only and the candidate is a series of probabilities.')\n",
    "    print('In order to format the candidate like the ground truth, we need to take the argmax over the 2nd dimension')\n",
    "    print(f'Here is the shape when argmaxed over the second dimension: torch.argmax(output.permute(0,2,1),dim=1).shape = {torch.argmax(output.permute(0,2,1),dim=1).shape}')\n",
    "\n",
    "    print('\\nZooming in on the loss function inputs:')\n",
    "    print('Let\\'s ignore the batch dimension and print just a single ground truth and candidate:')\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(f'Single Ground Truth Notes: {notes[0]}')\n",
    "    print(f'\\nSingle Candidate Notes: {output.permute(0,2,1)[0]}')\n",
    "    print(f'\\nArgmax of Single Candidate Notes: {torch.argmax(output.permute(0,2,1), dim=1)[0]}')\n",
    "    print('\\nWow this model is stupid right now, makes sense it hasn\\'nt been trained at all yet.')\n",
    "    \n",
    "    loss.backward()     # Compute loss for every node in the computation graph\n",
    "\n",
    "    # This line to avoid the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    optimizer.step()    # Update model parameters\n",
    "    params['last_global_step'] += 1\n",
    "    break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MODEL INPUTS  (the notes are input for training purposes)\n",
      "~~~~~~~~~~~~~~~~\n",
      "spec shape: torch.Size([12, 512, 500])\n",
      "notes shape: torch.Size([12, 500])\n",
      "\n",
      "MODEL OUTPUT\n",
      "~~~~~~~~~~~~~~~\n",
      "output shape: torch.Size([12, 499, 435])\n",
      "notes shape: torch.Size([12, 499])\n",
      "\n",
      "LOSS FUNCTION INPUT SHAPE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Ground Truth Notes shape: torch.Size([12, 499])\n",
      "Candidate Notes shape: torch.Size([12, 435, 499])\n",
      "\n",
      "It looks like the ground truth is input as notes only and the candidate is a series of probabilities.\n",
      "In order to format the candidate like the ground truth, we need to take the argmax over the 2nd dimension\n",
      "Here is the shape when argmaxed over the second dimension: torch.argmax(output.permute(0,2,1),dim=1).shape = torch.Size([12, 499])\n",
      "\n",
      "Zooming in on the loss function inputs:\n",
      "Let's ignore the batch dimension and print just a single ground truth and candidate:\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Single Ground Truth Notes: tensor([ 37,   4,  52,   3,  74,   2,  81,   3,  89,   2,  96,   1, 108,   2,\n",
      "        119,   1, 126,   2, 190,   3, 199,   2, 208,   1, 223,   2, 237,   2,\n",
      "        251,   3, 265,   2, 279,   2, 294,   3, 301,   4, 323,   3, 330,   4,\n",
      "        338,   3, 345,   4, 352,   3, 360,   4, 382,   1, 396,   2, 410,   2,\n",
      "        424,   3, 433, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434,\n",
      "        434, 434, 434, 434, 434, 434, 434, 434, 434])\n",
      "\n",
      "Single Candidate Notes: tensor([[ 4.3463,  4.3566,  4.2759,  ...,  4.0235,  4.3987,  4.6352],\n",
      "        [ 4.6510,  4.9942,  4.2209,  ...,  4.6461,  4.5274,  4.5987],\n",
      "        [ 3.6690,  3.8614,  3.8954,  ...,  3.8005,  4.5084,  3.4062],\n",
      "        ...,\n",
      "        [-1.2468, -1.3441, -1.0943,  ..., -0.8259, -0.5982, -0.7583],\n",
      "        [ 2.3984,  2.0545,  2.3355,  ...,  2.1734,  2.1308,  2.4849],\n",
      "        [ 0.0241,  0.0112,  0.2046,  ...,  0.3059,  0.0830,  0.1214]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Argmax of Single Candidate Notes: torch.Size([499])\n",
      "\n",
      "Wow this model is stupid right now, makes sense it hasn'nt been trained at all yet.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}