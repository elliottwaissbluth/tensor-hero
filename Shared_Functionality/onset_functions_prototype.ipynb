{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## First, we would like to create a function that randomly samples the dataset for onsets\n",
    "\n",
    "# NOTE: Run the next two cells to generate 'onsets' and 'songs', described below. These can be used to test onset detection methods\n",
    "\n",
    "## second note: change \"processed_path\" to your own processed path, probably .../Training Data/Processed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Generate a list of simplified notes files\n",
    "processed_path = Path(r'/Users/ewaissbluth/Documents/GitHub/tensor-hero/Training Data/Training Data/Processed')\n",
    "song_paths = [] # paths to song.ogg files\n",
    "notes_simplified_paths = [] # paths to corresponding notes_simplified\n",
    "for p1 in [processed_path / x for x in os.listdir(processed_path)]:\n",
    "    if '.DS_Store' in str(p1):\n",
    "        continue\n",
    "    for p2 in [p1 / x for x in os.listdir(p1)]:\n",
    "        if '.DS_Store' in str(p2):\n",
    "            continue\n",
    "        if 'notes_simplified.npy' in os.listdir(p2):\n",
    "            song_path = Path(str(p2).replace('Processed', 'Unprocessed')) / 'song.ogg'\n",
    "            if not os.path.exists(song_path) == True:\n",
    "                continue\n",
    "            notes_simplified_paths.append(p2 / 'notes_simplified.npy')\n",
    "            song_paths.append(song_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "641\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Select 20 songs at random, from each song, select one 60 second slice\n",
    "subset_idx = random.sample([x for x in range(len(song_paths))], 20)\n",
    "subset_idx.sort()\n",
    "\n",
    "# Define empty lists for the songs and the onsets\n",
    "# songs = [<numpy array song 1>, <numpy array song 2> ...]\n",
    "# onsets = [<onset times song 1>, <onset times song 2> ...]\n",
    "songs = []\n",
    "onsets = []\n",
    "# Load the songs and the notes arrays one at a time\n",
    "for idx in tqdm(subset_idx):\n",
    "    # Load the song\n",
    "    y, sr = librosa.load(song_paths[idx])\n",
    "    \n",
    "    # resample the song if it isn't sr=22050 (for consistent sizing)\n",
    "    if not sr == 22050:\n",
    "        y = librosa.resample(y, sr, 22050)\n",
    "        sr = 22050\n",
    "    \n",
    "    # Load notes\n",
    "    notes = np.load(notes_simplified_paths[idx])\n",
    "    # Cut off the appended silence from the notes\n",
    "    notes = notes[7:-7]\n",
    "\n",
    "    # Choose a random 60 second subset\n",
    "    song_len_in_seconds = math.floor(y.shape[0]/sr)\n",
    "    random_point = random.randint(0, song_len_in_seconds-60)\n",
    "\n",
    "    # slice the song and the notes\n",
    "    subsong = y[random_point*sr:(random_point+60)*sr]\n",
    "    subnotes = notes[random_point*100:(random_point+60)*100]\n",
    "\n",
    "    # Append the subsong to the array\n",
    "    songs.append(subsong)\n",
    "\n",
    "    # Get onsets from notes\n",
    "    notes_times = np.where(subsong > 0)[0] / 100\n",
    "\n",
    "    # Append onsets to array\n",
    "    onsets.append(notes_times)\n",
    "\n",
    "# Save songs and onsets\n",
    "with open('onsets.pkl', 'wb') as f:\n",
    "    pickle.dump(onsets, f)\n",
    "f.close()\n",
    "with open('songs.pkl', 'wb') as f:\n",
    "    pickle.dump(songs, f)\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next, we would like to define the skeleton of some code that can be used to evaluate the different onset detection methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First, we load onsets and song segments. Recall, \"onsets\" contains\n",
    "# ground truth onsets that we calculated from simplified_notes.npy\n",
    "# songs contains numpy arrays of song waveforms.\n",
    "with open('onsets.pkl', 'rb') as f:\n",
    "    onsets = pickle.load(f)\n",
    "f.close()\n",
    "with open('songs.pkl', 'rb') as f:\n",
    "    songs = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# PSEUDOCODE:\n",
    "# Next, we iterate through onsets and songs\n",
    "for i in range(len(onsets)):\n",
    "    reference_onset = onsets[i]\n",
    "    audio = songs[i]\n",
    "\n",
    "    # DO ANALYSIS ON EACH SONG SEGMENT\n",
    "    # REPORT THE AVERAGE F1 SCORE OVER ALL 20 SEGMENTS FOR EACH\n",
    "    # HYPERPARAMETER / PREPROCESSING METHOD\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}