{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook guided by\n",
    "-   https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "source": [
    "Okay here's how it has to go.\n",
    "\n",
    "1) Loop through all the files, loading them, recording their length, adding that to self.length, then using that as __len__\n",
    "\n",
    "2) Load the files in 1gb at a time.\n",
    "\n",
    "3) Note the length of that 1GB section\n",
    "\n",
    "4) Every time __getitem__ is called, increase an index that keeps track of how much of the 1GB chunk has been read.\n",
    "\n",
    "5) When all of it has been read, load in another 1GB chunk, and reset the index that keeps track of chunk consumption."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subroot_paths):\n",
    "        '''\n",
    "        Parses through data dispersed in distributed folders (subroots >> folders containing data >>>). \n",
    "        Shuffles and returns single training examples.\n",
    "        subroot_paths => list containing string paths to subroot directories\n",
    "        '''\n",
    "        # self.data_size = data_size          # Amount of data to load\n",
    "        self.X = np.zeros((3, 81, 0))       # src\n",
    "        self.y = np.zeros(0)                # target\n",
    "        self.length = 0                     # total length of all data\n",
    "        self.chunk_length = 0               # length of individual chunks\n",
    "        self.chunk_traversed = 0            # Number of samples in chunk that have already been processed\n",
    "        self.subroot_idx = 0                # index of subroots directory to load into chunk\n",
    "        self.chunk_loaded = False           # if true, the chunk has been loaded, if false it needs loading \n",
    "        self.subroot_paths = subroot_paths  # List of files in root\n",
    "        self.total_chunk_length = 0         # Updated amount of chunk traversed to modify idx in __getitem__\n",
    "\n",
    "        # Get length of dataset\n",
    "        for path in self.subroot_paths:\n",
    "            fileList = os.listdir(path)  \n",
    "            if 'notes.npy' not in fileList or 'song.npy' not in fileList:\n",
    "                continue\n",
    "\n",
    "            # Load notes tensor to gather length for __len__\n",
    "            notes = np.load(Path(path) / 'notes.npy')\n",
    "            self.length += notes.shape[0]\n",
    "\n",
    "    def load_chunk(self):\n",
    "        '''\n",
    "        Loads 1GB of data or the rest of the data in subroot_paths\n",
    "        '''\n",
    "        self.X = np.zeros((3, 81, 0))       # src\n",
    "        self.y = np.zeros(0)                # target\n",
    "\n",
    "        # Load data\n",
    "        data_loaded = 0     # Amount of data loaded so far\n",
    "        chunk_length = 0    # Number of samples in chunk\n",
    "        while data_loaded < 1:\n",
    "\n",
    "            if self.subroot_idx > (len(self.subroot_paths)-1):\n",
    "                print('All subroots have been traversed')\n",
    "                self.subroot_idx = 0\n",
    "                break\n",
    "\n",
    "            # Path to files, in this case src = song and target = notes\n",
    "            notes_path = Path(self.subroot_paths[self.subroot_idx]) / 'notes.npy'\n",
    "            song_path = Path(self.subroot_paths[self.subroot_idx]) / 'song.npy'\n",
    "            self.subroot_idx += 1\n",
    "            \n",
    "            # Get data size\n",
    "            try:\n",
    "                data_loaded += notes_path.stat().st_size / 1e9  # Measure amount of data input\n",
    "                data_loaded += song_path.stat().st_size / 1e9\n",
    "            except WindowsError as err:  # If the files aren't all there\n",
    "                print('Windows Error: Data in {} not found, skipping\\n\\n'.format(subroot))\n",
    "                continue\n",
    "            \n",
    "            # Load numpy arrays\n",
    "            notes = np.load(notes_path)\n",
    "            song = np.load(song_path)\n",
    "\n",
    "            # Put all the note and all the song data into one big array\n",
    "            self.X = np.concatenate((self.X, song), axis=2)\n",
    "            self.y = np.concatenate((self.y, notes), axis=0)\n",
    "\n",
    "        chunk_length = self.y.shape[0]\n",
    "        return chunk_length\n",
    "\n",
    "            # print('{:3.2f} / {:3.2f} GB data loaded\\n'.format(data_loaded, self.data_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.chunk_loaded == False:  # Time to load a new chunk\n",
    "            print('Loading Chunk')\n",
    "            self.chunk_length =  self.load_chunk()\n",
    "            print('Chunk Length = {}\\n'.format(self.chunk_length))\n",
    "            self.chunk_traversed = 0\n",
    "            self.chunk_loaded = True\n",
    "\n",
    "        # Split song into 150 ms windows\n",
    "        X = torch.from_numpy(np.take(self.X, range(idx-7-self.total_chunk_length,idx+8-self.total_chunk_length), axis=2, mode='wrap'))\n",
    "        X = X.permute(0, 2, 1)\n",
    "        y = 1 if self.y[idx-self.total_chunk_length] > 0 else 0  # Only care about onsets, losing note information\n",
    "\n",
    "        # Check whether new chunk should be loaded\n",
    "        self.chunk_traversed += 1\n",
    "        if self.chunk_traversed > (self.chunk_length-1):\n",
    "            self.total_chunk_length += self.chunk_length\n",
    "            print('Full chunk traversed, {} / {} total samples traversed'.format(self.total_chunk_length, self.length))\n",
    "            self.chunk_loaded = False\n",
    "        return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by song\n",
    "def train_val_test_split(root, data_amount, val = 0.1, test = 0.1, shuffle = False):\n",
    "    '''\n",
    "    Takes a directory input and outputs 3 lists of subdirectories of specified size.\n",
    "    I'm going to operate under the assumption that songs converge on a mean length if you get enough of them.\n",
    "    - root: root of subdirectories\n",
    "    - data_amount: amount of data to load\n",
    "    - val: validation split\n",
    "    - test: test split\n",
    "    - shuffle: shuffle names of directories\n",
    "    '''\n",
    "\n",
    "    subroot_paths = []\n",
    "    data_loaded = 0\n",
    "\n",
    "    # Generate list of song folders\n",
    "    for dirName, subdirList, fileList in os.walk(root):  \n",
    "        if 'notes.npy' not in fileList or 'song.npy' not in fileList:\n",
    "            continue\n",
    "\n",
    "        # Get data size\n",
    "        notes_path = Path(dirName) / 'notes.npy'\n",
    "        song_path = Path(dirName) / 'song.npy'\n",
    "        try:\n",
    "            data_loaded += notes_path.stat().st_size / 1e9  # Measure amount of data input\n",
    "            data_loaded += song_path.stat().st_size / 1e9\n",
    "        except WindowsError as err:  # If the files aren't all there\n",
    "            print('Windows Error: Data in {} not found, skipping\\n\\n'.format(subroot))\n",
    "            continue\n",
    "        \n",
    "        if data_loaded > data_amount:\n",
    "            break\n",
    "\n",
    "        subroot_paths.append(dirName)\n",
    "        \n",
    "    # Shuffle subroots if applicable\n",
    "    if shuffle:\n",
    "        random.shuffle(subroot_paths)\n",
    "\n",
    "    # Split dataset\n",
    "    num_val = math.floor(val * len(subroot_paths))\n",
    "    num_test = math.floor(test * len(subroot_paths))\n",
    "\n",
    "    train = subroot_paths[num_val:(len(subroot_paths)-num_test)]\n",
    "    val = subroot_paths[0:num_val]\n",
    "    test = subroot_paths[-num_test:]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Chunk\n",
      "Chunk Length = 528486\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 1,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 1,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 1, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 1, 0,  ..., 0, 0, 1])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 1, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 1,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "tensor([0, 0, 0,  ..., 0, 1, 0])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4bd3b81435ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d9b389110f7c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# Split song into 150 ms windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wrap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# Only care about onsets, losing note information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtake\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[0;32m    192\u001b[0m            [5, 7]])\n\u001b[0;32m    193\u001b[0m     \"\"\"\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'take'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root = Path(r'X:\\Training Data\\Processed')\n",
    "\n",
    "train_paths, _, _ = train_val_test_split(root, 5)\n",
    "my_dataset = DistributedFolderDataset(train_paths)\n",
    "loader = torch.utils.data.DataLoader(my_dataset, batch_size = 10000, shuffle=False, num_workers=0)\n",
    "\n",
    "for batch_idx, (local_batch, local_labels) in enumerate(loader):\n",
    "    continue\n"
   ]
  },
  {
   "source": [
    "## Calculate kernel dimensions through convolutions and max pools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "H =  9\nW = 79\n\nH =  9\nW = 26\n\nH =  7\nW = 24\n\nH =  7\nW = 8\n\n"
     ]
    }
   ],
   "source": [
    "def kernel_dim(H, W, kernel_size, padding = [0,0], dilation = [1,1], stride = [1,1]):\n",
    "    H = (H+2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1)/(stride[0]) + 1\n",
    "    H = math.floor(H)\n",
    "    print('H = ', H)\n",
    "\n",
    "    W = (W+2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1)/(stride[1]) + 1\n",
    "    W = math.floor(W)\n",
    "    print('W = {}\\n'.format(W))\n",
    "    return H, W\n",
    "\n",
    "H, W = 15, 81\n",
    "kernel_size = [7,3]\n",
    "H, W = kernel_dim(H, W, kernel_size)\n",
    "\n",
    "kernel_size = [1,3]\n",
    "H, W = kernel_dim(H, W, kernel_size, stride=[1,3])\n",
    "\n",
    "kernel_size = [3,3]\n",
    "H, W = kernel_dim(H, W, kernel_size)\n",
    "\n",
    "kernel_size = [1,3]\n",
    "H, W = kernel_dim(H, W, kernel_size, stride=[1,3])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Define and initialize model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using CUDA\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(7, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1120, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=(7,3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3,3))\n",
    "        self.fc1 = nn.Linear(7*8*20, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(7*8*20, -1)\n",
    "        x = x.permute(1, 0)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.squeeze(x)\n",
    "        # x = self.sigmoid(x)  # Removed sigmoid because using nn.BCEWithLogitsLoss\n",
    "        return x\n",
    "\n",
    "# Parameters for dataloader\n",
    "params = {'batch_size' : 15000,\n",
    "          'shuffle' : False,\n",
    "          'num_workers': 0}\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.Tensor([10])) # weight = torch.Tensor([10]).repeat_interleave(params['batch_size']))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Chunk\n",
      "Chunk Length = 536713\n",
      "\n",
      "Train Epoch: 0 [0/2043967 (0%)]\tLoss: 5.026576\n",
      "Epoch 0: train loss: 5.026576042175293\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8234641551971436\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.636552095413208\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.426703691482544\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.020479202270508\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.981693983078003\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 4.416798114776611\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.7679948806762695\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.4823718070983887\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.4664628505706787\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.3213751316070557\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.809999942779541\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8760323524475098\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.9589977264404297\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8950071334838867\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8916120529174805\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.982490301132202\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8814640045166016\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.5760011672973633\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.828878879547119\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.8791704177856445\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.580991744995117\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.430774450302124\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.0879971981048584\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.9417829513549805\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.7222187519073486\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.0597949028015137\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.982417106628418\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 2.818960428237915\n",
      "Training Accuracy: 0.0\n",
      "\n",
      "Epoch 0: train loss: 3.2266829013824463\n",
      "Training Accuracy: 0.006676722275848876\n",
      "\n",
      "Epoch 0: train loss: 3.1598875522613525\n",
      "Training Accuracy: 0.006676722275848876\n",
      "\n",
      "Epoch 0: train loss: 2.8072500228881836\n",
      "Training Accuracy: 0.006676722275848876\n",
      "\n",
      "Epoch 0: train loss: 2.588772773742676\n",
      "Training Accuracy: 0.006676722275848876\n",
      "\n",
      "Epoch 0: train loss: 2.6214540004730225\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.826368570327759\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Full chunk traversed, 536713 / 2043967 total samples traversed\n",
      "Loading Chunk\n",
      "Chunk Length = 518177\n",
      "Epoch 0: train loss: 2.779201030731201\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 3.064023494720459\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.959552526473999\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.972642660140991\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.899585485458374\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.7879178524017334\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.775423049926758\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 2.65878963470459\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 3.0601441860198975\n",
      "Training Accuracy: 0.013723802781551758\n",
      "\n",
      "Epoch 0: train loss: 3.110322952270508\n",
      "Training Accuracy: 0.02036725641852339\n",
      "\n",
      "Epoch 0: train loss: 3.164923906326294\n",
      "Training Accuracy: 0.02036725641852339\n",
      "\n",
      "Epoch 0: train loss: 2.7623090744018555\n",
      "Training Accuracy: 0.02726707427272554\n",
      "\n",
      "Epoch 0: train loss: 2.8367714881896973\n",
      "Training Accuracy: 0.034113075211096855\n",
      "\n",
      "Epoch 0: train loss: 2.838477611541748\n",
      "Training Accuracy: 0.040957608415400054\n",
      "\n",
      "Epoch 0: train loss: 2.815539598464966\n",
      "Training Accuracy: 0.040957608415400054\n",
      "\n",
      "Epoch 0: train loss: 2.9528729915618896\n",
      "Training Accuracy: 0.047724351714093234\n",
      "\n",
      "Epoch 0: train loss: 2.918607473373413\n",
      "Training Accuracy: 0.05451457875787623\n",
      "\n",
      "Epoch 0: train loss: 2.8540453910827637\n",
      "Training Accuracy: 0.05451457875787623\n",
      "\n",
      "Epoch 0: train loss: 2.746021270751953\n",
      "Training Accuracy: 0.05451457875787623\n",
      "\n",
      "Epoch 0: train loss: 2.8359735012054443\n",
      "Training Accuracy: 0.061360579696247544\n",
      "\n",
      "Epoch 0: train loss: 2.735684633255005\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.780801296234131\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.818992853164673\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.518655300140381\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.9008259773254395\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.800489902496338\n",
      "Training Accuracy: 0.06827409640175208\n",
      "\n",
      "Epoch 0: train loss: 2.7797203063964844\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.940595865249634\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.745776653289795\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.6856436729431152\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.7213895320892334\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.6856610774993896\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 3.124053716659546\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.8161869049072266\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.7927308082580566\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Full chunk traversed, 1054890 / 2043967 total samples traversed\n",
      "Loading Chunk\n",
      "Chunk Length = 521554\n",
      "Epoch 0: train loss: 2.6380393505096436\n",
      "Training Accuracy: 0.07515630144713686\n",
      "\n",
      "Epoch 0: train loss: 2.6220192909240723\n",
      "Training Accuracy: 0.08213586618570652\n",
      "\n",
      "Epoch 0: train loss: 2.929051160812378\n",
      "Training Accuracy: 0.08892609322948952\n",
      "\n",
      "Epoch 0: train loss: 2.848212957382202\n",
      "Training Accuracy: 0.09576573398689901\n",
      "\n",
      "Epoch 0: train loss: 3.0019662380218506\n",
      "Training Accuracy: 0.102508504295813\n",
      "\n",
      "Epoch 0: train loss: 2.84785532951355\n",
      "Training Accuracy: 0.102508504295813\n",
      "\n",
      "Epoch 0: train loss: 2.7851691246032715\n",
      "Training Accuracy: 0.10938826311775092\n",
      "\n",
      "Epoch 0: train loss: 2.4589240550994873\n",
      "Training Accuracy: 0.10938826311775092\n",
      "\n",
      "Epoch 0: train loss: 2.2399301528930664\n",
      "Training Accuracy: 0.10938826311775092\n",
      "\n",
      "Epoch 0: train loss: 2.7867319583892822\n",
      "Training Accuracy: 0.10938826311775092\n",
      "\n",
      "Epoch 0: train loss: 2.5898046493530273\n",
      "Training Accuracy: 0.11638348368638045\n",
      "\n",
      "Epoch 0: train loss: 2.5499167442321777\n",
      "Training Accuracy: 0.11638348368638045\n",
      "\n",
      "Epoch 0: train loss: 2.756796360015869\n",
      "Training Accuracy: 0.11638348368638045\n",
      "\n",
      "Epoch 0: train loss: 3.2987875938415527\n",
      "Training Accuracy: 0.12301764167425404\n",
      "\n",
      "Epoch 0: train loss: 2.779392957687378\n",
      "Training Accuracy: 0.12990767463466876\n",
      "\n",
      "Epoch 0: train loss: 2.2824182510375977\n",
      "Training Accuracy: 0.12990767463466876\n",
      "\n",
      "Epoch 0: train loss: 2.8711888790130615\n",
      "Training Accuracy: 0.13675367557304008\n",
      "\n",
      "Epoch 0: train loss: 2.6603074073791504\n",
      "Training Accuracy: 0.14370241789617935\n",
      "\n",
      "Epoch 0: train loss: 2.421872138977051\n",
      "Training Accuracy: 0.14370241789617935\n",
      "\n",
      "Epoch 0: train loss: 2.811769723892212\n",
      "Training Accuracy: 0.14370241789617935\n",
      "\n",
      "Epoch 0: train loss: 3.057985544204712\n",
      "Training Accuracy: 0.14370241789617935\n",
      "\n",
      "Epoch 0: train loss: 2.7206790447235107\n",
      "Training Accuracy: 0.14370241789617935\n",
      "\n",
      "Epoch 0: train loss: 2.717893362045288\n",
      "Training Accuracy: 0.15062131629326697\n",
      "\n",
      "Epoch 0: train loss: 2.6066999435424805\n",
      "Training Accuracy: 0.15759892405307913\n",
      "\n",
      "Epoch 0: train loss: 2.7815439701080322\n",
      "Training Accuracy: 0.15759892405307913\n",
      "\n",
      "Epoch 0: train loss: 2.6712098121643066\n",
      "Training Accuracy: 0.16454326317401405\n",
      "\n",
      "Epoch 0: train loss: 2.660545825958252\n",
      "Training Accuracy: 0.17149445172060018\n",
      "\n",
      "Epoch 0: train loss: 2.5480053424835205\n",
      "Training Accuracy: 0.17149445172060018\n",
      "\n",
      "Epoch 0: train loss: 2.7448959350585938\n",
      "Training Accuracy: 0.17149445172060018\n",
      "\n",
      "Epoch 0: train loss: 2.9452691078186035\n",
      "Training Accuracy: 0.17149445172060018\n",
      "\n",
      "Epoch 0: train loss: 3.068875789642334\n",
      "Training Accuracy: 0.17149445172060018\n",
      "\n",
      "Epoch 0: train loss: 2.9237005710601807\n",
      "Training Accuracy: 0.1782895712112769\n",
      "\n",
      "Epoch 0: train loss: 3.015361785888672\n",
      "Training Accuracy: 0.1782895712112769\n",
      "\n",
      "Epoch 0: train loss: 2.6698925495147705\n",
      "Training Accuracy: 0.1782895712112769\n",
      "\n",
      "Epoch 0: train loss: 2.627809762954712\n",
      "Training Accuracy: 0.1782895712112769\n",
      "\n",
      "Full chunk traversed, 1576444 / 2043967 total samples traversed\n",
      "Loading Chunk\n",
      "All subroots have been traversed\n",
      "Chunk Length = 467523\n",
      "Epoch 0: train loss: 2.8560104370117188\n",
      "Training Accuracy: 0.1782895712112769\n",
      "\n",
      "Epoch 0: train loss: 2.705939531326294\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.804811477661133\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.5742948055267334\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.6648902893066406\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.6677937507629395\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.802278995513916\n",
      "Training Accuracy: 0.1852295071300075\n",
      "\n",
      "Epoch 0: train loss: 2.4533920288085938\n",
      "Training Accuracy: 0.18549027454944234\n",
      "\n",
      "Epoch 0: train loss: 2.4676754474639893\n",
      "Training Accuracy: 0.19254909692769012\n",
      "\n",
      "Epoch 0: train loss: 2.7800049781799316\n",
      "Training Accuracy: 0.19254909692769012\n",
      "\n",
      "Epoch 0: train loss: 2.87109112739563\n",
      "Training Accuracy: 0.19254909692769012\n",
      "\n",
      "Epoch 0: train loss: 2.7430689334869385\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.5616416931152344\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.753042697906494\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.938042163848877\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.7966582775115967\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.678968667984009\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.7265496253967285\n",
      "Training Accuracy: 0.19945918892036907\n",
      "\n",
      "Epoch 0: train loss: 2.928311586380005\n",
      "Training Accuracy: 0.2062738781986206\n",
      "\n",
      "Epoch 0: train loss: 2.760840654373169\n",
      "Training Accuracy: 0.2131697820953078\n",
      "\n",
      "Epoch 0: train loss: 2.9824893474578857\n",
      "Training Accuracy: 0.2131697820953078\n",
      "\n",
      "Epoch 0: train loss: 3.1315953731536865\n",
      "Training Accuracy: 0.2131697820953078\n",
      "\n",
      "Epoch 0: train loss: 2.645578384399414\n",
      "Training Accuracy: 0.22013907269540067\n",
      "\n",
      "Epoch 0: train loss: 2.9295761585235596\n",
      "Training Accuracy: 0.226920493334775\n",
      "\n",
      "Epoch 0: train loss: 2.7578532695770264\n",
      "Training Accuracy: 0.226920493334775\n",
      "\n",
      "Epoch 0: train loss: 2.6547253131866455\n",
      "Training Accuracy: 0.2339122891905789\n",
      "\n",
      "Epoch 0: train loss: 2.733400583267212\n",
      "Training Accuracy: 0.2339122891905789\n",
      "\n",
      "Epoch 0: train loss: 2.892195463180542\n",
      "Training Accuracy: 0.2339122891905789\n",
      "\n",
      "Epoch 0: train loss: 2.8020992279052734\n",
      "Training Accuracy: 0.24079204801251683\n",
      "\n",
      "Epoch 0: train loss: 2.765678644180298\n",
      "Training Accuracy: 0.24079204801251683\n",
      "\n",
      "Epoch 0: train loss: 2.6814255714416504\n",
      "Training Accuracy: 0.24079204801251683\n",
      "\n",
      "Full chunk traversed, 2043967 / 2043967 total samples traversed\n",
      "Epoch 0: train loss: 2.394223928451538\n",
      "Training Accuracy: 0.24269031740727712\n",
      "\n",
      "Loading Chunk\n",
      "All subroots have been traversed\n",
      "Chunk Length = 232072\n",
      "Full chunk traversed, 232072 / 232072 total samples traversed\n",
      "Validation Accuracy: 0.7520381605708574\n",
      "\n",
      "Loading Chunk\n",
      "All subroots have been traversed\n",
      "Chunk Length = 0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "cannot do a non-empty take from an empty axes.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e7c40a093c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m#  Transfer to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mlocal_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2c22d51fd298>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# Split song into 150 ms windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wrap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_chunk_length\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# Only care about onsets, losing note information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtake\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[0;32m    192\u001b[0m            [5, 7]])\n\u001b[0;32m    193\u001b[0m     \"\"\"\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'take'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: cannot do a non-empty take from an empty axes."
     ]
    }
   ],
   "source": [
    "max_epochs = 2\n",
    "root = Path(r'X:\\Training Data\\Processed')\n",
    "train_paths, val_paths, test_paths = train_val_test_split(root, 5, shuffle=True)\n",
    "\n",
    "# Define datasets and loaders\n",
    "trains = DistributedFolderDataset(train_paths)\n",
    "train_loader = torch.utils.data.DataLoader(trains, **params)\n",
    "vals = DistributedFolderDataset(val_paths)\n",
    "val_loader = torch.utils.data.DataLoader(vals, **params)\n",
    "tests = DistributedFolderDataset(test_paths)\n",
    "test_loader = torch.utils.data.DataLoader(tests, **params)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.Tensor([100])) # Start out with high loss for one epoch, then change afterwards\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "train_accs =[]\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    num_true = 0\n",
    "\n",
    "    # Training\n",
    "    for batch_idx, (local_batch, local_labels) in enumerate(train_loader):\n",
    "        #  Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device, dtype = torch.float32), local_labels.to(device, dtype = torch.float32)\n",
    "\n",
    "        # Model computations\n",
    "        y_pred = model(local_batch)\n",
    "        loss = criterion(y_pred, local_labels)\n",
    "        preds = torch.argmax(y_pred, dim=-1).cpu().numpy()\n",
    "        \n",
    "        num_true += np.sum(preds == local_labels.cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            # print training update\n",
    "            print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(local_batch), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "        print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        print('Training Accuracy: {}\\n'.format(num_true / len(trains)))\n",
    "        train_accs.append(num_true / len(trains))\n",
    "\n",
    "        # Free up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight = torch.Tensor([10]))\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    num_true = 0\n",
    "\n",
    "    for batch_idx, (local_batch, local_labels) in enumerate(val_loader):\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device, dtype = torch.float32), local_labels.to(device, dtype = torch.int64)\n",
    "\n",
    "        # Model computations\n",
    "        y_pred = model(local_batch)\n",
    "        preds = torch.argmax(y_pred, dim=-1).cpu().numpy()\n",
    "        num_true += np.sum(preds == local_labels.cpu().numpy()) \n",
    "\n",
    "    # Set model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    print('Validation Accuracy: {}\\n'.format(num_true / len(vals)))\n",
    "    val_accs.append(num_true / len(vals))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\ewais\\Documents\\GitHub\\tensor-hero\\Embedding Generation\n"
     ]
    }
   ],
   "source": [
    "print(str(Path().cwd().parent) + r'\\Audio Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}